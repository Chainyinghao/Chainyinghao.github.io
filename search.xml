<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[openstack（七）]]></title>
    <url>%2F2020%2F05%2F06%2Fopenstack-6%2F</url>
    <content type="text"><![CDATA[cinder服务 cinder是openstack的块存储服务，它负责volume从创建到删除整个生命周期的管理，其中具体包括： 通过对外提供API，使得可以查询和管理volume、volume snapshot和volume type。 提供scheduler来完成volume的调度，合理优化存储资源的分配。 提供driver框架来支持多种存储的后端，包括LVM、ceph和其他商业存储（EMC、华为存储等等）。 组件 1、cinder-api 客户端可以将请求发送到endpoints指定的地址，向cinder-api请求操作。Cinder-api对接收到的HTTP API请求做如下处理： 检查客户端传入的参数是否合法有效 调用cinder其他子服务处理客户端请求 将cinder其他子服务返回的结果序列号返回给客户端 2、cinder-scheduler 创建volume时，cinder-scheduler会基于容量、volume type等条件选择出最合适的存储节点，然后让其创建volume。 3、cinder-volume Cinder-volume在存储节点上运行，OpenStack对volume的操作，最后都是交给cinder-volume来完成的。它自己本身是不管理真正的存储设备，存储设备最后都是由volume provider来管理。Cinder-volume将LVM作为默认的volume provider。cinder-volume的作用： 通过driver架构支持多种volume provider 定期向OpenStack报告存储节点的状态 实现volume生命周期的管理 4、cinder-backup备份提供了三种驱动服务：ceph，TSM，swift，其中默认备份是swift。cinder驱动服务的配置在cinder.conf中是： 1backup_driver=cinder.backup.drivers.swift 详细可参考这里：[https://blog.csdn.net/u010433148/article/details/54629097] Glance服务 glance是为虚拟机的创建提供镜像的服务，我们基于OpenStack是构建基本的Iaas平台对外提供虚拟机，而虚拟机在创建时必须选择需要安装的操作系统，glance服务就是选择提供不同的操作系统镜像。 组件 Glance-API 主要用来响应各种REST请求然后通过其他模块（主要是glance-registry组件和后端存储接口）完成镜像的上传、删除、查询等操作。可以简单的再分为两部分：一层中间件，它主要是做一些请求的解析工作（如分析出版本号），另外一部分提供实际的服务（如与镜像上传下载的后端存储接口交互）。默认绑定端口是9292。 Glance-registry 镜像注册服务用于提供镜像元数据的REST接口，主要工作是存储或者获取镜像的元数据，与mysql进行交互，也可以简单的再细分为两部分，API和具体server。元数据是指镜像相关的一些信息（如id，size，status，location，checksum，min_disk，min_ram，owner等）真正的镜像数据保存在实际所使用的后端存储里。默认绑定端口是9191. Image store 严格来说image store不属于glance的组件，它只是一个接口层，提供镜像存储和查询的接口。 工作流程 1、glance-api接收REST API的请求，通过其他模块来完成诸如镜像的创建、删除、查询、获取等操作。 2、glance-registry与数据库进行交互，更新数据库中的两张表，image和image_proterties表，分别用来记录镜像的大小、格式和镜像定制化的相关信息等，]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack（六）]]></title>
    <url>%2F2020%2F05%2F06%2Fopenstack-5%2F</url>
    <content type="text"><![CDATA[openstack网络类型（OVS） local local网络与其他网络和节点隔离，此网络中的实例只能和同一节点上的local网络中的实例之间进行通信，主要用于单机的一个测试。 flat flat是没有VLAN tag的网络，flat网络中的主机实例可以跨节点和同一flat网络中的主机实例通信。宿主机中的物理网卡通过br-int网桥与flat network相连，每一个flat网络都需要占用一个物理网卡。 因此在ml2文件中创建了一个flat网络后 flat_networks=flat1，还需要将其与我们的某一个网桥所绑定bridge_mappings=flat1:br-eth1 ，通过这个网桥与我们的物理网卡通信。这时需要通过ovs-vsctl命令创建网桥和相应的设备，比如： 12ovs-vsctl add-br br-eth1ovs-vsctl add-port br-eth1 eth1 重启所有节点的neutron服务后，查看ovs网桥情况，此时会发现在网桥br-int和br-eth1上会多了一对peer设备来指向彼此，实例的网卡连接在br-int上，物理网卡eth1连接在br-eth1上，此时两个网桥之间通过那一对peer设备来进行通信。 VLAN VLAN是带标签的网络，同一个VLAN中的实例可以相互通信，不同VLAN的主机需要通过三层router转发进行通信。在OVS机制下，不同VLAN instance的虚拟网卡都接到br-int上（Linux bridge则是不同的VLAN接到不同的网桥上）。在ml2文件中定义VLAN的label和VLAN号段，比如：network_vlan_ranges=default:3001:4000，然后和flat网络一样需要指定VLAN网络和物理网络的对应关系bridge_mappings=default:br-eth1。重启相关服务后，如果我们创建一个VLAN网络，那么这个时候在br-int上会连接一个tap设备，这个设备是这个VLAN的DHCP接口，br-int和br-eth1之间的通信方式和flat网络一样，通过一对peer设备去通信。 VXLAN vxlan网络是基于隧道技术的overlay网络，vxlan网络通过唯一的VNI标识号与其他vxlan区分，也就是说只有相同的VNI号的隧道才能进行通信，vxlan数据包会通过VNI封装成UDP包进行传输，这样将二层的包封装在三层去进行传输，能够克服VLAN和物理网络基础设置的限制。另外，vxlan之间的通信时借助于br-tun网桥进行通信。 vxlan的封装和解封是通过VTEP（virtual tunnel endpoint）设备来进行的，每一个VTEP都有一个IP interface，并且配置得有一个IP地址，通过该interface来发送和接收vxlan的数据包。而我们的br-tun网桥其实可以分为两个部分，如图： 上层是VTEP，下层是一个普通的VLAN bridge，所以对于host来说它有两层网络，一部分是VLAN，一部分是vxlan。 GRE GRE是与vxlan类似的一种overlay网络，主要区别是封住的协议不同，GRE采用的是IPIP协议进行封装。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud-init（二）]]></title>
    <url>%2F2020%2F04%2F07%2Fcloud-init-1%2F</url>
    <content type="text"><![CDATA[初始化判断 cloud-init主要通过读取/etc/cloud/cloud.cfg配置文件，来执行相应模块，模块一般位于Python第三方包cloud-init目录下:/usr/lib/python2.6/site-packages/cloudinit/config，模块有多种运行模式（frequency），包括PER_INSTANCE, PER_BOOT, PER_ONCE ,PER_ALWAYS，在上述python文件中配置： 模式为per-once的模块，一旦运行完毕会在一个名为sem的目录中创建一个信号文件，从而防止模块的在下次启动时重复运行，若虚拟机初始化时，sem目录中不存在相应的信号文件，Cloudinit则会执行一次，以后都不会执行。 对于windows系统，主要有cloudbase-init读取注册表项判断是不是初始化。例如注入密码时，第一次注入之后会在注册表中更新HKEY_LOCAL_MACHINE/SOFTWARE/Cloudbase Sloutions/Cloudbase-Init/实例ID/Plugins/SetUserPasswordPlugin值为1,表示下次不再更新。如果需要从数据源读取密码重新注入，只需删除SetUserPasswordPlugin项目即可。 相关问题 1、在定制实例时，会依次读取配置文件cloud.cfg配置项，在某些环境的主机需配置DNS才能访问外网，通过cloud_config_modules模块的runcmd配置项配置DNS，由于配置项package-update-upgrade-install在runcmd之前，cloud-init会先安装软件，这时外网是不通的，所以报错。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>cloud-init</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud-init（一）]]></title>
    <url>%2F2020%2F04%2F07%2Fcloud-init%2F</url>
    <content type="text"><![CDATA[cloud-init是一个支持多操作系统的软件包，用于处理云主机早期的初始化操作，当系统启动时，cloud-init可以在云的datasource中获取metadata（nova metadata服务或者config drive（config drive是一个特殊的文件系统，openstack会将metadata写到config drive，并在instance启动的时候挂载给instance，然后去读metadata）），完成包括但不限于下面所列举的定制化工作： 设置default locale 设置云主机hostname 生成实例的SSH私钥 添加SSH KEYS到.ssh/authorized_keys ，使用户可以登录 设置临时挂载点 设置用户密码 配置网络 安装软件包 从图中可知道相关的一个流程：当cloud-init启动后会去读取源数据，如果配置了dhcp服务，则去nova-metadata中读取，如果没有配置的话就去cloud drive读取；当去metadata里面去读的话默认是从l3-agent里面读取。 注：云主机中lsblk查看块设备，挂载/dev/sr0,进入这个目录可以查看相关文件： 我们查看日志进行分析： 第一阶段：扫描文件系统、挂载/dev/sr0，读取数据，卸载、加载config driver、读取文件cloud.cfg。 第二阶段：扫描网卡、配置网卡、写入到文件、配置dns、完成第二阶段 第三阶段：init-network，读取check-cache，调用相关cloud_init_module初始化（主机名初始化，check-cache、consume-user-data、consume-vendor-data、config-migrator、config-bootcmd、config-write-files、config-growpart、config-resizefs、config-set_hostname、config-update_etc_hosts、config-rsyslog、config-users-groups、config-ssh） 第四阶段：调用config_modules的相关模块，mounts，locale，set-passwords，yum-add-repo，package-update-upgrade-install，timezone，puppet，chef，salt-minion，mcollective，disable-ec2-metadata，runcmd 第五阶段：调用final_modules模块，config-rightscale_userdata、config-scripts-per-boot、config-scripts-per-instance、config-scripts-user、config-ssh-authkey-fingerprints、config-keys-to-console、config-phone-home、config-final-message cloud-init会在云主机上创建如下的目录结构用来记录相关的信息： 其中： data目录会记录云主机的instance id，datasource，hostname等信息 instances目录下记录了云主机的instance的user-data，cloud-config，用到的datasource，运行的命令等信息。 scripts下的per-boot存放运行启动脚本，系统引导时运行；per-once：只运行一次脚本；per-instance：一个实例初始化会执行一次 配置文件cloud.cfg： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465users: - defaultdisable_root: 0ssh_pwauth: 1mount_default_fields: [~, ~, &apos;auto&apos;, &apos;defaults,nofail&apos;, &apos;0&apos;, &apos;2&apos;]resize_rootfs_tmp: /devssh_deletekeys: 0ssh_genkeytypes: ~syslog_fix_perms: ~cloud_init_modules: - migrator - bootcmd - write-files - growpart - resizefs - set_hostname - update_hostname - update_etc_hosts - rsyslog - users-groups - sshcloud_config_modules: - mounts - locale - set-passwords - yum-add-repo - package-update-upgrade-install - timezone - puppet - chef - salt-minion - mcollective - disable-ec2-metadata - runcmdcloud_final_modules: - rightscale_userdata - scripts-per-once - scripts-per-boot - scripts-per-instance - scripts-user - ssh-authkey-fingerprints - keys-to-console - phone-home - final-messagesystem_info: default_user: name: ec2-user lock_passwd: true gecos: Cloud User groups: [wheel, adm, systemd-journal] sudo: [&quot;ALL=(ALL) NOPASSWD:ALL&quot;] shell: /bin/bash distro: rhel paths: cloud_dir: /var/lib/cloud templates_dir: /etc/cloud/templates ssh_svcname: sshd# vim:syntax=yaml cloud-init安装时会将这几个阶段执行的任务以服务的形式注册到系统中，比如在systemd的环境下，我们能够看到这几个阶段分别对应的服务： generator - cloud-config.target local - cloud-init-local.service nework - cloud-init.service config - cloud-config.service final - cloud-final.service 这些进程只会执行一次，不会以常驻进程的形式存在: generator 读取配置文件cloud.cfg local 此时instance还不知道该如何配置网卡，cloud-init的任务就是从config drive中获取配置信息，然后写入/etc/network/interfaces文件，如果没有config drive，则将所有网卡配置成dhcp模式，只有当网卡正确配置以后，才能获取到metadata。 network 在local阶段之后，当网络设置完成了才进行这个阶段，这个阶段会调用。两个模块disk_setup，mounts来完成磁盘格式化分区，添加挂载点等操作，另外执行一些系统引导早期可以调用的模块，读取cloud_init_modules模块的指定配置。 config 执行配置模块，读取cloud_config_modules模块的指定配置 final 这个阶段初始化已经基本完成，因此大部分的模块在这个阶段都能正常运行，比如安装软件，运行一些自动化工具，如puppet，salt等，以及用户定义的脚本。 cloud-init传入脚本 比如设置hostname： 123#cloud-confighostname: my1.cloudman.ccmanage_etc_hosts: true cloud-init 只会读取以 #cloud-config 开头的数据，所以这一行一定要写对。hostname: http://my1.cloudman.cc 告诉 cloud-init 将 hostname 设置为 http://my1.cloudman.cc。manage_etc_hosts: true 告诉 cloud-init 更新 /etc/hosts 文件。 脚本传递到instance： instance 部署时，直接将其粘贴到 Customization Script 输入框中（社区版） 将其保存为文件，命令行 nova boot 或者 openstack server create 部署 instance 时，使用参数 --user-data 传入。 将其保存为文件进行上传]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>cloud-init</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack（五）]]></title>
    <url>%2F2020%2F04%2F07%2Fopenstack-4%2F</url>
    <content type="text"><![CDATA[neutron组件 neutron网络服务组件提供虚拟机实例对网络的连接，其中plugins能够提供对多种网络设备和软件的支持，使OpenStack环境的构建和部署具备更多的灵活性，最重要的就是为虚拟机实例提供网络连接。 组件： Neutron-sever：接收和路由API请求到OpenStack中的网络plug-in. OpenStack networking plug-ins and agents：创建端口、网络和子网，提供IP地址。 Messageing queue：在neutron-sever和agents之间路由信息，同时也会作为一个数据库存储plugins的网络连接状态。 neutron网络服务组件利用软件定义提供网络、子网和路由功能，它模仿物理网络设备的功能，在网络中划分多个子网，路由器在不同的子网间传递数据。其中包括二层交换，三层路由，负载均衡，防火墙和VPN等。 neutron管理的网络资源包括network、subnet和port，其中network是一个隔离的二层广播域，neutron支持多种类型的network，包括local、flat、VLAN、vxlan和GRE。 架构 Neutron server 对外提供网络API，接收请求，并调用plugin处理请求。 neutron plugin 处理neutron server发来的请求，维护网络，并通过调用agent来处理后续工作。根据功能又可以分为core plugins和service plugins。前者负责维护网络中的network、subnet和port；后者提供网络中的routing、防火墙、load balance等。 neutron agent 接收来自plugin的请求，并负责在network provider上真正实现网络功能。 network provider 提供网络服务的虚拟或者物理网络设备，向Linux bridge或者OVS等。 Queue 各组件通过messaging queue通信和调用 Database 用来存放OpenStack的网络状态信息，包括network、subnet、port、router等。 将server、plugin部分详细的看，如图： 其中： core API 对外提供管理network、subnet和port的restful API。 extension API 对外提供管理router，LBaaS，Firewalld等服务的restful API。 common service 校验和认证API请求。 neutron core 通过调用相应的plugin来处理请求。 core plugin API 定义了core plugin的抽象功能集合，用来响应请求和调用core plugin组件。 extension plugin API 定义了core plugin的抽象功能集合，用来响应请求和调用service plugin组件。 core plugin 维护network、subnet和port的状态；调用相应的agent来处理。 service plugin 实现了routing、Firewall、load balance等；调用相应的agent来处理。 关于ML2 core plugin 它是neutron在H版本实现的一个新的core plugin，用于替代原有的Linux bridge和OVS，传统的core plugin存在的问题： 无法使用多个network provider 这就说明了以前当你选择了Linux bridge plugin之后就只能选择Linux bridge agent，无法使用其他的虚拟交换机。 开发新的core plugin工作量大 所有传统的core plugin都需要编写大量重复和类似的数据库访问的代码，大大增加了plugin开发和维护的工作量。 ML2作为新一代的plugin，提供了一个框架，允许在OpenStack网络中同时使用多种layer2网络技术，不同的节点可以使用不同的网络实现机制。 这样的话，可以在不同节点上分别部署Linux bridge agent、OVS，hyper-V以及其他第三方agent。另外，ML2不但支持异构部署方案，同时能够与现有的agent无缝集成：以前用的agent不需要变，只需要将neutron server上的传统core plugin替换为ML2。 ML2的架构 ML2对二层的网络进行抽象和模拟，引入了type drivers和mechanism drivers机制，这两类解耦了neutron所支持的网络类型和提供这些网络的机制，这样的好处就是使得它具有很好的扩展性。 其中的网络类型（type）包括： local flat VLAN vxlan GRE mechanism driver负责提供和实现这些网络，并确保其状态，根据后端提供类型可以分为： Agent-based：包括Linux bridge和open vswitch controller-based：包括opendaylight、VMwareNSX 基于物理交换机：Cisco nexus等 neutron启用ml2，只需要在配置文件中开启就行（neutron.conf）： 而关于ml2的配置，则是在（/etc/neutron/plugins/ml2/ml2_conf.ini）：]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack（四）]]></title>
    <url>%2F2020%2F04%2F07%2Fopenstack-3%2F</url>
    <content type="text"><![CDATA[Nova计算服务组件是OpenStack中非常重要、核心的部分，负责承载和管理云计算系统，与其他组件有密切关系。 Nova计算服务组件使用基于消息，无共享、松耦合无状态的架构。OpenStack项目中的核心服务组件都运行在多台主机节点上，包括Nova，cinder，neutron，swift和glance等服务组件，状态信息都存储在数据库中。控制节点服务通过HTTP与内部服务进行交互，但与scheduler服务、网络和卷服务的通信依赖高级消息队列协议进行。为避免消息阻塞而造成长时间等待响应，Nova计算服务组件采用异步调用的机制，当请求被接收后，响应被触发，发送回执，而不关注该请求是否被处理。 OpenStack项目中的控制节点服务影响着整个云环境的状态，API服务器为控制节点作为web服务前端而服务，处理各种交互信息，计算节点提供各种计算资源和计算服务，Nova计算服务组件中的网络服务提供虚拟网络。 driver框架 OpenStack作为开放的云操作系统，其开放性体现在采用了基于driver的框架。以nova为例，OpenStack的计算节点支持多种hypervisor。包括KVM、hyper-V、VMware、XEN、docker等。 Nova-compute为这些hypervisor定义了统一的接口，hypervisor只需要实现这些接口就可以driver的形式插入到OpenStack中去使用。 在nova-compute的配置文件/etc/nova/nova.conf中，由compute-driver配置项指定该计算节点使用哪一种hypervisor的driver。 关于messaging 在创建虚拟机的过程中，nova-*子服务之间的调用严重依赖messaging。这是因为它是采用了异步调用的方式。 同步调用 API直接调用scheduler的接口就是直接调用，其特点是API发出请求后需要一直等待，直到scheduler完成对compute的调度，将结果返回给API后API才能够继续做后面的工作。 异步调用 API通过messaging间接调用scheduler就是异步调用，其特点就是API发出请求后不需要等待，直接返回，继续做后面的事。scheduler从messaging接收到请求后执行调度，完成后将结果也通过messaging发送给API。 好处： 解耦各子服务：子服务不需要知道其他服务运行在哪里，只需要发送消息给messaging就能完成调度 提高性能：无需等待返回结果，这样就可以执行更多的工作，提高系统的吞吐量。 提高伸缩性：子服务可以根据需要进行扩展，启动更多的实例处理更多的请求，在提高可用性的同时也提高了整个伸缩性。而且这种变化不会影响到其他子服务。 nova组件 Nova-api service：负责对终端用户调用compute API的接收和反馈 Nova-api-metadata：负责接收虚拟机对metadata（元数据）访问请求，一般部署nova-network的多主机模式才会用nova-api-metadata。 Nova-compute：该服务通过hypervisor APIs（XenAPI、libvirt for KVM or QEMU、VMwareAPI）创建和终止虚拟机实例。该服务接收消息队列中的信息，执行一系列操作命令，如创建和更新虚拟机实例状态。 Nova-scheduler：该服务接收消息队列中的请求，从而决定虚拟机实例运行在哪个计算节点上。 Nova-conductor：nova-compute经常需要更新数据库，但是出于安全性和伸缩性的考虑，nova-compute不会直接访问数据库，而是交由nova-conductor去处理，并且这两个服务不能安装在同一个节点上。 我们可以通过nova service-list查看具体安装了哪些组件： 请求流程 客户或程序向api（nova-api）发送创建虚拟机的请求。 API对请求做一些必要的处理后，向message（rabbitMQ）发送了消息，让scheduler创建一个虚拟机 scheduler从messaging获取到API发送的请求后，执行调度算法，从若干节点中选出节点A。 scheduler向messaging发送消息，告知在A上创建这个虚拟机 计算节点A的compute（nova-compute）从messaging中获取到scheduler发给他的消息后，然后在本节点的hypervisor上启动虚拟机。 而对于其中nova-compute而言，它所做的事有这些： （1）为instance准备资源，根据指定的flavor依次为instance分配内存、磁盘和vCPU。 （2）创建instance的镜像文件，它会先检查是否已经下载，如果没有，则会先向glance去下载相应的镜像文件到本地，然后通过qemu-img创建镜像。 （3）创建实例的XML文件 （4）创建虚拟网络以及启动实例 在虚机创建的过程中，compute如果需要查询或者更新数据库信息，会通过messaging向conductor（nova-conductor）发送消息。 调度方式 在nova.conf中，nova通过schedulerdriver、scheduleravailable_filters和schedulerdefaultfilters这三个参数来配置nova-scheduler。 1、filter scheduler 它是nova-scheduler默认的调度器，调度过程分为两步： 通过过滤器（filter）选择满足条件的计算节点 通过权重计算选择在最优（权重值最大）的计算节点上创建instance。 它也允许使用第三方scheduler，配置好driver就行。也可以使用多个filter一次进行过滤，过滤之后的节点再通过计算权重选出最合适的节点。 开始有6个计算节点 通过filter过滤，刷了两个 然后计算权重，权重高的host5入选 2、filter 当filterscheduler需要执行调度操作的时候，会让filter对计算节点进行判断，返回true或者false。 Nova.conf中available_filters选项用于配置配置可用的filter： 另外还有一个选项用于指定scheduler真正使用的filter，默认值： Filter scheduler将按照列表中的顺序依次过滤。 Retryfilter 作用是刷掉之前已经调度过的节点。比如说A、B、C三个节点都通过了过滤，最终A因为权重最大被选中，但是由于某个原因，操作在A上失败了，默认情况下nova-scheduler会重新执行过滤操作（默认是3，max_attempts），那么这时候retryfilter就会将A直接刷掉，避免操作再次失败。 Availabilityzonefilter 为提高容灾性和提供隔离服务，可以将计算节点划分到不同的availability zone中。OpenStack默认有一个命名为nova的availability zone，所有的计算节点初始都放在nova中。创建instance时，需要指定将instance部署到在哪个zone中。在做filter的时候，将不属于指定zone的计算节点过滤掉。 Ramfilter 将不能满足flavor内存需求的计算节点过滤掉。需要注意的是：为了提高系统的资源使用率，OpenStack在计算节点可用内存是允许overcommit，也就是说可以超过实际内存的大小。超过的程度通过文件中参数来控制： Diskfilter 将不能满足flavor磁盘需求的计算节点过滤掉，和上面一样，允许overcommit。 Corefilter 将不满足flavor vcpu需求的计算节点过滤掉。 Computefilter 保证只有nova-compute服务正常工作的计算节点，才能够被调度。 Computecapabiliesfilter 如果想将instance指定部署到x86_64架构的节点上，就可以利用它。 Imagepropertiesfilter 根据所选image的属性来筛选匹配的计算节点。跟flavor类似，image也有metadata用于指定其属性 Servergroupantiaffinityfilter 可以尽量将instance分散部署到不同的节点上。为保证分散部署，可以进行如下操作： 123456789• 创建一个anti-affinity策略的server group：Nova server-group-create --policy anti-affinity group-1• 依次创建instance，将instance放到group-1中：Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins1Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins2Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins3调度使会将ins1、2、3部署到不同的计算节点，创建instance的时候，如果没有指定server group,servergroupantiaffinityfilter会直接通过，不做任何过滤。 Servergroupaffinityfilter 与上面一个相反，它会尽量将instance部署到同一个计算节点上。 1234Nova server-group-create --policy affinity group-1Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins1Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins2Nova boot --image IMAGEID --flavor 1 -hint group=group-1 ins3 3、weight 经过过滤，scheduler选出能够部署instance的计算节点，nova-scheduler会通过计算节点空闲的内存量计算权重值：空闲内存越多，权重越大。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack（三）]]></title>
    <url>%2F2020%2F04%2F07%2Fopenstack-2%2F</url>
    <content type="text"><![CDATA[kolla-ansible部署（all-in-one） 环境]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack(二)]]></title>
    <url>%2F2020%2F03%2F09%2Fopenstack-1%2F</url>
    <content type="text"><![CDATA[安装（queen版，下一篇讲用kolla-ansible来安装） 环境 123192.168.40.150 computer01 centos7 4.4.197192.168.40.151 controller192.168.40.152 cinder 1、配置主机名的映射(在/etc/hosts文件中添加相应的主机和ip映射) 2、配置时间同步，控制节点为ntp server 1234567891011所有节点：yum install chrony -ycontroller：vim /etc/chrony.conf allow 192.168.0.0/16systemctl restart chronydComputer01和cinder：vim /etc/chrony.conf server controller iburst注释其他几行 测试： 在controller节点上： 其他节点上： 3、安装OpenStack包（所有节点） 1234yum install centos-release-openstack-queens -yyum upgradeyum install python-openstackclient -yyum install openstack-selinux -y 4、安装数据库（controller） 12345678910111213141516yum install mariadb mariadb-server python2-PyMySQL -yvim /etc/my.cnf.d/mariadb-server.cnf datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socklog-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.piddefault-storage-engine=Innodbbind-address=192.168.40.151 //controller的IPinnodb_file_per_table=onmax_connections=4096init-connect=&apos;SET NAMES utf8&apos;character-set-server=utf8mysql_secure_installation 5、安装rabbitMQ（controller） 1234567yum install rabbitmq-server -ysystemctl enable rabbitmq-server.servicesystemctl start rabbitmq-server.service添加用户并设置权限：rabbitmqctl add_user openstack 12345rabbitmqctl set_permissions -p / openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 6、安装数据库缓存（controller） 12yum install memcached python-memcached -y编辑/etc/sysconfig/memcached 注：（options，后面不加controller） 12systemctl enable memcached.servicesystemctl start memcached.service 7、安装etcd（controller） 8、安装keystone（controller） 创建keystone数据库并授权 安装、配置 1yum install openstack-keystone httpd mod_wsgi -y 编辑 /etc/keystone/keystone.conf 文件： 同步keystone数据库 1/bin/sh -c &quot;keystone-manage db_sync&quot; keystone 数据库初始化 12keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group keystone 引导身份认证 1keystone-manage bootstrap --bootstrap-password 123456 --bootstrap-admin-url http://controller:35357/v3/ --bootstrap-internal-url http://controller:5000/v3/ --bootstrap-public-url http://controller:5000/v3/ --bootstrap-region-id RegionOne 9、配置http服务 编辑 /etc/httpd/conf/httpd.conf 文件，配置 ServerName 参数 1ServerName controller /usr/share/keystone/wsgi-keystone.conf 链接文件 1ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 启动（如果报错，可关闭selinux） 12systemctl enable httpd.servicesystemctl start httpd.service 配置administrator账号 1234567export OS_USERNAME=adminexport OS_PASSWORD=123456export OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_AUTH_URL=http://controller:35357/v3export OS_IDENTITY_API_VERSION=3 10、创建domain，projects，users，roles 创建域： 1openstack domain create --description &quot;Domain&quot; example 创建服务项目 1openstack project create --domain default --description &quot;Service Project&quot; service 创建平台demo项目 1openstack project create --domain default --description &quot;Demo Project&quot; demo 创建demo用户 1openstack user create --domain default --password-prompt demo 创建用户角色 1openstack role create user 添加用户角色到demo项目和用户 1openstack role add --project demo --user demo user 11、检查配置 取消环境变量 1unset OS_AUTH_URL OS_PASSWORD 使用admin申请token 1openstack --os-auth-url http://controller:35357/v3 --os-project-domain-name Default --os-user-domain-name Default --os-project-name admin --os-username admin token issue 使用demo申请token 1openstack --os-auth-url http://controller:5000/v3 --os-project-domain-name Default --os-user-domain-name Default --os-project-name demo --os-username demo token issue 通过脚本来生成token，为了提高可操作性和工作效率，可以创建一个统一而完整的openRC文件，其包括通用变量和特殊变量 12source admin-openrcopenstack token issue 12、安装glance服务（controller） 创建glance数据库，并授权 获取admin用户的环境变量，并创建服务认证 12source admin-openrcopenstack user create --domain default --password-prompt glance 将admin添加到glance的项目和用户中 1openstack role add --project service --user glance admin 创建glance项目 1openstack service create --name glance --description &quot;OpenStack Image&quot; image 创建glance镜像服务的API端点 1openstack endpoint create --region RegionOne image public http://controller:9292 1openstack endpoint create --region RegionOne image internal http://controller:9292 1openstack endpoint create --region RegionOne image admin http://controller:9292 13、安装和配置glance组件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748yum install openstack-glance -y编辑 /etc/glance/glance-api.conf 文件：[database] //配置数据库连接connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken] //和[paste_deploy]一起，配置keystone身份认证服务组件访问auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone[glance_store] //配置虚拟机的存储路径和存储方式stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/编辑 /etc/glance/glance-registry.conf 文件：[database]connection = mysql+pymysql://glance:123456@controller/glance[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = glancepassword = 123456[paste_deploy]flavor = keystone同步镜像服务数据库：/bin/sh -c &quot;glance-manage db_sync&quot; glance启动：systemctl enable openstack-glance-api.service openstack-glance-registry.servicesystemctl start openstack-glance-api.service openstack-glance-registry.service 14、验证： 获取admin用户的环境变量，且下载镜像： 12345source admin-openrcwget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img上传镜像（如果没有9292端口报错，可能是日志权限问题）：openstack image create &quot;cirros&quot; --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public 查看： 15、安装配置computer（controller） 创建nova_api，nova，nova_cell0数据库 创建nova用户 12source admin-openrcopenstack user create --domain default --password-prompt nova 添加admin角色赋给项目和用户 1openstack role add --project service --user nova admin 创建nova计算服务 1openstack service create --name nova --description &quot;OpenStack Compute&quot; compute 创建API服务端点 1openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1 1openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1 1openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1 创建placement服务用户 1openstack service create --name placement --description &quot;Placement API&quot; placement 123openstack endpoint create --region RegionOne placement public http://controller:8778openstack endpoint create --region RegionOne placement internal http://controller:8778openstack endpoint create --region RegionOne placement admin http://controller:8778 16、安装和配置nova组件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api编辑 /etc/nova/nova.conf 文件：[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@controllermy_ip = 192.168.40.151use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[api_database]connection = mysql+pymysql://nova:123456@controller/nova_api[database]connection = mysql+pymysql://nova:123456@controller/nova[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3 memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled = trueserver_listen = $my_ipserver_proxyclient_address = $my_ip[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = 123456由于软件包的一个bug，需要在/etc/httpd/conf.d/00-nova-placement-api.conf 文件中添加如下配置：&lt;Directory /usr/bin&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt;&lt;/Directory&gt;重启httpd同步nova-api数据库：/bin/sh -c &quot;nova-manage api_db sync&quot; nova 注册cell0数据库：/bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; nova创建cell1：/bin/sh -c &quot;nova-manage cell_v2 create_cell --name=cell1 --verbose&quot; nova同步nova数据库：/bin/sh -c &quot;nova-manage db sync&quot; nova 验证： 重启： 12systemctl enable openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.servicesystemctl restart openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service 17、安装配置compute（computer节点） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253yum install openstack-nova-compute编辑 /etc/nova/nova.conf 文件：[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:123456@controllermy_ip = 192.168.40.150 //计算节点的网络管理IP地址use_neutron = Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = novapassword = 123456[vnc]enabled = Trueserver_listen = 0.0.0.0server_proxyclient_address = $my_ipnovncproxy_base_url = http://controller:6080/vnc_auto.html[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]os_region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:35357/v3username = placementpassword = 123456[libvirt]virt_type=kvm启动：systemctl enable openstack-nova-computesystemctl enable libvirtd.servicesystemctl start libvirtd.servicesystemctl restart openstack-nova-compute.service 确认nova计算服务组件已经成功运行和注册： 123controller：source admin-openrcopenstack compute service list --service nova-compute 发现计算节点： 1/bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova 18、在controller上验证计算服务操作 列出服务组件 列出身份服务中的API端点与身份服务的连接 列出镜像 检查cells和placement API 19、安装和配置网络组件（controller） 创建数据库以及授权 创建用户以及服务 1openstack user create --domain default --password-prompt neutron 将admin赋给neutron和service 1openstack role add --project service --user neutron admin 创建service entity 1openstack service create --name neutron --description &quot;OpenStack Networking&quot; network 创建网络服务API 端点 1openstack endpoint create --region RegionOne network public http://controller:9696 1openstack endpoint create --region RegionOne network internal http://controller:9696 1openstack endpoint create --region RegionOne network admin http://controller:9696 20、配置网络部分（controller） 123456789101112131415161718192021222324252627282930313233343536yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables配置服务组件，编辑 /etc/neutron/neutron.conf 文件：[DEFAULT]auth_strategy = keystonecore_plugin = ml2service_plugins =transport_url = rabbit://openstack:123456@controllernotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[database]connection = mysql+pymysql://neutron:123456@controller/neutron[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[nova]auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = 123456:[oslo_concurrency]lock_path = /var/lib/neutron/tmp 21、配置网络二层插件（controller） 1234567891011121314151617181920212223编辑 /etc/neutron/plugins/ml2/ml2_conf.ini 文件：[ml2]type_drivers = flat,vlantenant_network_types =mechanism_drivers = linuxbridgeextension_drivers = port_security[ml2_type_flat]flat_networks = provider[securitygroup]enable_ipset = true配置 Linux 网桥，编辑 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 文件：[linux_bridge]physical_interface_mappings = provider:ens33[vxlan]enable_vxlan = false[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 将其设置为1 12345678910111213141516171819202122232425262728293031323334353637配置 DHCP 服务编辑 /etc/neutron/dhcp_agent.ini 文件:[DEFAULT]interface_driver = linuxbridgedhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = true配置 metadata，编辑 /etc/neutron/metadata_agent.ini 文件:[DEFAULT]nova_metadata_host = controllermetadata_proxy_shared_secret = 123456配置计算服务使用网络服务，编辑 /etc/nova/nova.conf 文件:[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456service_metadata_proxy = truemetadata_proxy_shared_secret = 123456创建服务软链接:ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini同步数据库:/bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron重启 compute API 服务:systemctl restart openstack-nova-api.service配置网络服务开机启动:systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.servicesystemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service 22、配置compute节点网络服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950yum install openstack-neutron-linuxbridge ebtables ipset配置公共组件，编辑 /etc/neutron/neutron.conf 文件：[DEFAULT]auth_strategy = keystonetransport_url = rabbit://openstack:123456@controller[keystone_authtoken]auth_uri = http://controller:5000auth_url = http://controller:35357memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = 123456[oslo_concurrency]lock_path = /var/lib/neutron/tmp配置 Linux 网桥，编辑 /etc/neutron/plugins/ml2/linuxbridge_agent.ini 文件：[linux_bridge]physical_interface_mappings = provider:ens33[vxlan]enable_vxlan = false[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver配置计算节点网络服务，编辑 /etc/nova/nova.conf 文件：[neutron]url = http://controller:9696auth_url = http://controller:35357auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = 123456重启 compute 服务：systemctl restart openstack-nova-compute.service设置网桥服务开机启动：systemctl enable neutron-linuxbridge-agent.servicesystemctl start neutron-linuxbridge-agent.service 23、安装horizon组件（controller） 1234567891011121314151617181920212223242526272829303132333435363738394041424344yum install openstack-dashboard -y编辑 /etc/openstack-dashboard/local_settings 文件：OPENSTACK_HOST = &quot;controller&quot;ALLOWED_HOSTS = [&apos;*&apos;]###配置 memcache 会话存储###SESSION_ENGINE = &apos;django.contrib.sessions.backends.cache&apos;CACHES = &#123; &apos;default&apos;: &#123; &apos;BACKEND&apos;: &apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;: &apos;controller:11211&apos;, &#125;&#125;###开启身份认证 API 版本 v3###OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/v3&quot; % OPENSTACK_HOST###开启 domains 版本支持###OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True###配置 API 版本####OPENSTACK_API_VERSIONS = &#123; &quot;identity&quot;: 3, &quot;image&quot;: 2, &quot;volume&quot;: 2,&#125;OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &apos;Default&apos;OPENSTACK_NEUTRON_NETWORK = &#123; &apos;enable_router&apos;: False, &apos;enable_quotas&apos;: False, &apos;enable_distributed_router&apos;: False, &apos;enable_ha_router&apos;: False, &apos;enable_lb&apos;: False, &apos;enable_firewall&apos;: False, &apos;enable_***&apos;: False, &apos;enable_fip_topology_check&apos;: False,#修改时，注意底部采用原有的“&#125;”，重复会无法重启 web 服务TIME_ZONE = &quot;Asia/Shanghai&quot;重启 web 服务和会话存储:systemctl restart httpd.service memcached.service 前端访问： 访问 http://192.168.40151/dashborad 查看 openstack 的 web 页面： 如果报错： 在WSGISocketPrefix run/wsgi 下添加： **WSGIApplicationGroup %{GLOBAL}**然后重启 24、安装、配置cinder 在controller节点上，配置数据库，创建cinder数据库、和前面一样，创建授权用户 加载admin-user环境变量，并创建identity服务凭据（123456）： 12source admin-openrc openstack user create --domain default --password-prompt cinder 1234将admin role赋予cinder用户和service projectopenstack role add --project service --user cinder admin创建cinder和cinder v2服务实体（最好是创建v2 v3）openstack service create --name cinder --description &quot;OpenStack block storage&quot; volume 1openstack service create --name cinderv2 --description &quot;OpenStack block storage&quot; volumev2 12创建API endpointopenstack endpoint create --region RegionOne volume public http://controller:8776/v1/%\(tenant_id\)s 1openstack endpoint create --region RegionOne volume internal http://controller:8776/v1/%\(tenant_id\)s 1openstack endpoint create --region RegionOne volume admin http://controller:8776/v1/%\(tenant_id\)s 1openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\(tenant_id\)s 1openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\(tenant_id\)s 1openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\(tenant_id\)s 安装和配置cinder组件 1yum install -y openstack-cinder 编辑/etc/cinder.conf文件： 配置数据库连接： 配置rabbitmq： 配置认证服务： 配置节点管理IP地址： 配置锁路径： 将cinder服务信息同步到数据库（忽略输出中不推荐的信息）： 1/bin/sh -c &quot;cinder-manage db sync&quot; cinder 编辑nova.conf： 服务启动： 123systemctl restart openstack-nova-apisystemctl enable openstack-cinder-api openstack-cinder-schedulersystemctl restart openstack-cinder-api openstack-cinder-scheduler 25、安装和配置存储节点（cinder） 安装并启动lvm2 创建逻辑卷和卷组 123456789101112pvcreate /dev/sdbvgcreate cinder-volumes /dev/sdb``` cinder块存储卷一般只能被虚拟机实例访问使用，但是存储节点操作系统可以管理包括磁盘在内的本地硬件设备，操作系统中的LVM卷扫描工具可以扫描/dev目录下的所有设备，如果项目在他们的卷上使用LVM，扫描工具检测到这些卷时会尝试缓存它们，可能会在底层操作系统和项目卷上产生各种问题。您必须重新配置LVM，让它只扫描包含``cinder-volume``卷组的设备。编辑``/etc/lvm/lvm.conf``文件并完成下面的操作：![GcNM2n.png](https://s1.ax1x.com/2020/04/07/GcNM2n.png)每个过滤器组中的元素都以``a``开头，即为 accept，或以 r 开头，即为**reject**，并且包括一个设备名称的正则表达式规则。过滤器组必须以``r/.*/``结束，过滤所有保留设备。您可以使用 :命令:`vgs -vvvv` 来测试过滤器。* 安装组件 yum install openstack-cinder targetcli python-keystone -y 12345678910111213141516171819* 编辑配置文件：配置数据库：![GcNwx1.png](https://s1.ax1x.com/2020/04/07/GcNwx1.png)配置MQ：![GcNfxI.png](https://s1.ax1x.com/2020/04/07/GcNfxI.png)[![GcNORs.png](https://s1.ax1x.com/2020/04/07/GcNORs.png)](https://imgchr.com/i/GcNORs)配置认证：![GcUFJJ.png](https://s1.ax1x.com/2020/04/07/GcUFJJ.png)![GcUVQ1.png](https://s1.ax1x.com/2020/04/07/GcUVQ1.png)配置my_ip：![GcU4fJ.png](https://s1.ax1x.com/2020/04/07/GcU4fJ.png)配置lvm：![GcaMn0.png](https://s1.ax1x.com/2020/04/07/GcaMn0.png)![GcaQBV.png](https://s1.ax1x.com/2020/04/07/GcaQBV.png)配置镜像位置：![GcaGh4.png](https://s1.ax1x.com/2020/04/07/GcaGh4.png)配置锁路径：![Gcd9C4.png](https://s1.ax1x.com/2020/04/07/Gcd9C4.png)* 配置启动： systemctl enable openstack-cinder-volume.service target.service systemctl start openstack-cinder-volume.service target.service 1234567* 验证：source admin-openrc[![GcdeUO.png](https://s1.ax1x.com/2020/04/07/GcdeUO.png)](https://imgchr.com/i/GcdeUO)如果报错：ERROR: publicURL endpoint for volumev3 service not found 在admin-openrc中加入： export OS_VOLUME_API_VERSION=2 1加载demo脚本并创建一个1G的卷： source demo-openrc cinder create --name test1 1 ![GcdRG4.png](https://s1.ax1x.com/2020/04/07/GcdRG4.png) 然后查看所创建的卷： ![GcdLJe.png](https://s1.ax1x.com/2020/04/07/GcdLJe.png)]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openstack（一）]]></title>
    <url>%2F2020%2F03%2F09%2Fopenstack%2F</url>
    <content type="text"><![CDATA[关于openstack的服务组件 1、keystone：为各服务组件提供用户认证和权限验证功能 2、ceilometer：为各服务组件提供监控、检索和计量功能 3、horizon：为用户操作各服务组件提供基于web形式的图形界面 4、glance：为虚拟机实例提供镜像服务，同时，glance服务中的镜像介质存放在swift中 5、neutron：为虚拟机实例提供网络连接服务，同时也为ironic提供PXE网络 6、ironic：提供物理机的添加、删除、电源管理和安装部署等功能 7、Nova：为虚拟机实例提供计算资源 8、trove：为虚拟机镜像提供注册服务；使用Nova启动数据库实例；依附虚拟机实例，提供数据存储、操作和管理；可以备份数据库实例到swift中。 9、cinder：为虚拟机实例提供块设备，同时备份块设备数据到swift中 10、Sahara：通过heat编排集群配置；在swift中保存数据或二进制文件，将任务分派给虚拟机实例处理，通过Nova运行数据处理实例。在glance中注册Hadoop。 11、heat：可以编排cinder、neutron、glance和Nova各种资源 创建云主机的流程 1、界面或者是命令行通过restful API向keystone获取认证信息。 2、keystone通过用户请求认证信息，并生成auth-token返回给对应的认证请求。 3、界面或者是命令行通过restful API向nova-api发送一个boot instance的请求（携带上认证生成auth-token），包含所6要创建的虚拟机的信息，如CPU、内存、硬盘、网络等。 4、nova-api收到请求后向keystone发送请求去验证是否是有效的用户和token。 5、keystone返回其验证结果。 6、通过认证后nova-api和数据库进行通信。 7、数据库记录新建云主机的信息。 8、nova-api通过rpc.call向nova-scheduler请求是否有创建虚拟机的资源，让其进行资源的调度。 9、nova-scheduler监听MQ队列中的消息，然后从中获取到请求。 10、nova-scheduler通过查询nova数据库中计算资源的情况，并通过调度算法计算符合虚拟机创建的节点。 11、对于有符合虚拟机创建的主机，nova-scheduler更新数据库中虚拟机对应的物理主机的信息。 12、nova-scheduler通过rpc-cast向nova-compute发送对应的创建虚拟机请求的消息，让它去负责创建VM。 13、nova-compute从对应的消息队列中获取创建虚拟机请求的消息。 14、nova-compute通过rpc.call向nova-conductor请求获取虚拟机消息（flavor）。 15、nova-conductor从MQ中获取到请求消息。 16、nova-conductor查询虚拟机对应的信息。 17、nova-conductor从数据库获取虚拟机对应信息。 18、nova-conductor把虚拟机信息发给MQ。 19、nova-compute从MQ中获取信息。 20、nova-compute向keystone请求token，并通过HTTP请求向glance-api获取虚拟机所需要的镜像 21、glance-api向keystone验证该token是否有效。 22、通过验证后，nova-compute获取到该镜像的信息（URL）。 23、nova-compute向keystone请求token，并通过HTTP请求neutron-server获取创建虚机所需要的网络信息。 24、neutron-server收到后，向keystone验证。 25、验证通过后，nova-compute得到一个网络信息。 26、nova-compute向keystone请求token，用来向cinder获取相应的持久化存储信息。 27、cinder向keystone验证，并返回结果。 28、验证通过后，nova-compute获得该信息。 29、nova-compute根据instance的信息调用配置的虚拟化驱动来创建虚拟机。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ipv6]]></title>
    <url>%2F2020%2F03%2F09%2Fipv6-1%2F</url>
    <content type="text"><![CDATA[开启IPV6 123456789101112加载ipv6模块：modprobe ipv6vi /etc/modprobe.d/disable_ipv6.conf#alias net-pf-10 off #注释掉options ipv6 disable=0 #修改为0vi /etc/sysctl.confnet.ipv6.conf.all.disable_ipv6 = 0net.ipv6.conf.default.disable_ipv6 = 0net.ipv6.conf.lo.disable_ipv6 = 0sysctl -p 这时重启网络后会生成链路本地地址（Link-local address），如图： 配置IPV6地址 手动配置 1ifconfig eth1 inet6 add 2001:470:18:ac4::2/64 自动获取 centos 1234567891011121314151617vim /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=noneDEFROUTE=yesDEVICE=eth0DHCPV6C=yesTYPE=EthernetPEERDNS=yesPEERROUTES=yesIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noNAME=eth0ONBOOT=yes ubuntu14/16 1234vim /etc/network/interfaces.d/eth0.cfg或者/etc/network/interfacesauto eth0iface eth0 inet6 dhcp ubuntu18 123456vim /etc/netplan/50-cloud-init.yamlnetwork: version: 2 ethernets: ens3: dhcp6: true debian 1234vim /etc/network/interfaceiface eth0 inet6 autoup sleep 5up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.eth0.leases -v eth0 || true OpenSUSE42.3 1234567Vim /etc/sysconfig/network/ifcfg-eth0BOOTPROTO=dhcp6IPV6INIT=yesDHCPV6C=yesDHCLIENT6_MODE=managedSTARTMODE=auto 注：也可以通过dhclient -6 -d [ 网卡 ]来获取 ipv6镜像在openstack平台上相关问题 最近在openstack平台上进行镜像测试的时候，发现在ipv6网络下的一些问题。 ipv6的网络无法下发网卡配置 像centos8，openSUSE42.3镜像在ipv6的网络下，cloud-init没有下发相应的配置，导致一直获取不到ipv6的地址。通过修改cloud-init网络部分的源码来解决（/usr/lib/python2.7/site-packages/cloudinit/net/sysconfig.py）。 但是对于cloud-init是低版本的而言，没有这个文件，最后解决办法是在/var/lib/cloud/scripts/per-instance路径下，添加一个脚本，在初始化每个实例的时候才会去执行这些脚本，并且脚本只有在初始网络是ipv6的情况下才会去执行： 1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash###centosfunction update_eth0()&#123; echo &quot;TYPE=Ethernet&quot; &gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;BOOTPROTO=dhcp6&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;DEFROUTE=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;PEERDNS=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;PEERROUTES=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6INIT=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6_AUTOCONF=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6_DEFROUTE=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6_PEERDNS=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6_PEERROUTES=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;IPV6_FAILURE_FATAL=no&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;DHCPV6C=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;NAME=eth0&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;DEVICE=eth0&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0 echo &quot;ONBOOT=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0&#125;function update()&#123; mount /dev/sr0 /mnt cat /mnt/openstack/latest/network_data.json | python -m json.tool &gt; /tmp/file1 NETWORK_TYPE=`cat /tmp/file1 | grep -i type|awk -F&quot;:&quot; &apos;&#123;print $2&#125;&apos;|sed -n 2p|sed &apos;s/\&quot;//g&apos;` if [ $NETWORK_TYPE == &apos;ipv6_dhcpv6-stateful&apos; ];then update_eth0 service network restart fi umount /mnt rm -rf /tmp/file1 &#125;update 因为这边都是通过ipv6有状态方式去下放ip地址，所以判断条件就写死。 没有获取到默认路由，导致无法通信 查看ipv6的路由表，发现缺少默认路由： 注：图中标红为正常主机应有现象，迭代前缺少，导致无法通信 此时： 1234567891011vim /etc/sysctl.confnet.ipv6.conf.default.accept_ra = 1net.ipv6.conf.all.accept_ra = 1net.ipv6.conf.eth0.accept_ra = 1sysctl -p重启主机注：此时允许接收ra报文，因为默认路由消息是在ra报文里面]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IPV6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ipv6（一）]]></title>
    <url>%2F2020%2F03%2F08%2Fipv6%2F</url>
    <content type="text"><![CDATA[ipv6动态主机配置协议DHCPv6是针对ipv6编制方案设计，为主机分配ipv6地址/前缀和其他网络配置参数。 三种角色 DHCPV6 CLIENT 通过dhcpv6服务器进行交互，获取ipv6地址/前缀和网络配置信息，完成自身的地址配置功能。 DHCPV6 RELAY dhcpv6的中继代理，负责转发来自客户端或服务器方向的dhcpv6报文，协助dhcpv6客户端和dhcpv6服务器完成地址配置功能。一般情况下，dhcpv6客户端通过本地链路范围的组播地址与dhcpv6服务器通信，以获取ipv6地址/前缀和其他网络配置参数。如果服务器和客户端不在同一个链路范围内，则需要通过dhcpv6中继代理来转发报文，这样可以避免每个链路范围内都部署dhcpv6服务器，实现了集中管理。另外就是，这个中继代理不是必须的，如果dhcpv6的client和server位于同一个链路内，或者直接通过单播地址交互完成地址分配，那么这个时候就可以不需要它了。 DHCPV6 SERVER 负责处理来自客户端或中继代理的地址分配，地址续租，地址释放等请求，为客户端分配ipv6地址/前缀和其他网络配置信息。 相关概念 1、组播地址 在dhcpv6协议中，客户端不用配置dhcpv6 server的ipv6地址，而是发送目的地址为组播地址的solicit报文来定位dhcpv6服务器。所用到的组播地址有两个： • ff02::1:2：所有dhcpv6服务器和中继代理的组播地址，这个地址是链路范围的，用于客户端和相邻服务器及中继之间通信。所有的dhcpv6和中继代理都是该组的成员 • ff02::1:3：所有dhcpv6服务器组播地址，这个地址是站点范围的，用于中继代理和服务器支架你的通信，站点内的所有dhcpv6服务器都是成员。 2、端口号 dhcpv6报文承载在UDPv6上，客户端监听的UDP目的端口号是546，服务器、中继代理监听的UDP端口号是547。 3、DUID 每个服务器或者客户端有且只有一个唯一标识符，服务器使用DUID来识别不同的客户端，客户端则使用DUID来识别服务器。客户端和服务器的DUID的内容分别通过dhcpv6报文中的client identifier和server identifier选项来携带，两种选项的格式一样，通过option-code字段的取值来区分。 4、M/O标记（RA报文中） ipv6自动执行无状态自动配置，并在相邻路由器发送的路由器公告消息中使用基于以下标记的配置协议：托管地址配置标记，也称为M标记，路由器通告的RA报文中的管理标记，当M标记为1时，表示链路上的iPv6主机采用dhcpv6方式获取ipv6地址/前缀。其他有状态配置标记，也称为O标记，路由器通告的RA报文中的其他配置标记，当O标志位为1时，表示链路上的ipv6主机采用dhcpv6方式获取ipv6地址/前缀以外的其他网络配置参数。 • 当M和O标记均设置为0 的时候，此时不具有dhcpv6基础结构的网络，主机使用非链接本地地址的路由器公告以及其他方法（手动配置）来配置其他设置。 • M和O均设置为1，dhcpv6用于这两种地址（链接本地地址和其他非链接本地地址）和其他配置设置。该组合成为dhcpv6有状态，其中dhcpv6将有状态地址分配分配给ipv6主机。 • M设置为0，O设置为1，dhcpv6不用于分配地址，仅用来分配其他配置设置。相邻路由器配置为通告非链路本地地址前缀，ipv6主机从中生成无状态地址。此组合叫做dhcpv6无状态；只分配无状态配置设置。 • M标记设置为1，O设置为0，这时dhcpv6用于地址配置，但不用于其他设置。 5、dhcpv6报文 报文类型 dhcpv6报文 dhcpv4报文 说明 1 SOLICIT DHCP DISCOVER 客户端使用solicit报文来确定dhcpv6服务器的位置 2 ADVERTISE DHCP OFFER 服务器通过发送advertise报文来对solicit报文进行回应，宣告自己的存在 3 REQUEST DHCP REQUEST 客户端发送request报文来向dhcpv6服务器请求ipv6地址和其他配置信息 4 CONFIRM - dhcpv6客户端向任意可达的dhcpv6服务器发送confirm报文检查自己目前获得的ipv6地址是否适用于它所连接的链路 5 RENEW DHCP REQUEST dhcpv6客户端向其提供地址和配置信息的DHCPv6服务器发送renew报文来延长地址的生存期并更新配置信息 6 REBIND DHCP REQUEST 如果renew报文没有得到应答，dhcpv6客户端向任意可达的dhcpv6服务器发送rebind报文来续租 7 REPLY DHCP ACK/NAK dhcpv6服务器发送携带了地址和配置信息的reply消息来回应从dhcpv6客户端收到的solicit、request，renew，rebind报文。dhcpv6服务器发送携带配置信息的reply消息来回应收到的information-request报文。用来回应dhcpv6客户端发来的confirm、release、decline报文。 8 RELEASE DHCP RELEASE DHCPv6客户端为其分配地址的dhcpv6服务器发送release报文，表明自己不再使用地址 9 DECLIEN DHCP DECLIEN DHCPv6的客户端向服务器发送decline报文，声明dhcpv6服务器分配的地址已被使用了 10 RECONFIGURE - 服务器向客户端发送reconfigure报文，用于提示dhcpv6客户端有新的网络配置在dhcpv6服务器上 11 INFORATION-REQUEST DHCP INFORM 客户端向服务器发送报文来请求除ipv6地址以外的网络配置信息 12 RELAY-FORWARD - 中继代理通过relay-forward报文向dhcpv6服务器转发dhcpv6客户端请求报文 13 RELAY-REPLY - dhcpv6服务器向中继代理发送relay-reply报文，携带转发给客户端的报文 6、工作方式 • dhcpv6有状态自动分配（stateful）。DHCPv6服务器自动配置ipv6地址/前缀，同时分配DNS、NIS、SNTP服务器等网络配置参数 • dhcpv6无状态自动分配（SLAAC）。iPv6地址仍然通过路由通告方式自动生成，dhcp服务器只分配除ipv6地址以外的配置。 有状态自动分配（stateful） dhcpv6服务器为客户端分配地址/前缀的过程分为两类： • 四步交互分配过程 四步交互常用于网络中有多个dhcpv6服务器的情况，dhcpv6客户端首先通过组播发送solicit报文来定位可以为其提供服务dhcpv6服务器，在收到多个服务器的advertise报文后，根据dhcpv6服务器的优先级选择一个为其分配地址和配置信息的服务器，接着通过request/reply报文交互完成地址申请和分配过程。 dhcpv6服务器端如果没有配置使能两步交互，无论客户端报文中是否包含rapid commit选项，服务器都采用四步交互方式为客户端分配地址和配置信息。 (1) 服务器首先发送solicit报文，请求dhcpv6服务器为其分配ipv6地址和网络配置参数 (2) 如果solicit报文中，没有携带rapid commit选项，或者服务器不支持快速分配的选项，则服务器回复advertise报文，通过客户端我可以给你分配地址和配置参数 (3) 如果客户端收到多个服务器回复的advertise报文，则根据advertise报文中的服务器优先级等参数，选择优先级最高的一台服务器并向所有的服务器发送request组播报文，其中携带了已选择服务器的DUID。 (4) 服务器收到后，回复reply报文，将地址和网络配置信息分配给客户端使用。 • 两步交互 一般用在只有一个dhcpv6服务器的情况，dhcpv6客户端首先通过组播发送solicit报文来定位可以为其提供服务的dhcpv6服务器，服务器收到客户端的solicit报文后，为其分配地址和配置信息，直接回应reply报文，完成地址申请和分配过程。 无状态自动分配(SLAAC) 客户端以组播方式向dhcpv6服务器发送information-request报文，该报文中携带option request选项，指定客户端想要从服务器获取的配置参数。 服务器收到报文后，为客户端分配网络配置参数，并单播发送reply报文，将网络配置参数返回给客户端。 IPv6地址/前缀分配 1、选择IPv6地址池 DHCPv6服务器的接口可以绑定IPv6地址池，DHCPv6服务器将选择该IPv6地址池为接口下的DHCPv6客户端分配地址/前缀。对于存在中继的场景，DHCPv6服务器的接口可以不绑定IPv6地址池，而是根据报文中第一个不为0的“link-address”字段（标识DHCPv6客户端所在链路范围），选择与地址池中已配置的网络前缀或IPv6地址前缀属于同一链路范围的地址池。 2、选择IPv6地址/前缀 确定地址池后，DHCPv6服务器将按照下面步骤为DHCPv6客户端分配IPV6地址/前缀： 如果地址池中为客户端指定了地址/前缀，优先从地址池中选择与客户端DUID匹配的地址/前缀分配给客户端。 如果客户端报文中的IA选项携带了有效的地址/前缀，优先从地址池中选择该地址/前缀分配给客户端。如果该地址/前缀在地址池中不可用，则另外分配一个空闲地址/前缀给客户端。如果IPV6前缀长度比指定分配长度大，则按指定分配长度来分配。 从地址池中选择空闲地址/前缀分配给客户端，保留地址（例如RFC 2526中定义的任播地址）、冲突地址、已被分配的地址不能再分配给客户端。 如果没有合适的IPv6地址/前缀可以分配，则分配失败。 DAD机制 简介 地址重复检测（DAD: Duplicate Address Detection）背景要求: 节点在发送路由器公告（RA）之前要获得唯一的本地链路地址。 IPv6自动配置要求在使用地址之前进行地址重复检测（DAD）。 标准DAD过程 (1) 在发送邻居请求（NS）前，接口必须加入全节点组播地址(FF02::1)和生成IPv6地址的请求节点组播地址（solicited-node multicast address），即接收目的地址为这些IPv6地址的分组。 (2)生成IPv6地址后随机延时一段时间后开始发送用于DAD的邻居请求（NS）消息。邻居请求（NS）消息的源地址为::，目的地址为临时单播地址的请求节点组播地址。在规定时间内没有收到应答的NA报文，则认为该单播地址在链路上是唯一的，可以分配给接口，如果收到应答NA报文，则表明该地址已被其它节点使用，不能配置给接口。 (3) 在DAD过程中地址处于Tentative状态（“暂时的”）（IFA_F_TENTATIVE)。在完成DAD过程后，tenativeAddr将会被作为“首选的”地址（PreferedAddr）。若发现了重复地址，则该地址变为&quot;废弃的&quot;地址（deprecatedAddr）。 总得来说就是：未确定唯一性的地址不能使用，即不能接收目的地址或者发送源地址为此地址的分组，但是与DAD相关的邻居公告（NA）消息除外。每次系统默认一个主机在应用新的IP地址之前会发送3次DAD,如果三次以后均没有收到任何回应，那么该地址被认为是可以配置在接口上的。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IPV6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（十）]]></title>
    <url>%2F2020%2F03%2F08%2Fk8s-9%2F</url>
    <content type="text"><![CDATA[k8s监控 metric-server 要进行监控得先安装metrics，k8s通过Heapster 和 metrics - server 来获取资源的使用情况，但是自 Kubernetes 1.11版本后 heapster已经被废弃了，取而代之的是更丰富的 metrics-server。 安装： 1、下载项目 1git clone https://github.com/kubernetes-incubator/metrics-server 2、修改metrics-server/deploy/1.8+/resource-reader.yaml文件 3、修改 metrics-server/deploy/1.8+/metrics-server-deployment.yaml文件： 4、部署 12cd metrics-server/deploy/1.8+kubectl apply -f . 注：如果镜像拉取失败，则更换镜像源 1sed -i &apos;s#k8s.gcr.io#gcr.azk8s.cn/google_containers#g&apos; metrics-server/deploy/1.8+/metrics-server-deployment.yaml 如果报错： 1kubectl logs -f -n kube-system metrics-server-747bc9cc6b-62cjl 这时修改文件metrics-server-deployment.yaml，添加最后几行： 最后如果没有报错，则执行kubectl top pods查看相关数据收集和负载情况进行验证。 prometheus + grafana 1、在master进行安装部署 1git clone https://github.com/redhatxl/k8s-prometheus-grafana.git 2、在node节点下载镜像 123docker pull prom/node-exporterdocker pull prom/prometheus:v2.0.0docker pull grafana/grafana:4.2.0 3、部署node-exporter组件 1kubectl create -f k8s-prometheus-grafana/node-exporter.yaml 4、部署prometheus组件 1234kubectl create -f k8s-prometheus-grafana/prometheus/rbac-setup.yamlkubectl create -f k8s-prometheus-grafana/prometheus/configmap.yaml kubectl create -f k8s-prometheus-grafana/prometheus/prometheus.deploy.yml kubectl create -f k8s-prometheus-grafana/prometheus/prometheus.svc.yml 5、部署grafana组件 123kubectl create -f k8s-prometheus-grafana/grafana/grafana-deploy.yamlkubectl create -f k8s-prometheus-grafana/grafana/grafana-svc.yamlkubectl create -f k8s-prometheus-grafana/grafana/grafana-ing.yaml 6、此时验证一下node-exporter 7、查看prometheus的连接情况 8、然后登陆grafana进行设置（账号和密码都是admin） 注：添加数据源这里：关于URL如果9090这里报错，则直接用nodeport+暴露端口，然后选择direct 导入dashboard的时候，直接对话框输入315]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（九）]]></title>
    <url>%2F2019%2F08%2F12%2Fk8s-8%2F</url>
    <content type="text"><![CDATA[flannel 和calico网络 flannel flannel的功能简单来说就是让集群中的不同节点的主机创建的docker都具有全集群唯一的虚拟ip地址，但是无法做到pod与pod之间通信的安全隔离，如果在caas公有云上，用flannel的话两个用户的pod之间能够互相访问。 在默认的docker配置中，每个节点上的docker服务会分别负责所在节点容器的IP分配，这样导致的一个问题就是，不同节点上容器可能会获得相同的内外IP地址。所以flannel设计目的就是为集群中的所有节点重新规划IP地址使用规则，从而使得不同节点上的容器能够获得“同属一个内网，但是不重复的IP地址，并且不同节点上的容器能够直接通过内网IP通信”。 flannel实质上是一种overlay网络，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，默认是采用UDP转发。 特点： 使集群中的不同node主机创建的docker容器都具有全集群唯一的虚拟IP地址 通过overlay网络将一个分组封装在另一个分组内来将网络服务与底层基础设施分离，再将封装的数据包转发到端点后，将其解封装 创建一个新的虚拟网卡flannel0接收docker网桥的数据，通过维护路由表，对接收到的数据进行封包和转发（VXLAN） etcd保证了所有node上flannel所看到的配置是一样的，同时每个node上的flannel监听etcd的数据变化，实时感知集群中node的变化。 组件： cni0：网桥设备，每创建一个pod都会创建一对veth pair。其中一段是pod中的eth0，另一端是cni0网桥中的端口，pod中从网卡eth0发出的流量都会发送到cni0网桥设备的端口上，cni0设备获得的ip地址是该节点分配的网段的第一个地址。 flannel.1：overlay网络的设备，用来进行vxlan报文的处理（封包和拆包）。不同node之间的pod数据流量都从overlay设备以隧道的形式发送到对端。 flanneld：flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配，同时flanneld监听k8s集群数据库，为flannel.1设备提供封装数据时必要的mac，ip等网络数据信息。 通信流程： pod中产生数据，根据pod的路由信息，将数据发送到cni0。 Cni0根据节点的路由表，将数据发送到隧道设备flannel.1 Flannel.1查看数据包的目的IP，从flanneld获得对端隧道设备的必要信息，封装数据包 Flannel.1将数据包发送到对端设备。对端节点的网卡接收到数据包，发现数据包为overlay数据包，解开外层封装，并发送内层封装到flannel.1设备 Flannel.1设备查看数据包，根据路由表匹配，将数据发送到cni0设备 Cni0匹配路由表，发送数据给网桥上对应的端口。 flannel可以指定不同的转发后端网络 1、hostGW 原理很简单，直接添加路由，将目的主机当做网关，直接路由原始封包。例如，我们从etcd中监听到一个EventAdded事件subnet为10.1.15.0/24被分配给主机Public IP 192.168.0.100，hostgw要做的工作就是在本主机上添加一条目的地址为10.1.15.0/24，网关地址为192.168.0.100，输出设备为上文中选择的集群间交互的网卡即可。 优点：简单，直接，效率高 缺点：要求所有的pod都在一个子网中，如果跨网段就无法通信。 2、UDP 如果需要pod不在同一个子网内，需要将pod的网络包作为一个应用层的数据包，使用UDP封装之后再集群里传输，即overlay。 包的封装： 当容器进行跨主机通信时： 因为该封包的目的地不在本主机subnet内，因此封包会首先会通过网桥转发到主机中。 在主机上经过路由匹配，进入网卡flannel.1（flannel.1是一个三层的虚拟tun设备，而flanneld是一个proxy，它会监听flannel.1并转发流量） 当封包进入flannel.1时，flanneld就可以从flannel.1中将封包读取出，由于flannel.1是三层设备所以读取出的封包仅仅是包含IP层的报头及其负载。 最后flanneld会将获取的封包作为负载数据通过udp socket发往目的主机。 在目的主机的flanneld会监听public IP所在设备，从中读取udp封包的负载，并将其放入到flannel.1设备内。 容器网络封包到达目的主机，之后就可以通过网桥转发到目的容器了。 优点：Pod能够跨网段访问 缺点：隔离性不够，udp不能隔离两个网段。 3、vxlan 包类型： 事实上，flannel只使用了vxlan的部分功能，由于VNI被固定为1，本质上工作方式和udp backend是类似的，区别无非是将udp的proxy换成了内核中的vxlan处理模块。而原始负载由三层扩展到了二层，但是这对三层网络方案flannel是没有意义的，这么做也仅仅只是为了适配vxlan的模型。 flannel缺点： 1、不支持pod之间的网络隔离，它设计思想就是将所有的pod都放在一个大二层网络中，所以pod之间没有隔离策略。 2、设备复杂，效率不高，流量经过多种设备的封装、解析会造成传输效率的下降。 calico calico是一种容器之间互通的网络方案。在虚拟化平台中，比如OpenStack、docker等都需要实现workloads之间互联，但同时也需要对容器做隔离控制，就像在Internet中的服务仅开放80端口一样，提供隔离和管控机制。而在多数的虚拟化平台实现中，通常都使用二层隔离技术来实现容器的网络，这些二层的技术有一些弊端，比如需要依赖VLAN、bridge和隧道技术，其中bridge带来了复杂性，VLAN隔离和tunnel隧道则消耗更多的资源并对物理环境有要求。我们尝试把host当做Internet中的路由器，同样适用BGP同步路由，并使用iptables来做安全访问策略，设计出来了calico方案。 适用场景 k8s环境中的pod之间需要隔离 设计思想 calico不使用隧道或者NAT来实现转发，而是巧妙的把所有的二三层流量转换成三层流量，并通过host上路由配置完成跨host转发。 设计的优势： 1、更优的资源利用 二层网络通讯需要依赖广播消息机制，广播消息的开销与host 的数量呈指数级增长，calico使用的三层路由方法，则完全抑制了二层广播，减少了资源开销。 另外，二层使用VLAN隔离技术，天生有数量限制，即使用vxlan解决，但是会带来隧道开销问题。calico会使资源利用率更高。 2、可扩展性 calico使用与Internet类似的方案，它的网络比任何数据中心都大，calico同样天然具有可扩展性。 3、更容易debug 因为没有隧道，所以意味着workloads之间路径更短更简单，配置更少，在host上更容易进行debug调试。 4、更少的依赖 仅依赖三层路由可达 架构 工作组件 1、felix：运行在每一台host的agent进程，主要负责网络接口管理和监听、路由、ARP管理、ACL管理和同步、状态上报等。 它会监听etcd中心的存储，从它获取事件，比如说用户在这台机器上加了一个IP，或者是创建了一个容器等等。用户创建pod后，felix负责将其网卡、IP、MAC都设置好，然后在内核的路由表里面写一条，注明这个IP应该到这张网卡，同样如果用户制定了隔离策略，felix同样将该策略创建到ACL中，以实现网络隔离。 2、etcd：分布式键值存储，主要负责网络元数据一致性，确保calico网络状态的准确性，可以与k8s共用。 3、BGP client（BIRD）：calico为每一台host部署了一个BGP client，使用BIRD实现，BIRD是一个单独的项目，实现了众多动态路由协议比如BGP、OSPF、RIP等。在calico的角色是监听host上由felix注入的路由信息，然后通过BGP协议广播告诉剩余节点，从而实现网络互通。 从内核里面获取哪一些IP的路由发生变化，然后通过标准BGP的路由协议扩散到整个其他的宿主机上，让外界都知道这个IP在这里，你们路由的时候到这里来。 4、BGP router reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。 架构特点： 由于calico是一种纯三层的实现，因此可以避免与二层方案相关的数据包封装的操作，中间没有任何NAT，没有任何的overlay，所以他的转发效率可能是所有方案中最高的。因为它的包直接走原生的TCP/IP协议栈，它的隔离也因为这个栈而变得好做。因为TCP/IP协议栈提供了一整套的防火墙的规则，所以他可以通过iptables的规则达到比较复杂的隔离逻辑。 node之间的两种网络： 1、ipip 从字面意思来说就是把一个IP数据包又套在一个IP包里面，即把IP层封装到IP层的一个tunnel。它的作用其实基本上就相当于一个基于IP层的网桥。不同于传统网桥是基于mac层的，ipip通过两端的路由做一个tunnel，把两个本来不通的网络通过点对点连接起来了。 2、BGP]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（八）]]></title>
    <url>%2F2019%2F08%2F12%2Fk8s-7%2F</url>
    <content type="text"><![CDATA[数据管理 pod和容器是短暂的，具有很短的生命周期，会被频繁的创建和销毁，容器销毁的时候，保存在容器内部的文件系统的数据也会被销毁。 为了持久化保存数据，可以使用kubernetes volume。本质上，它是一个目录，和docker类似，当它被mount到pod中的时候，pod中的所有容器都可以访问这个volume。它支持多种backend，像emptyDir，hostPath，NFS，Ceph等，它提供了各种backend的抽象，在使用的时候不需要关心数据是存放在本地还是云硬盘上。 1、emptyDir是最基础的volume类型，一个emptyDir volume是host主机上的一个空目录，它对于容器来说是持久化的，对于pod来说则不是，当pod从节点删除的时候，volume的内容也会被删除，但如果只是容器被销毁而pod还在，则volume还在，也就是说它的生命周期和pod保持一致。pod内的容器都可以共享mount，他们可以自定义各自的mount路径。 比如： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: producer-consumerspec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c - echo &quot;hello world&quot; &gt; /producer_dir/hello; sleep 30000; - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000; volumes: - name: shared-volume emptyDir: &#123;&#125; 这里我们模拟了一个producer-consumer的场景，pod中存在两个容器，他们共享一个volume。producer负责写数据，consumer负责读数据。最下面定义了一个emptyDir类型volume： producer容器将shared-volume mount到producer_dir目录 producer通过echo向其写数据 consumer容器将其mount到consumer_dir目录 consumer读取数据 我们可以通过kubectl logs查看： 然后kubectl describe查看一下： 从上面两张图可以看出，他们共享一个volume。 因为emptyDir是docker host文件系统的目录，相当于执行了docker run -v producer_dir和docker run -v consumer_dir。emptyDir是一个临时目录，其优点是能够方便的为pod内的容器临时提供共享存储，不需要额外的配置，它不具备持久性。 2、hostPath hostPath是将docker host文件系统中已经存在的目录mount给容器。大部分应用都不会使用它，因为这个增加了pod与节点的耦合，限制了pod的使用。不过那些需要访问k8s或者docker内部数据的应用则需要使用hostPath（比如kube-apiserver和kube-controller-manager） 3、pv和pvc persistentVolume（PV）和persistentVolumeClaim（PVC）是k8s提供的两种API资源，用于抽象存储细节，管理员关注于如何通过PV提供存储功能而无需关注用户如何使用，同样的，用户只需要挂载PVC到容器中而不需要关注存储卷采用何种技术实现。 PVC和PV的关系与pod和node关系类似，前者消耗后者的资源。PVC可以向PV申请指定大小的存储资源并设置访问模式。 PV和PVC遵循以下声明周期： 供应准备：通过集群外的存储系统或者云平台来提供存储持久化支持。 静态提供：管理员手动创建多个PV，供PVC使用 动态提供，动态提供：动态创建PVC特定的PV，并绑定 绑定：用户创建PVC并指定需要的访问资源的访问模式，在找到可用的PV之前 ，PVC会保持未绑定状态。 使用：用户可在pod中像volume一样使用PVC 释放：用户删除PVC来回收存储资源，PV将变成released状态，由于还保留着之前的这些数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他PVC使用。 回收：PV可以设置三种策略，(a)保留（Retain），允许人工处理保留的数据；(b)回收（Recycle），将执行清除策略，之后可以被新的PVC使用，需要插件支持；©删除（Delete），将删除PV和外部关联的存储资源 目前只有NFS和hostpath类型支持回收策略。 两种方式提供的PV资源供给： 1、static 通过集群管理者创建多个PV，为集群使用者提供存储能力而隐藏真实存储的细节，并且存在于kubernetes api中，可被直接使用。 2、dynamic 动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储卷，在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建。 存储卷成功之后再创建PV对象，才能够再kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。 要使用pv/pvc的存储方式，就必须要有一个共享的文件目录，让k8s集群内的所有服务器全部都可以访问到，所以先搭建一个nfs服务。 1、搭建nfs 2、创建PV的配置文件 123456789101112131415apiVersion: v1kind: PersistentVolumemetadata: name: pv-test labels: pv: pv-testspec: capacity: storage: 1Gi //指定pv的容量 accessModes: //指定访问的模式 - ReadWriteMany storageClassName: nfs //配置nfs的信息 nfs: path: /home/nfs_share server: 192.168.154.128 其中： accessModes：指定访问模式，支持三种：ReadWriteOnce，PV以read-write挂载到单个节点上；ReadOnlyMany，以read-only的方式挂载到多个节点上；ReadWriteMany，以read-write的方式挂载到多个节点上。 persistentVolumeReclaimPolicy：指定PV的回收策略； storageClassName：指定PV的class，相当于为PV设置了一个分类，PVC可以指定class申请相应class的PV 默认的回收策略为reatain（保留），状态为available，是因为还没有绑定到任何的PVC上面，当定义好pvc以后，两个就可以进行绑定。 3、创建PVC 1234567891011121314apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mypvcspec: accessModes: - ReadWriteMany resources: requests: storage: 200Mi storageClassName: nfs //绑定PVC和PV selector: matchLabels: pv: pv-test 通过绑定PVC和PV，表明这个PVC希望使用 storageClassName: nfs 的PV。PVC向PV申请存储空间，并且通过selector指定与label名为nfs-pv的pv相匹配。 此时在查看： 4、将PVC挂载到相应的容器里面 12345678910111213141516apiVersion: v1kind: Podmetadata: name: httpd-pvpodspec: containers: - image: httpd name: httpd-withpvc-pod imagePullPolicy: Always volumeMounts: - mountPath: &quot;/usr/local/apache2/htdocs/&quot; name: httpd-volume volumes: - name: httpd-volume persistentVolumeClaim: claimName: mypvc 5、验证 创建共享文件，并进入到容器查看：]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es（二）]]></title>
    <url>%2F2019%2F08%2F08%2Fes-1%2F</url>
    <content type="text"><![CDATA[将zabbix数据导入到ES数据库中 1、 实验环境 Centos7、Zabbix（4.0.7）、ES数据库（5.0.2） 2、 配置ES数据库中支持的zabbix监控项类型（对应如下图）： 为每个类型添加相应的ES mapping，命令为： 1curl -H &quot;Content-Type:application/json&quot; -XPUT http://localhost:9200/uint -d &apos;&#123;&quot;settings&quot;:&#123;&quot;index&quot;:&#123;&quot;number_of_replicas&quot;:1,&quot;number_of_shards&quot;:5&#125;&#125;,&quot;mappings&quot;:&#123;&quot;values&quot;:&#123;&quot;properties&quot;:&#123;&quot;itemid&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;clock&quot;:&#123;&quot;format&quot;:&quot;epoch_second&quot;,&quot;type&quot;:&quot;date&quot;&#125;,&quot;value&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;&#125;&#125;&#125;&#125;&apos; 3、 修改zabbix server配置文件（/etc/zabbix/Zabbix_server.conf） （1） 历史数据保存路径（HistoryStorageURL） 1HistoryStorageURL=127.0.0.1:9200 //本路径为本地ES数据库的路径 （2） 历史数据保存格式（HistoryStorageTypes） 1HistoryStorageTypes=uint,dbl,str,log,text 4、 修改zabbix前端配置文件（/etc/zabbix/web/zabbix.conf.php），添加： 12$HISTORY[&apos;url&apos;] = &apos;http://127.0.0.1:9200&apos;;$HISTORY[&apos;types&apos;] = [&apos;uint&apos;,&apos;dbl&apos;,&apos;str&apos;,&apos;log&apos;,&apos;text&apos;]; 5、 重启服务 12systemctl restart httpdsystemctl restart Zabbix-server 6、验证 在kibana上查看： 此外，还可以通过zabbix 的api；filebeat+logstash来实现。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es（一）]]></title>
    <url>%2F2019%2F08%2F08%2Fes%2F</url>
    <content type="text"><![CDATA[Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；可以处理PB级别的数据。 1、工作原理 当ES的节点启动以后，它会利用多播寻找集群中的其他节点，并与之建立起连接： 2、相关概念 集群 ES可以作为一个独立的单个搜索服务器，不过为了处理大量的数据集，实现容错和高可用性，ES可以运行在许多相互合作的服务器上，这些服务器的集合叫做集群。 节点 形成服务器集群的每个服务器叫做节点，它存储你的数据，并参与集群的索引和搜索。和集群一样，节点也是通过唯一的名字去区分，默认名字是一个随机的UUID，当服务器启动的时候就会设置到节点。当然也可以自定义节点的名称。名称对于管理员来说十分重要，它可以帮助你辨认出集群中的各个服务器与哪个节点相对应。 节点通过配置集群的名称，就可以加入到集群中。默认，节点都加入到一个叫elasticsearch的集群，这意味着如果你在网络中启动了大量的节点，并且假如他们都能相互通讯的话，那么他们将会被自动的加入到一个叫elasticsearch集群。 分片（shard） 当有大量文档的时候，由于内存的限制，磁盘处理能力不足，无法足够快的响应客户端的请求等，一个节点可能不够，这种情况下，数据可以分成较小的分片，每个分片放在不同的服务器上，当你查询的索引分布在多个分片上的时候，ES会把查询发送给相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。 副本 为提高查询吞吐量或实现高可用，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有一个或者多个副本，es中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。 全文检索 全文检索就是对一篇文章进行索引，可以根据关键字进行搜索，它把内容根据词的意义进行分词，然后分别创建索引。 索引 索引 （index）是许多文档的集合，这些文档都具备一些相似的特征。 类型 类型是索引的一个逻辑分类或者划分。 文档 文档是可以被索引的基本单位。文档使用一种互联网广泛存在的数据交换格式保存（json），尽管文档本质是存放在索引中，但实际是被索引到一个索引中的一个类型中。 3、与mysql的对比（可能并不准确，为了便于理解） 4、索引的基本操作： 新建一个index可以直接向elasticsearch服务器发出PUT请求。比如： 1curl -X PUT &apos;localhost:9200/weather&apos; 创建成功以后会返回一个json： 如果加上一个pretty表示美化json格式的返回结果： 此时查询索引： 1curl -X GET &apos;localhost:9200/_cat/indices?v&apos; 可以看见我们刚才创建的index，但是状态是yellow，这是因为还有一个复制分片没有下发还没有被分发，就是说es默认在索引中创建一个复制分片，因为这个时候我们仅仅运行一个节点，并且在没有新的节点加入集群之前，这个复制分片是不可以被分发的（复制分片和主分片不能在同一个节点上）。这个复制分片一旦被分配到第二节点中，集群的健康状态就会变成green。 其中各字段表示的意义： index：索引名 UUID：UUID值 pri：主分片数 rep：副本分片数 docs.count：文档数 docs.deleted：删除文档数 store.size：全部分片存储大小 pri.strore：主分片存储大小 删除索引： 1curl -X DELETE &apos;localhost:9200/weather&apos; 5、文档基本操作 创建文档： 创建一个文档，我们必须告诉es，要把文档保存到索引的哪个类型中。比如将一个简单的文档索引到customer索引的external类型中，并把文档的ID设置为1： 需要说明的是，如果你在创建一个文档之前，索引不存在，它会自动给你创建。然后用GET查询： 查看所有已经创建好的文档： 1curl -X GET &apos;localhost:9200/weather/external/_search?pretty&apos; total：返回的记录数 max_score：最高的匹配程度 hits：返回的记录组成的数据 6、索引/替换文档 如果我们创建已经存在的ID的文档，则新的这个文档会替换以前的文档，索引新文档的时候，如果没有指定一个id，es会随机为文档生成一个随机的id，实际生成的id将会保存在调用api接口的返回结果中。此时，如果索引文档没有指定id，我们需要用POST方法取代之前的 PUT方法。 7、更新文档 es更新文档在底层是删除旧的文档然后索引新的文档，比如先创建好一个dzl的文档，只有name一个字段，然后更新： 此时增加了一个新的字段。 8、批处理 通过_bulk API批量执行，它会使用一种非常有效的机制去执行多个操作，使其能够尽可能快并且减少网络往返次数。比如通过一个bulk操作索引两个文档： 其中，每两行构成一次操作，第一行是操作类型：可以是index，create，update或者是delete。第二行就是可选的数据体，不同的操作类型，第二行里面的可选的数据体是不一样的： index 和 create 第二行是source数据体 delete 没有第二行 update 第二行可以是partial doc，upsert或者是script 需要注意的是，每一行都必须要有一个换行符，所以json格式只能在一行里面不能使用格式化后的内容。 关于_bulk的请求路径： （1） /_bulk （2）/{index}/_bulk （3）/{index}/{type}/_bulk 如果提供了index和type那么数据体里面的action就可以不提供，缺少type需要在数据体里面去添加。 此外，还有几个参数可以用来控制一些操作： （1）数据体里面可以使用_version字段 （2）数据体里面可以使用_routing字段 （3）可以设置wait_for_active_shards参数，数据拷贝到多个shard之后才进行bulk操作 （4）refresh控制多久间隔多搜索可见 删除操作没有第二行数据体，因为他只需要知道文档的id就行。 如：{“delete”:{“_id”:“2”}}’ 批量API会按顺序的执行所有的操作，如果某个操作因为某些原因执行失败，它会继续执行剩下的操作。api返回结果的时候，会提供每一个操作的状态。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（七）]]></title>
    <url>%2F2019%2F08%2F07%2Fk8s-6%2F</url>
    <content type="text"><![CDATA[关于k8s的健康检查 强大的自愈能力是k8s这类容器编排引擎的一个重要特性。自愈的默认实现方式是自动重启发生故障的容器。除此之外，用户还可以利用liveness和readiness探测机制设置更精细的健康检查，进而实现以下需求： 零停机部署 避免部署无效的镜像 更加安全的滚动升级 默认的健康检查： 每个容器启动的时候都会执行一个进程，此进程由dockerfile的CMD或者是ENTRYPOINT指定。如果进程退出时返回码非零，则认为容器发生故障，k8s会根据restartPolicy重启容器。 但是有可能会出现发生了故障但是进程不退出的情况，比如访问web服务器时显示500内部错误，可能是系统超载，可能是资源死锁，此时httpd进程并没有异常退出，对于这种我们可以通过liveness探测来处理这类场景。 liveness liveness探测让用户自定义判断容器是否健康的条件，如果探测失败k8s就会重启容器。 启动进程首先创建/tmp/healthy，30秒以后删除，在这里设定中，如果/tmp/healthy存在，则容器处于正常状态，否则为故障。 liveness部分： 1、探测的方式是：通过cat命令去检查/tmp/healthy文件是否存在，如果命令执行成功，返回值则为零，k8s则认为这次探测成功，否则就是失败。 2、initialDelaySeconds：10，指定容器启动10秒后开始执行liveness探测，这里的话一般会根据容器的准备时间来设置，如果业务容量启动需要30秒那么则应该大于30秒。 3、periodSeconds：5，每5秒执行一次探测，k8s如果连续执行3次探测失败，则会杀掉进程并且重启。 readiness 用户通过liveness探测什么时候通过重启实现自愈，readiness探测则是告诉k8s什么时候可以将容器加入到service负载均衡池里面，对外提供服务。 15秒以后（initialDelaySecond+periodSeconds），第一次进行探测，此时成功，设置为READY。 30秒以后，/tmp/healthy被删除，连续3次readiness探测失败，设置为不可用 对比 1、readiness和liveness探测是两种health check，如果不特意配置，k8s将对两种探测采取相同的默认行为，即通过判断容器启动进程的返回值是否为0来判断探测是否成功。 2、两种判断的配置方法完全一样，支持的配置参数也一样，不同在于失败后的行为：liveness探测是重启容器；readiness则是将容器设置为不可用，不接受service转发的请求。 3、liveness探测和readiness探测都是独立执行的，两个之前没有依赖，用liveness探测判断容器是否需要重启以实现自愈；用readiness判断是否准备好对外提供服务。 关于一些配置参数： 123• exec：通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。• httpGet：通过发送http请求检查服务是否正常，返回200-400状态码则表明容器健康。• tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。 在scale中的应用 对于多副本的应用，当执行scale up操作的时候，新副本会作为backend被添加到service的负载均衡中，与已有的副本一起处理客户的请求。考虑到应用启动需要一个准备的过程，此时我们可以通过readiness来判断是否准备好，避免将请求发送到还没有准备好的backend上。比如： 这里使用了httpGet方法，该方法探测成功的判断条件是http请求的返回代码在200-400之间。 其中： schema：指定协议，http（默认）和HTTPS path：指定访问路径 port：指定端口 host：要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。 httpHeaders：自定义HTTP请求headers，HTTP允许重复headers。 上面配置的作用是： 1、容器启动10秒后开始检测 2、如果http://[container_ip]:8080/healthy返回代码不是200~400，表示容器没有就绪，不接收service web-svc的请求。 3、每个5秒检测一次 4、直到返回代码为200~400，表明容器已经就绪，然后将其加入到web-svc的负载均衡中，开始 处理客户请求。 5、探测会继续以5秒的时间间隔执行，如果连续发生3次失败，容器又会从负载均衡中移除，直到下次探测成功又重新加入。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（六）]]></title>
    <url>%2F2019%2F08%2F07%2Fk8s-5%2F</url>
    <content type="text"><![CDATA[通过scale命令来实现副本集的扩展 比如我们此时跑了个kubernetes-bootcamp的pod： 然后将其副本扩展到三个： 1kubectl scale deployments/kubernetes-bootcamp --replicas=3 deployment.extensions/kubernetes-bootcamp scaled 此时： 这时pod增加到了三个，我们去访问这个应用的时候，会将请求发送到不同的pod上面去处理，实现了均衡负载。减少副本的话使用： 1kubectl scale deployments/kubernetes-bootcamp --replicas=2 当然，如果是用yml文件创建的，直接修改副本集的数量就可以直接控制。 pod的更新回滚 通过yaml/yml文件 1、更新 滚动更新是一次只更新一小部分副本，成功之后再更新更多的副本，最终保证所有副本的更新。这样做的好处就是能过做到零停机，整个过程中始终有副本再保持运行，保证了业务的运行。 比如现在运行着http 2.2.31的副本： 然后将其更新到2.2.32，此时我们通过重修修改配置文件来实现： 这是一个逐渐更新并且替换的过程，我们可以通过查看日志来看： 从第二行开始看： 先增加一个pod，总数为1 减少一个pod，总数为1 增加一个pod，总数为2 减少一个，总数为0 每次都只更新一个pod，当然可以通过maxSurge和maxUnavaliable参数来控制每次更新的pod数量。 maxSurge：此参数控制滚动更新过程中副本总数超过DESIRED的上限，可以是具体的整数，也可以是百分数，向上取整，默认是25%。 maxUnavaliable：此参数控制滚动更新，不可用的副本占DESIRED的最大比例，下取整，默认值为25%。 前者越大，初始创建副本的数量也就越多；后者越大，初始销毁的旧副本数量也就越大。 2、回滚 kubectl apply每次更新应用的时候，都会记录下当前的配置，保存为一个revision，这样就可以回滚到某个特定的revision。 默认配置下，k8s只会保留最近几个revision，可以在配置文件下通过revisionHistoryLimit属性增加revision数量。 比如这时候存在httpd.v1.yml、httpd.v2.yml、httpd.v3.yml，然后镜像分别是2.4.16、2.4.17、2.4.18，然后我们分别执行： 123kubectl apply -f httpd.v1.yml --recordkubectl apply -f httpd.v2.yml --recordkubectl apply -f httpd.v3.yml --record –record的作用是将当前的命令记录到revision中，这样k8s就可以知道每个版本所对应的配置文件了，然后就可以通过kubectl rollout history deployment httpd可以查看revision的历史信息： 如果需要回滚到某一个版本，只需要执行： 1kubectl rollout undo deployment httpd --to-revision=1 这个时候我们再去看revision：]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（五）]]></title>
    <url>%2F2019%2F08%2F07%2Fk8s-4%2F</url>
    <content type="text"><![CDATA[关于service 每个pod都有自己的IP地址，当controller用新的pod替代发生故障的pod的时候，这时IP地址就会发生变化，但是如果一组pod对外提供服务，那么这个时候IP地址就会发生变化，就无法再去通过IP地址访问服务，在k8s中就通过service去完成这个事。 kubernetes service从逻辑上代表了一组pod，具体是哪些，则是需要有label来选择，service有自己的一个IP，而且这个IP是不变的，客户端只需要访问这个service的IP地址，k8s负责建立和维护service和pod之间的映射，无论后端怎么变化，对客户端来说则是不会改变，因为service没有改变。 比如我们创建一个： pod分配了各自的IP，这些IP只能被k8s cluster中的容器或者节点所访问： 这个时候我们创建一个service： 其中selector指明了挑选那些label为 run:httpd的pod作为service的后端，然后将TCP的8080端口映射到80端口上。 此时这个service就得到了一个cluster-IP，然后就通过该IP就可以访问后端的pod。 从图中我们可以看见，除了我们创建的一个service，还有service kubernetes，cluster内部通过这个service访问k8s api server。 此时，我们可以具体查看这个映射关系： 其中endpoints罗列了两个pod的IP和端口，pod的IP是在容器中配置的，service的cluster IP则是通过iptables来实现并且将其映射到pod IP上的。 关于cluster-IP cluster-IP是一个虚拟IP，是由k8s节点上的iptables规则来管理的。当我们通过iptables-save查看当前规则的时候，其中有两条： 第一条：如果cluster内的pod（10.244.0.0）要访问http-svc，则允许。 第二条：其他地址访问http-svc则跳转到KUBE-SVC-LLMSFKRLGJ6BVN7Z 然后KUBE-SVC-LLMSFKRLGJ6BVN7Z的规则是： 有二分之一的概率跳转到KUBE-SEP-QR5DAR5CWYHCVCRA，剩下的几率跳到 KUBE-SEP-S3AQ2KACAP2OENJQ。 由此可以看见iptables将service的流量转发到后端pod，并且使用了相应的均衡负载策略。 kube-dns kube-dns是一个DNS服务器，每当有新的service被创建，kube-dns会添加该service的DNS，cluster中的pod可以通过&lt;SERVICE_NAME&gt;.&lt;NAMESPACE_NAME&gt;访问service。 比如上面就可以通过service http-svc.default来访问app。 k8s提供了多种类型的service，默认是cluster-IP。 1、cluster IP：service通过cluster内部的IP对外提供服务，只有cluster内的节点和pod可以访问。 2、nodeport：service通过cluster节点的静态端口对外提供服务，cluster外部可以通过.访问service。 比如我们修改一下http-svc的配置文件： 然后重新生成一个service，此时通过： 1kubectl get service http-svc -o wide 其中，EXTERNAL—IP若为nodes，表示可通过cluster每个节点自身的IP访问services。PORTS前者表示为cluster的监听端口；后者表示节点上的监听端口，每个节点都会监听这个端口并将请求转发给service。任何一个节点都可以通过node的地址加端口来访问这个service。nodeport这种方式默认是随机选择一个端口，我们可以在配置文件中ports:内加入nodePort：30000来指定端口。 3、load balance：service利用云服务提供商提供的load balance对外提供服务，cloud provider负责将load balance的流量导向service。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（四）]]></title>
    <url>%2F2019%2F08%2F07%2Fk8s-3%2F</url>
    <content type="text"><![CDATA[k8s的架构 其中： etcd：保存了整个集群的配置信息和各种资源的状态信息 kube-apiserver：提供了资源操作的唯一入口，并提供了认证、授权、访问控制、API注册和发现等机制 kube-scheduler：负责资源的调度，按照预定的调度策略将pod调度到相应的机器上 kube-controller-manager：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等 kubelet：负责维持容器的生命周期，同时也负责volume（CVI）和网络（CNI）的管理 container runtime：负责镜像管理以及pod和容器的真正运行（CRI），默认的容器运行时为docker kube-proxy：负责为service提供cluster内部的服务发现和负载均衡。 除了核心组件以外，还有一些add-ons（附件） kube-dns：为整个集群提供DNS服务 ingress controller：为服务提供外网入口 heapster：提供资源监控 dashboard：提供GUI fluentd-elasticsearch：负责日志采集、存储和查询 分层架构 核心层：对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析） 管理层：系统度量、自动化以及策略管理 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtual（三）]]></title>
    <url>%2F2019%2F08%2F07%2Fvirtual-2%2F</url>
    <content type="text"><![CDATA[常见的虚拟化架构 1.OpenVZ（OVZ） 采用SWsoft的virtutozzo虚拟化服务器软件产品的内核，是基于Linux平台的操作系统级服务器虚拟化架构。这个架构直接调用宿主机（母机）中的内核，模拟生成出子服务器（VPS,小机）所以，他经过虚拟化后相对于母服务器，性能损失大概只有1-3%。OVZ可以超售，用户用多少资源就扣除宿主机多少资源；另一个特点是直接调用宿主机的内核，所以导致部分软件无法使用，以及部分内核文件是无法修改的。 2、KVM 是Linux下的全功能虚拟化架构，基于KVM架构的VPS，默认是没有系统的，可自己上传ISO或调用服务商自带的ISO手动安装系统或使用服务商提供的官方的KVM模板。由于KVM架构全功能虚拟化架构，甚至拥有独立的BIOS控制，所以对宿主机性能影响较大，所以基于KVM的VPS较贵，但较为自由。 KVM是基于虚拟化扩展（inter VT或AMD-V）的x86硬件，是Linux完全原生的全虚拟化解决方案。在KVM架构中，虚拟机实现为常规的Linux进程，由标准的Linux调度进程去进行调度。事实上，每个虚拟CPU显示为一个常规的Linux进程，这样使得KVM能够享受Linux内核的所有功能。 它本身不执行任何模拟，需要用户空间程序通过/dev/kvm接口设置一个客户机虚拟服务器的地址空间，向它提供模拟的I/O，并将它的视频显示映射回宿主机显示屏。 关于KVM的内存特性： 内存管理 一个虚拟机的内存与任何其他Linux进程的内存一样进行存储，可以以大页面的方式进行交换以实现更高的性能，也可以以磁盘文件的形式进行共享。NUMA（非一致性内存访问，针对多处理器的内存设计）允许虚拟机有效地访问大量内存。 KVM支持最新的基于硬件的内存虚拟化功能，支持inter的扩展页表（EPT）和AMD的嵌套页表，以实现更低的CPU使用率和更高的吞吐量。 内存页面共享通过一项名为内核同页合并（KSM）的内核功能来支持。KSM扫描每个虚拟机的内存，如果虚拟机拥有相同的内存页面，KSM将这些页面合并到一个虚拟机之间共享的页面，仅存储一个副本。如果一个客户机尝试更改这个共享页面，那么他将得到自己的专用副本。 存储 KVM能够使用Linux支持的任何存储方式来存储虚拟机的镜像，像本地磁盘，NAS，SAN等。多路径I/O可用于改进存储的吞吐量和提供冗余。由于KVM是Linux内核的一部分，它可以利用所有成熟可靠的存储架构。 KVM还支持全局文件系统等共享文件系统上的虚拟机镜像，以允许虚拟机镜像在多个宿主之间共享或者使用逻辑卷共享。磁盘空间按需分配，仅在虚拟机需要的时候分配存储空间，而不是提前分配好，提供了存储利用率。 设备驱动程序 KVM支持混合虚拟化，其中准虚拟化的驱动程序安装在客户机操作系统中，允许虚拟机使用优化的I/O接口而不使用模拟的设备，从而为网络和块设备提供了高性能的I/O。 性能和可伸缩性 它继承了Linux的性能和可伸缩性。 3、Xen 是基于硬件的完全分割，物理上有多少的资源就只能分配多少资源，因此很难超售，可分为Xen-PV(半虚拟化)和Xen-HVM（全虚拟化）。 它是一个直接在系统硬件上运行的虚拟机管理程序，Xen在系统硬件和虚拟机之间插入一个虚拟化层，将系统硬件转换为一个逻辑计算资源池，然后动态地分配给任何操作系统或者是应用程序。使得在虚拟机中运行的操作系统能够与虚拟资源交互。 Xen被设计成微内核的实现，其本身只是负责管理处理器和内存资源。在上面运行的所有虚拟机中存在一个特殊的虚拟机（虚拟机0），里面运行的是经过修改的支持半虚拟化的Linux系统，大部分的输入输出设备都交由这个虚拟机直接控制，而Xen本身并不直接控制他们。这样做可以最大程度地复用Linux内核的驱动程序。其次，Xen上面运行的虚拟机，即可以支持半虚拟化又可以支持全虚拟化。 4、hyper-V 它是微软的一款虚拟化产品，不可以超售内存但是可以超售硬盘。它的设计借鉴了Xen，采用了微内核的架构，兼顾了安全性和性能的要求。hyper-V底层的hypervisor运行在最高的特权级别下，微软将其称为ring -1（inter称其为rootmode），而虚拟机的操作系统内核和驱动运行在ring0，应用程序运行在ring3。 5、inter虚拟化 inter虚拟化技术其实是一系列硬件技术的集合，VMM软件选择利用各项技术，从而提高虚拟化软件的性能或者实现各种不同的功能。 大致可以分为三类： 第一类是和处理器相关的，叫做VT-X，是实现处理器虚拟化的硬件扩展，这也是硬件虚拟化的基础。 第二类是和芯片组相关的，称为VT-D，是从芯片组的层面为虚拟化提供支持，通过它，可以实现像直接分配物理设备给客户机的功能。 第三类是输入输出设备相关的，主要目的是通过定义新的输入输出协议（VMDq），使新一代的输入输出设备可以更好的支持虚拟化环境。]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtual（二）]]></title>
    <url>%2F2019%2F08%2F07%2Fvirtual-1%2F</url>
    <content type="text"><![CDATA[虚拟化的模型 KVM的基本架构分为两种。 1、类型一（原生虚拟化） 系统上电后首先加载运行虚拟机监控程序，而传统的操作系统则是运行在其创建的虚拟机中。这种的虚拟机监控程序，从某种意义上说，可以视为一个特别为虚拟机而优化裁剪的操作系统内核。因为，虚拟机监控程序作为运行在底层的软件层，必须要去实现诸如系统的初始化、物理资源的管理等操作系统的职能。它对虚拟机的创建、调度和管理，与操作系统对进程的创建、调度和管理有共通之处。这一类型的虚拟机监控程序一般会提供一个具有一定特权的特殊虚拟机，由这个特殊虚拟机来运行需要提供给用户日常操作和管理使用的操作系统环境。著名的开源有Xen、商业的有VMware ESX/ESXi和微软的hyper-V。 2、类型二（寄宿虚拟化） 这种类型，在系统上电之后还是运行传统一般意义上的操作系统（宿主机操作系统），而虚拟机监控程序，作为一个特殊的应用程序运行在它的上面，可以看做是对操作系统的扩展。这样最大的优势就是可以充分利用现有的操作系统。因为虚拟机监控程序通常不必自己实现物理资源的管理和调度算法，所以实现起来就比较简单，这一类型的虚拟机监控程序既然是依赖于操作系统来调度和管理，也就会受到一些限制。比如，通常无法为了虚拟机的优化而对操作系统进行修改。这类有KVM，VMware workstation、virtualbox。]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtual（一）]]></title>
    <url>%2F2019%2F08%2F07%2Fvirtual%2F</url>
    <content type="text"><![CDATA[关于虚拟化这块，自己其实也很迷，很多东西也只是知道个基本的原理。先简单说说。 随着计算机硬件技术的发展，物理资源的容量越来越大而价格越来越低，在既有的计算元件架构下，物理资源不可避免产生闲置和浪费。为了充分利用物理资源，提高效率，一个比较直接的办法就是更新计算元件以利用更加丰富的物理资源。但是，往往我们为了对稳定性和兼容性的追求，就并不情愿地对已经存在的计算元件做大幅度的变更，虚拟化就出来了，它通过引入一个新的虚拟化层，对下管理真实的物理资源，对上提供虚拟的系统资源，从而实现在扩大硬件容量的同时，简化软件的重新配置。 在x86平台的虚拟化技术中，新引入的虚拟化层通常被称为虚拟机监控器（VMM），也叫作hypervisor。虚拟机监控器运行的环境，也就是真实的物理平台，称之为宿主机，而虚拟出来的平台通常称为客户机，里面运行的系统对应地也称为客户机操作系统。 然后是各种虚拟化技术，可能比较乱，因为自己在学的时候也比较乱，emmmmm。 网络虚拟化 （关于网络虚拟化这块，详细的可以看这个 网站) 对物理网络及其组件进行一个抽象，并从中分离网络业务流量的一种，采用网络虚拟化可以将多个物理网络抽象成一个虚拟网络，或者将一个物理网络分割成多个逻辑网络。 1、网络功能虚拟化（NFV） 是从网络运营商的角度出发的一种软硬件分离的架构，主要是希望通过标准化IT虚拟化技术，采用业界标准的大容量服务器、存储和交换机承载各种各样的网络软件功能，实现软件的灵活加载，从而可以在数据中心，网络节点和用户端等不同的位置进行一个灵活的部署配置，NFV打破了网络物理设备层和逻辑业务层之间的绑定关系，每个物理设备被虚拟化的网元所取代，通过对虚拟网元进行一个配置来满足其需求。 2、传统网络虚拟化和发展 传统的网络虚拟化技术以VLAN和VPN（虚拟专用网）为代表，通过协议封装在物理网路上提供相互隔离的虚拟专用网络，随着软件虚拟化，软件定义网络（SDN）等技术的发展，利用分布式的软件技术实现网络功能集的合理抽象、分割以及灵活调度逐步成为网络虚拟化及网络功能虚拟化的主流实现模式。现阶段，叠加网络（overlay）组网技术、虚拟化流量调度技术等是网络虚拟化以及网络功能虚拟化技术的主要研究热点。 在网络技术领域，overlay指的是一种在网络结构上进行叠加的虚拟化技术模式，其大体框架是对基础网络不进行大规模修改的前提下，实现应用在网络上的承载，并能与其他业务分离，目前主要应用于数据中心内部网络的大规模互联。 3、虚拟化资源调度 虚拟化资源调度技术借助SDN，NFV等理念实现对网络流量、业务功能等资源的虚拟化和智能调度，其中包括虚拟化流量调度和业务链。 （1）虚拟化流量调度 虚拟化流量调度技术是通过虚拟化技术突破目前ip网络分布式路由选路的局限，利用集中式路由计算与流量调度，实现全网流量动态均衡与网络结构优化。虚拟化流量调度技术主要应用于ip骨干网，重点包括如何合理定义ip路由功能集的抽象、集中式路由决策系统的实现方式，集中式系统的可靠性以及该种模式下的保护路径计算实时性算法等。目前虚拟化流量调度技术在朝着与SDN等新技术结合的方向发展，如通过新增路径计算单元/控制器（PCE/controller）系统来实现集中式路由决策系统。 （2）业务链 网络中的虚拟防火墙、负载均衡器、网关等业务处理功能被称为业务功能点，而流量经过一系列的业务功能点的处理，就形成了业务链。与虚拟化流量调度不同，业务链更侧重于解决虚拟网络中如何通过控制服务器对网络流量转发进行编程控制，将流量灵活的调配到某些个业务功能点进行处理，为用户无缝交付网络服务。 存储虚拟化 存储虚拟化是指为物理的存储提供一个抽象的逻辑视图，用户可以通过这个视图中的逻辑接口来访问被整合的存储资源。存储虚拟化主要有基于存储设备的虚拟化和基于网络的存储虚拟化两种。其中，磁盘阵列技术是基于存储设备的存储虚拟化的代表，该技术通过将多块物理磁盘组合成磁盘阵列，提供一个统一的、高性能的容错存储空间。网络附加存储（NAS）和存储区域网络(SAN)则是基于网络的存储虚拟化技术。对于用户而言，并不知道其真实的物理地址，对于管理者，能够在一个控制台上管理分散在不同位置的异构设备上的数据。 系统虚拟化 系统虚拟化实现了操作系统与物理计算机的分离，使得在一台物理设备上能够同时安装和运行一个或者是多个虚拟的操作系统，在操作系统内部的应用程序看来，与直接安装在物理计算机上的操作系统没有什么差异。它的核心就是使用虚拟化软件在一台物理机上虚拟出一台或者是多台虚拟机。 桌面虚拟化 桌面虚拟化将用户的桌面环境与其使用的终端设备解耦合。服务器上存放的是每个用户完整的桌面环境，用户可以使用不用的具有足够处理和显示功能的终端设备，通过网络访问该桌面环境。桌面个虚拟化将众多终端的资源集合到后台数据中心，以便对企业若干终端进行统一认证、统一管理和灵活的调配资源。 软件和硬件虚拟化 先放一张图： 在某本书中这样说的：从图中可以看出，实现虚拟化的重要一步就是，虚拟化层必须能够截获计算元件对物理资源的直接访问，并将其重定向到虚拟资源池中，根据虚拟化层是用纯软件的方式还是利用物理资源提供的机制来实现这种“截获并重定向”，我们分成软件虚拟化和硬件虚拟化。 1、软件虚拟化 就是用软件的方法在现有的物理平台上实现对物理平台访问的截获和模拟，常见的是：QEMU，它是通过纯软件来仿真x86平台处理器的取指、解码和执行，客户机的指令并不在物理平台上执行。由于所有的指令都是软件模拟的，因此性能比较差，但是可以在同一平台上模拟不同架构平台的虚拟机。 VMware的软件虚拟化则使用动态二进制翻译的技术，VMM在可控制的范围内允许客户机的指令在物理平台上直接运行，但是，客户机指令在运行前会被VMM扫描，其中敏感的指令会被动态替换成可以在物理平台上直接运行的安全指令，或者替换成对VMM的软调用。这样做的好处就是比纯软件虚拟化性能要好，但是丧失了跨平台的能力。 2、硬件虚拟化 简单的说，就是物理平台本身提供了对特殊指令的截获和重定向的硬件支持。以X86平台的虚拟化为例，支持虚拟技术的X86 CPU带有特别优化过的指令集来控制虚拟过程，通过这些指令集，VMM会很容易将客户机置于一个受限制的模式下去运行，一旦客户机试图访问物理资源，硬件会暂停客户机的运行，将控制权交回给VMM处理。VMM还可以利用硬件虚拟化的增强机制，将客户机在受限模式下实现对一些特定资源的访问，完全由硬件重定向到VMM指定的虚拟资源，整个过程不需要暂停客户机的运行和VMM软件的参与。 由于虚拟化硬件可提供全新的架构，支持操作系统直接在上面运行，无需进行二进制转换，减少了相关的性能开销，简化了VMM设计，性能更加强大。 半虚拟化和全虚拟化 1、半虚拟化 软件虚拟化 可以在缺乏硬件虚拟化支持的平台上完全通过VMM软件来实现对各个虚拟机的监控，以保证它们之间彼此隔离和独立。但是付出的代价就是软件复杂度的增加和性能上的损失。减轻这种负担的一种办法需要改动客户操作系统，使他以为自己运行在虚拟环境下，能够与VMM进行协同工作，这种方法叫做准虚拟化或者半虚拟化。本质上，它弱化了虚拟机对特殊指令的被动截获要求，将其转化成客户机操作系统的主动的通知，但是这需要修改操作系统的源码来实现。 比如Xen：操作系统作为虚拟服务器在Xen hypervisor上运行之前，它必须在内核层面进行某些改变，所以像Windows这些因为不公开源码，所以是无法修改其内核的。（Vmware通过动态二进制技术，将敏感指令翻译成受监管的、安全的具有相同功能的指令去执行，但是这种消耗了太多的CPU时钟周期，Xen解决方式是通过修改虚拟机的操作系统发出一个超级调用的指令去给VMM。） 2、全虚拟化 全虚拟化为客户机提供了完整的虚拟X86平台，包括处理器、内存和外设，支持任何理论上可在物理平台上运行的操作系统，为虚拟机的配置提供了最大程度的灵活性。这样的好处就是：不需要对客户机操作系统做任何修改即可正常运行任何非虚拟化环境中已经存在了的基于X86平台的操作系统和软件。]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
      <tags>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lvs（一）]]></title>
    <url>%2F2019%2F07%2F14%2Flvs%2F</url>
    <content type="text"><![CDATA[关于LVS（Linux virtual server），官方文档中是这样说的：针对高可压缩、高可用网络服务的需求，这是一个基于ip层和基于内容请求分发的负载平衡调度方法，并在Linux内核中实现了这些方法，将一组服务器构成一个实现可伸缩的、高可用网络服务的虚拟服务器。 它的体系结构为： 一组服务器通过高速的局域网或者地理分布的广域网相互连接，在他们的前端有一个负载调度器（load balancer），负载调度器将网络请求调度到真实服务器（real server）上，从而使得服务器集群地结构对客户是透明地，对于客户而言，这就像是一整台高性能，高可用地服务器。此时，系统的伸缩性通过在服务器群中透明地加入或者删除一个节点来达到。而这个负载调度技术在Linux内核中实现，所以叫他Linux虚拟服务器。 调度器是服务器集群系统的唯一入口点（single entry point），它可以采用IP负载均衡技术，基于内容请求分发技术，或者将两者结合。在IP负载均衡技术中，需要服务器池拥有相同的内容提供相同的服务。当客户请求到达的时候，调度器只根据服务器负载情况和设定的调度算法从服务器池中选出一个服务器，将该请求转发到选出的服务器，并记录这个调度。当这个请求的其他报文也到达的时候，也会被转发到前面选出的服务器。在基于内容请求分发技术中，服务器可以提供不同的服务，当客户请求到达时，调度器根据请求的内容选择服务器执行请求。整个过程都是在Linux的内核中实现，所以调度开销很小。吞吐率高。 LVS集群采用三层结构，主要由三个组成部分： 负载调度器；它是整个集群对外的前端机，负责将客户的请求发送到一组服务器上执行，而客户认为服务是来自一个IP地址。 服务器池：是一组真正执行客户请求的服务器，执行的服务有web、mail、ftp和DNS等。 共享存储：它为服务器池提供一个共享的存储区，这样很容易使得服务器池拥有相同的内容，提供相同的服务。 它分为三种模型： 1、Virtual Server via Network Address Translation（VS/NAT） 通过网络地址转换，调度器重写请求报文的目标地址，根据预设的调度算法，将请求分派给后端的真实服务器，真实服务器的相应报文经过调度器的时候，源地址被重写，在返回给客户，这对负载器的性能要求很高。 应用环境： 集群节点跟director必须在同一个ip网络中 RIP通常是私有地址，仅用于各集群节点间的通信 director位于client和real server之间，并处理进出的所有通信 real server必须将网关指向DIP 支持端口映射，入和出都在同一条路上，所以支持端口映射 real server可以使用任意操作系统 请求过程： 客户端请求发往LVS主机，此时，客户端请求报文的源ip为CIP，目标IP为LVS的VIP，当LVS收到客户端的请求报文时，会将请求报文的目的ip修改为后端某个real server的RIP，具体修改成谁的，要根据调度算法来看；当客户端的请求报文的目的ip修改为RIP后，那么此时报文会转发该RIP对应的real server上去进行处理；处理完以后生成响应报文，响应报文的此时源ip为RIP，目的ip是CIP，当报文达到LVS的时候，此时LVS会将源IP从RIP修改为VIP，从而发送给客户端。 2、Virtual Server via Direct Routing（VS/DR） 通过改写请求报文的MAC地址，将请求发给真实服务器，而真实服务器将响应报文直接返回给客户，这种方法没有ip隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器和真实服务器都有一块网卡连在同一个物理网段上。 应用环境（常用）： 集群节点跟director必须在同一个物理网络中 不支持端口映射 RIP可以使用公网地址，实现便捷的远程管理和监控 director仅负责处理入站请求，响应报文则由real server直接发往客户端 real server不能将网关指向DIP 不支持端口映射 访问流程： 当客户端请求集群服务时，请求报文发送至 Director 的 VIP（RS的 VIP 不会响应 ARP 请求），Director 将客户端报文的源和目标 MAC 地址进行重新封装，将报文转发至 RS，RS 接收转发的报文。此时报文的源 IP 和目标 IP 都没有被修改，因此 RS 接受到的请求报文的目标 IP 地址为本机配置的 VIP，它将使用自己的 VIP 直接响应客户端。 3、Virtual Server via IP Tunneling（VS/TUN） 采用NAT技术，由于请求和响应报文都需要调度器进行重写地址，这样当客户请求越来越多的时候，会容易造成性能瓶颈，所以TUN技术为了解决这个问题，调度器将报文通过IP隧道之间转发给真实的服务器，而真实服务器将响应直接返回给客户，所以调度器只负责处理请求报文。 应用环境： 转发的时候能够重新封装报文的首部 集群节点可以跨越Internet，实现多地存放real server RIP必须是公网地址 director仅负责处理入站请求，响应报文则有real server直接发往客户端 real server不能将网关指向DIP 只有支持隧道功能的OS才能用于real server 不支持端口映射 访问过程： 当请求到达 Director 后，Director 不修改请求报文的源 IP 和目标 IP 地址，而是使用 IP 隧道技术，使用 DIP 作为源 IP，RIP 作为目标 IP 再次封装此请求报文，转发至 RIP 的 RS 上，RS 解析报文后仍然使用 VIP 作为源地址响应客户端。 调度算法 静态调度： 1、RR轮询（Round Robin），就是将请求按一定的规律或顺序平均分配给后端主机处理，这种方式适合后端处理请求的主机之间性能差异很小的情况。 2、wrr加权轮询（weighted Round Robin），当后端处理请求的主机之间差异较大的时候，就需要使性能较差的主机处理相对少的任务，而性能较强的主机尽可能多的处理任务，这是用加权轮询是比较方便的办法 3、sh源地址hash调度：以源地址作为关键字做静态hash表来确定需要的RS 4、dh目的地址hash调度：以目的地址作为关键字做静态hash表来确定需要的RS 动态调度 1、lc最小连接数调度（lease connection）：将用户请求分配到连接数最少的RS上，当所有的RS连接数都为0，按轮询方式来调度 2、wlc加权最小连接数调度（weighted least connection)，将用户请求分配至后端连接数最少且权重最高的。（默认） 3、sed：最短期望延迟，谁小选谁 4、nq：永不排队 5、LBLC：基于本地的最少连接 6、LBLCR：基于本地的带复制功能的最少连接缓存复制机制]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived（二）]]></title>
    <url>%2F2019%2F07%2F13%2Fkeepalived-1%2F</url>
    <content type="text"><![CDATA[keepalived+nginx搭建 主从配置： 这种方案，使用一个VIP地址，前端使用2台机器，一台做主，一台做备，但同时只有一台机器工作，另一台备份机器在主机器不出现故障的时候永远处于浪费状态。 双主配置： 这种方案，使用两个VIP地址，前端使用2台机器，互为主备，同时有两台机器工作，当其中一台机器出现故障，两台机器的请求转移到一台机器负担，非常适合当前的架构。 实验环境： 12345node1（Nginx1）：192.168.248.129 node2（Nginx2）：192.168.248.130 node3（WEB1）：192.168.248.128node4（WEB2）：192.168.248.133 VIP：192.168.248.144 web部署 在node3和node4上执行下面的脚本： 123456#!/bin/bashyum install net-tools httpd -ysystemctl stop firewalldsetenforce 0echo &quot;&lt;h1&gt;This is RS1&lt;/h1&gt;&quot; &gt; /var/www/html/index.html # 修改不同的主页以便测试！systemctl start httpd nginx部署 在node1和node2上执行以下脚本： 123456789101112131415161718#!/bin/bashsystemctl stop firewalldsetenforce 0yum install nginx -ycat &gt; /etc/nginx/conf.d/proxy.conf &lt;&lt; EOFupstream websers&#123; server 192.168.248.128; server 192.168.248.133;&#125;server&#123; listen 8080; server_name 192.168.248.129; //node2改 location / &#123; proxy_pass http://websers; &#125;&#125;EOFnginx -s reload keepalived部署 在node1和node2节点执行以下脚本： 123456789101112131415161718192021222324#!/bin/bashyum install keepalived -ymv /etc/keepalived/keepalived.conf&#123;,.bak&#125;cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; EOF! Configuration File for keepalivedglobal_defs &#123; router_id node1 # node2修改&#125;vrrp_instance VI_1 &#123; state MASTER # node2节点BACKUP interface ens33 #指定检测网络接口 virtual_router_id 10 #虚拟路由标识，同一个VRRP实例要使用同一个标识 priority 100 # node2节点小于100 advert_int 1 #设置主备节点间同步检查时间间隔 authentication &#123; ##设置主备节点间的通信验证类型及密码，同一个VRRP实例中要一致 auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; ##使用虚拟ip进行访问 192.168.248.144 &#125;&#125;EOFsystemctl restart keepalived 我们需要自定义脚本检测nginx服务是否正常运行，此时需要修改配置文件加入： 12345678910vrrp_script chk_http_port &#123; #配合track_script进行脚本监控，chk_http_port自定义名字 script &quot;/usr/local/src/check_nginx_pid.sh&quot; interval 1 weight -2 # 条件成立，优先级-2&#125;# 调用script脚本 track_script &#123; chk_http_port &#125; 其中脚本内容为 ： 123456789101112#!/bin/bashA=`ps -C nginx --no-header |wc -l` if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then exit 1 else exit 0 fielse exit 0fi 然后访问测试，并可以尝试分别关闭keepalived和nginx服务后进行测试。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived（一）]]></title>
    <url>%2F2019%2F07%2F13%2Fkeepalived%2F</url>
    <content type="text"><![CDATA[keepalived是一款高可用软件，它的功能主要包括两方面： 1、通过ip漂移，实现服务的高可用；服务器集群共享一个虚拟ip，同一时间只有一个服务器占有虚拟ip，并对外提供服务，若该服务器不可用，则虚拟ip漂移到另一台服务器并对外提供服务。 2、对LVS应用服务器的应用服务器集群进行状态监控，若应用服务器不可用，则keepalived将其从集群中摘除，若应用服务器恢复，则keepalived将其又重新加入到集群中 健康检查和失败切换是keepalived的核心。所谓健康检查，就是采用TCP三次握手、icmp请求，http请求，udp echo请求等方式对负载均衡器后面的实际服务器进行一个存活状态确认；而失败切换主要是应用于配置了负载均衡器上，利用VRRP维持主备负载均衡器的心跳，当主负载均衡器出现问题的时候，由备负载均衡器承载对应的业务，从而在最大限度上减少流量损失，并提供服务的稳定性。 它可以单独使用，即通过ip漂移实现服务的高可用，也可以结合LVS使用，即一方面通过ip漂移实现LVS负载均衡层的高可用，另一方面实现LVS应用服务器层的状态监控。 相关原理 keepalived的实现基于VRRP，而VRRP是为了解决静态路由的高可用。 虚拟路由器由多个VRRP路由器组成，每个VRRP路由器都有各自的IP和共同的VRID，其中一个VRRP路由器通过竞选成为master，占有VIP，对外提供路由服务，其他成为backup，master以ip组播（组播地址为：224.0.0.18）形式发送VRRP协议，与backup保持心跳连接，若master不可用（或backup接收不到VRRP协议包），则backup通过竞选产生新的master并继续对外路由服务，从而实现高可用。 在网络层：通过ICMP协议向后端服务器集群中发送数据报文。 在传输层：利用TCP协议的端口连接和扫描技术检测后端服务器集群是否正常。 在应用层：自定义keepalived工作方式。 相关体系和组件 1、Scheduler I/OMultiplexer是一个I/O复用分发调度器，它负载安排keepalived所有内部的任务请求。 2、memory mngt：是一个内存管理机制，这个框架提供了访问内存的一些方法。 3、control plane是keepalived的控制面板，可以实现对配置文件编译和解析。 4、core components： watchdog：是计算机可靠领域中极为简单又有效的监测工具，keepalived正是通过它监控checkers和VRRP进程的。 checkers：这是keepalived最基础的功能，也是最主要的，可以实现对服务器运行状态检测和故障隔离。 ipvs wrapper：这个是IPVS功能的一个实现，这个模块将设置好的ipvs规则发送给内核空间并且提供给IPVS模块，最终实现IPVS模块的负载功能。 VRRP stack：可以实现HA集群失败切换功能。负责负载均衡器之间的失败切换。 netlink reflector：用来实现高可用集群failover时虚拟ip的设置和切换。 keepalived运行时，会启动3个进程： core：负责主进程的启动、维护和全局配置文件的加载 check：负载健康检查 vrrp：用来实现VRRP协议 配置文件说明 配置文件： /etc/keepalived/keepalived.conf 包含三部分： 1、全局配置，配置邮件等； 2、VRRP的配置，配置VRRP实例； 3、LVS配置，配置LVS的应用服务器 若只是单独使用keepalived，通过IP漂移实现服务的高可用，则只需要配置前两部分就可以，若结合LVS使用，实现LVS负载均衡层的高可用、应用服务层的状态监控，则还需要配置第三部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798global_defs &#123; notification_email &#123; #指定keepalived在发生切换时需要发送email到的对象，一行一个 monitor@3evip.cn &#125; notification_email_from monitor@3evip.cn #指定发件人 smtp_server stmp.3evip.cn #指定smtp服务器地址 smtp_connect_timeout 30 #指定smtp连接超时时间 router_id LVS_DEVEL #运行keepalived机器的一个标识&#125;vrrp_sync_group VG_1&#123; #监控多个网段的实例 group &#123; inside_network #实例名 outside_network &#125; notify_master /path/xx.sh #指定当切换到master时，执行的脚本 netify_backup /path/xx.sh #指定当切换到backup时，执行的脚本 notify_fault &quot;path/xx.sh VG_1&quot; #故障时执行的脚本 notify /path/xx.sh smtp_alert #使用global_defs中提供的邮件地址和smtp服务器发送邮件通知&#125;vrrp_instance inside_network &#123; state BACKUP #指定那个为master，那个为backup，如果设置了nopreempt这个值不起作用，主备考priority决定 interface eth0 #设置实例绑定的网卡 dont_track_primary #忽略vrrp的interface错误（默认不设置） track_interface&#123; #设置额外的监控，里面那个网卡出现问题都会切换 eth0 eth1 &#125; mcast_src_ip #发送多播包的地址，如果不设置默认使用绑定网卡的primary ip garp_master_delay #在切换到master状态后，延迟进行gratuitous ARP请求 virtual_router_id 50 #VPID标记 priority 99 #优先级，高优先级竞选为master advert_int 1 #检查间隔，默认1秒 nopreempt #设置为不抢占 注：这个配置只能设置在backup主机上，而且这个主机优先级要比另外一台高 preempt_delay #抢占延时，默认5分钟 debug #debug级别 authentication &#123; #设置认证 auth_type PASS #认证方式 auth_pass 111111 #认证密码 &#125; virtual_ipaddress &#123; #设置vip 192.168.36.200 &#125;&#125;virtual_server 192.168.36.99 80 &#123; delay_loop 6 #健康检查时间间隔 lb_algo rr #lvs调度算法rr|wrr|lc|wlc|lblc|sh|dh lb_kind DR #负载均衡转发规则NAT|DR|RUN persistence_timeout 5 #会话保持时间 protocol TCP #使用的协议 persistence_granularity &lt;NETMASK&gt; #lvs会话保持粒度 virtualhost &lt;string&gt; #检查的web服务器的虚拟主机（host：头） sorry_server&lt;IPADDR&gt; &lt;port&gt; #备用机，所有realserver失效后启用 real_server 192.168.200.5 23 &#123; weight 1 #默认为1,0为失效 inhibit_on_failure #在服务器健康检查失效时，将其设为0，而不是直接从ipvs中删除 notify_up &lt;string&gt; | &lt;quoted-string&gt; #在检测到server up后执行脚本 notify_down &lt;string&gt; | &lt;quoted-string&gt; #在检测到server down后执行脚本 TCP_CHECK &#123; connect_timeout 3 #连接超时时间 nb_get_retry 3 #重连次数 delay_before_retry 3 #重连间隔时间 connect_port 23 健康检查的端口的端口 bindto &lt;ip&gt; &#125; HTTP_GET | SSL_GET&#123; url&#123; #检查url，可以指定多个 path / digest &lt;string&gt; #检查后的摘要信息 status_code 200 #检查的返回状态码 &#125; connect_port &lt;port&gt; bindto &lt;IPADD&gt; connect_timeout 5 nb_get_retry 3 delay_before_retry 2 &#125; SMTP_CHECK&#123; host&#123; connect_ip &lt;IP ADDRESS&gt; connect_port &lt;port&gt; #默认检查25端口 bindto &lt;IP ADDRESS&gt; &#125; connect_timeout 5 retry 3 delay_before_retry 2 helo_name &lt;string&gt; | &lt;quoted-string&gt; #smtp helo请求命令参数，可选 &#125; MISC_CHECK&#123; misc_path &lt;string&gt; | &lt;quoted-string&gt; #外部脚本路径 misc_timeout #脚本执行超时时间 misc_dynamic #如设置该项，则退出状态码会用来动态调整服务器的权重，返回0 正常，不修改；返回1，检查失败，权重改为0；返回2-255，正常，权重设置为：返回状态码-2 &#125; &#125;&#125;]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker（五）]]></title>
    <url>%2F2019%2F07%2F09%2Fdocker-4%2F</url>
    <content type="text"><![CDATA[跨主机通信 通过docker网络驱动： Overlay：基于vxlan封装实现docker原生overlay网络 Macvlan：docker主机网卡接口逻辑上分为多个子接口，每个子接口标识一个vlan。容器接口直接连接docker主机 网卡接口：通过路由策略转发到另一台docker主机 通过第三方网络项目： 隧道方案： Flannel：支持UDP和VLAN封装传输方式 Weave：支持UDP（sleeve模式）和VXLAN（优先fastdb模式） OpenvSwitch：支持VXLAN和GRE封装 路由方案： Calico：支持BGP协议和IPIP隧道。每台宿主机作为虚拟路由，通过BGP协议实现不同主机容器间通信。 直接路由方式（通过内核转发） 实验环境： 由于使用容器的ip进行路由，就需要避免不同主机上的容器使用了相同的ip，为此我们应该为不同的主机分配不同的子网来保证。 主机1的地址为：192.168.248.128 主机2的地址为：192.168.248.129 主机1上docker容器分配的子网：172.17.1.0/24 主机2上docker容器分配的子网：172.17.2.0/24 思路： 从container1发送container2的数据包，首先发往container1的网关docker0，然后通过查找主机1的路由表得知需要将数据包发往主机2，数据包达到主机2后再转发给主机2的docker0，最后到达container2 。 过程： 1、配置docker0的ip地址 主机一修改daemon.json文件，添加： 123&#123; &quot;bip&quot;:&quot;172.17.1.252/24&quot;&#125; 主机二同理，但是修改成2.252/24，然后重启docker 2、添加路由规则 主机一： 1route add -net 172.17.2.0 netmask 255.255.255.0 gw 192.168.248.128 主机二： 1route add -net 172.17.1.0 netmask 255.255.255.0 gw 192.168.248.129 3、配置路由转发 如果有iptables的话： 主机一： 12iptables -t nat -F POSTROUTINGiptables -t nat -A POSTROUTING -s 172.17.1.0/24 ! -d 172.17.0.0/16 -j MASQUERADE 主机二： 12iptables -t nat -F POSTROUTINGiptables -t nat -A POSTROUTING -s 172.17.2.0/24 ! -d 172.17.0.0/16 -j MASQUERADE 没有的话： 1echo 1 &gt; /proc/sys/net/ipv4/ip_forward 4、启动容器测试 主机一： 1docker run -it --name container1 centos /bin/bash 主机二： 1docker run -it --name container2 centos /bin/bash 通过Overlay网络 这块参照这位博主，万分感谢。 先提一下overlay网络 overlay在网络技术领域，指的是一种网络架构上叠加的虚拟化，其大体框架是对基础不进行大规模修改的条件下，实现应用在网络上的承载，并能与其他网络业务进行分离，并且以基于ip的基础网络技术为主。overlay技术是在现有的物理网络之上构建一个虚拟网络，上层应用只与虚拟网络相关。主要有三部分构成： 边缘设备：与虚拟机之间相连的设备 控制平面：主要负责虚拟隧道的建立维护以及主机可达性信息的通告 转发平面：承载overlay报文的物理网络 要想使用docker原生overlay网络，需要满足下列条件： docker运行在swarm 使用key-value存储的docker主机集群 使用键值存储搭建docker主机集群需要满足： 集群中主机连接到键值存储，docker支持consul、etcd和zookeeper等 集群中主机运行一个docker守护进程 集群中主机必须具有唯一的主机名，因为键值存储使用主机名来标识群成员 集群中Linux主机内核版本在3.12+，支持vxlan数据包处理，否则无法通行 （注：关于这块我在其他地方看的，也没做过测试，反正最好这样吧，如果有问题告诉我） 由于docker overlay网络需要一个key-value数据库用于保存网络状态信息，包括network、endpoint、ip等。consul、etcd和zookeeper都是。consul是一种键值数据库，我们这可以用它存储系统的状态信息。 实验环境： server：192.168.248.129 kernel：4.4 client：192.168.248.130 kernel：4.4 1、安装consul（server上） 方式一： 123wget https://releases.hashicorp.com/consul/0.9.2/consul_0.9.2_linux_386.zipunzip consul_0.9.2_linux_386.zipmv consul /usr/bin/ &amp;&amp; chmod a+x /usr/bin/consul 方式二： 1docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap 2、启动consul（如果上一步用方法二就跳过） 12nohup consul agent -server -bootstrap -ui -data-dir /data/docker/consul \&gt; -client=192.168.248.129 -bind=192.168.248.129 &amp;&gt; /var/log/consul.log &amp; 注：-ui：consul的管理界面；-datadir：数据存储路径 3、各节点配置docker守护进程连接consul server上： 12vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store consul://192.168.248.129:8500 --cluster-advertise 192.168.248.129:2375 其中： –cluster-store 指定 consul 的地址。 –cluster-advertise 告知 consul 自己的连接地址。 client上： 12vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-store consul://192.168.248.129:8500 --cluster-advertise 192.168.248.130:2375 4、放行防火墙 12345firewall-cmd --add-port=8500/tcp --permanent firewall-cmd --add-port=2375/tcp --permanent firewall-cmd --add-port=4789/udp --permanent //容器之间流量的vxlan端口firewall-cmd --add-port=7946/tcp --permanent //docker守护进程端口firewall-cmd --add-port=7946/udp --permanent 5、打开web进行检查 6、创建一个overlay网络 1docker network create -d overlay ov_net 此时详细查看这个网络的信息： 由于consul的自动发现，另一台主机会自动同步新建的网络。 7、使用overlay网络创建容器 server和client上： 1docker run -it --net=ov_net busybox 此时我们可以查看容器内的网卡信息： 其中：eth0是和overlay进行通信，eth1是和主机的docker_gwbridge通信（docker_gwbridge为使用overlay网络的容器提供上外网的能力） 这里使用的是自动分配的ip，如果需要静态固定ip： 123456创建网络的时候：docker network create -d overlay --subnet=192.168.2.0/24 ov_net启动容器的时候：docker run -d --name host1 --net=ov_net --ip=192.168.2.2 centos7 8、测试 通信过程： 1、docker为每一个overlay网络创建一个单独的命名空间，在这个命名空间里面创建一个br0的bridge。 2、在这个命名空间内创建网口并挂载到br0上，分别是vxlan0和一对veth pair端口（一端在br0上，另一端在container上） 3、vxlan0用于建立vxlan tunnel，vxlan端口的vni由docker-daemon创建时分配，只有具有相同的vni才能通信。 4、docker主机集群通过key/value存储共享数据，在7946端口上，相互之间通过gossip协议学 习各个宿主机上运行了哪些容器。守护进程根据这些数据来vxlan设备上生成静态MAC转发表。 5、vxlan设备根据MAC转发表，通过主机上的4789端口将数据发到目标节点。 6、根据流量包的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。 7、宿主机的overlay网络的网络命名空间中的br0网桥，起到虚拟交换机的作用，在对端网络中，将流量根据MAC地址转发到对应容器内部。 查看namespace由于容器和overlay的网络的网络命名空间文件不在操作系统默认的/var/run/netns下，只能手动通过软连接的方式查看（此时要保证两个容器在运行）： 1ln -s /var/run/docker/netns /var/run/netns 在另外一台设备上进行查看会发现，都有一个2-9ff9fe9875的命名空间。 然后我们可以进入到这里面去查看相应的网桥和网口信息： 网桥上的插的网口： 然后查看vxlan接口的详细信息： 可以看见它的vni号为256 总结： 关于使用overlay通信存在着一下缺点： 1、由于vxlan网络与宿主机网络默认不再同一网络环境下，为了解决宿主机与容器的通信问题，docker为overlay网络中的容器额外增加了网卡eth1作为宿主机与容器通信的通道。这样在使用容器服务时，就必须根据访问性质的不同，选择不同的网卡地址，造成使用上的不便。 2、容器对外暴露服务仍然只能使用端口绑定的方式，外界无法简单地直接使用容器IP访问容器服务。 3、从上面的通信过程中来看，原生的overlay网络通信必须依赖docker守护进程及key/value存储来实现网络通信，约束较多，容器在启动后的一段时间内可能无法跨主机通信，这对一些比较敏感的应用来说是不可靠的。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker（四）]]></title>
    <url>%2F2019%2F07%2F09%2Fdocker-3%2F</url>
    <content type="text"><![CDATA[为none网络添加网卡 docker中的网络接口默认都是虚拟接口。虚拟接口最大的优势就是转发效率高。这是因为Linux通过在内核中进行数据复制来实现虚拟接口之间的数据转发，即发送接口的发送缓存中的数据包将被直接复制到接收接口的接收缓存中，而不需要通过外部物理网络设备进行交换。这样，对于本地系统和容器内系统来看，虚拟接口跟一个正常的以太网卡没有啥区别。 思路： 网络创建过程： 1、创建一对虚拟接口，分别放在本地主机上和容器里面 2、本地的虚拟接口接在docker0网桥上，并且具有一个一veth开头的名字 3、另一端的接口将放在新创建的容器的命名空间里面，并修改为eth0 4、从网桥可用地址中获取一个空闲地址分配给容器的eth0，并配置地址默认路由网关为docker0网卡的内部接口docker0的ip地址。 过程： 1、我们先启动一个none网络容器 1docker run -it --rm --net=none ubuntu:16.04 /bin/bash 2、在本机查找它的进程号，然后在本地创建它的一个网络命名空间 12docker inspect -f &apos;&#123;&#123;.State.Pid&#125;&#125;&apos; 6d9454` 注：容器号可通过docker ps获取 然后为了方便我们设置一个变量： pid=进程号 12mkdir -p /var/run/netnsln -s /proc/$pid/ns/net /var/run/netns/$pid 3、检查桥接网卡的ip和掩码 1ip addr show docker0 4、创建一对veth pair的虚拟接口，绑定A在docker0上 123ip link add A type veth peer name Bbrctl addif docker0 Aip link set A up 5、将B放在容器里面的网络命名空间里面，命名为eth0，并配置 12345ip link set B netns $pidip netns exec $pid ip link set dev B name eth0ip netns exec $pid ip link set dev eth0 upip netns exec $pid ip addr add 172.17.0.20/16 dev eth0ip netns exec $pid ip route add default via 172.17.0.1 端口映射 如果想让外部访问docker内的应用，可以通过-P或者-p参数来进行端口的映射，当用-P的时候，此时，系统随机映射一个49000-49900的端口到内部容器的开放端口；而通过-p则是由人员指定映射的端口。 12345678910111213-p hostport:containport，映射多个端口时，多次使用-p参数。映射指定地址的指定端口时：-p ip:hostport:containport映射到指定地址的任意端口：-p ip::containport还可以在后面使用协议标记来指定什么协议端口的映射（tcp\udp）-p hostport:containport/udp查看端口映射的配置：docker port命令或者是docker logs命令 点对点通信 通过上面的实验，我们可以拓展一下，将放在docker0的接口放在另一个容器中，去实现一个点对点的通信。这样就不用通过主机网桥进行一个桥接。 大致过程： 12345678910111213141516171819202122232425262728293031首先启动两个容器：docker run -i -t --rm --net=none debian /bin/bashdocker run -it --rm --net=none debian /bin/bash查找进程号，创建网络命名空间的跟踪文件：docker inspect -f &apos;&#123;&#123;\.State\.Pid&#125;&#125;&apos; 440docker inspect -f &apos;&#123;&#123;\.State\.Pid&#125;&#125;&apos; 94dmkdir -p /var/run/netnsln -s /proc/644/ns/net /var/run/netns/644ln -s /proc/502/ns/net /var/run/netns/502创建一对虚拟网卡：ip link add A type veth peer name B配置：ip link set A netns 644ip netns exec 644 ifconfig A 10.1.1.1/24 upip link set dev B netns 502ip netns exec 502 ifconfig B 10.1.1.2/24 up此时测试ping，如果ping不通，可配置静态路由：ip netns exec 644 ip route add 10.1.1.2/32 dev Aip netns exec 502 ip route add 10.1.1.1/32 dev B 注：644,502是进程号 容器互联 通过容器的互联让多个容器中的应用进行快速交互，它会在源和接收容器之间创建连接关系，接收容器可以通过容器名来快速发送源容器而不需要具体的ip地址。 使用–link name:alias参数可以让容器之间安全地进行交互 比如： 先创建一个数据库容器： 1docker run -d --name db training/postgres 然后创建一个web容器连接到数据库容器上： 1docker run -d -P --name web --link db:db training/webwebapp python app.py 可用docker ps来查看连接情况，docker此时相当于在两个互联的容器之间创建了一个虚拟的通道，而且不用映射他们的端口在宿主主机上，在启动db容器的时候没有使用端口映射，这样就避免的数据库暴露在外。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（九）]]></title>
    <url>%2F2019%2F06%2F25%2Fzabbix-8%2F</url>
    <content type="text"><![CDATA[SNMP 监控 如果需要监控打印机、路由器等设备，需要用到SNMP协议来完成，因为他们无法安装软件和操作系统。 此时在安装的时候需要安装snmp的支持，服务器可使用snmp agent来获取这些设备的信息。 Server设置 1、安装SNMP相应服务： 1yum install -y net-snmp* 2、编辑配置文件（/etc/snmp/snmpd.conf） 3、启动 12systemctl enable snmpdsystemctl restart snmpd Client设置（Linux环境） 1、安装SNMP相应服务： 1yum install -y net-snmp* 2、编辑配置文件（/etc/snmp/snmpd.conf），和server有点区别 3、启动 12systemctl enable snmpdsystemctl restart snmpd 4、放行防火墙（它是通过UDP161端口通信） 12firewall-cmd --add-port=161/udp --permanentfirewall-cmd --reload Client设置（Windows server环境） 1、打开服务器管理器 ------&gt;功能---------&gt;添加功能 2、选择SNMP服务，并安装 3、设置 4、放行防火墙（UDP161） 5、添加主机监控 6、添加监控项 我们需要找出要监控项目得SNMP字符串（或者OID），可以通过snmpwalk命令来找： 1snmpwalk -v 2c -c public &lt;host IP&gt; 2c表示SNMP标准版本,snmp推出了v1,v2,v3版本,你也可以 写成1,表示使用1版本.上面的命令会获取到一个SNMP的列表,包含键值，默认情况下我们snmp不加密，使用public作为共同体即可,这些列表中 有你需要的一些监控数据. 找到我们所需要的键值，当然也可以来获取某一个的OID值： 这里的ifInOctets.2的2表示我们需要监听的端口号，要特别注意的是3COM的端口号1是101，3是是103，但是cisco还是不变，1号还是数字1。 这个时候我们再去创建监控项： 将我们查询到的OID填入相应位置 需要注意的是： 1、如果SNMPv3凭据（安全名称，验证协议/口令，隐私协议）错误，Zabbix会从net-snmp收到错误，如果 私钥 错误，在这种情况下，Zabbix会从net-snmp收到TIMEOUT错误。 2、如果上面com2sec中，团体属性不是public的话，此时需要在zabbix服务器进行配置（服务器默认是public） 3、OID可以以数字或字符串形式给出。但是，在某些情况下，字符串OID必须转换为数字表示。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（三）]]></title>
    <url>%2F2019%2F05%2F18%2Fk8s-2%2F</url>
    <content type="text"><![CDATA[pod的定义文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576apiVersion: v1 //当前配置文件的版本kind: Pod //表示创建的资源类型metadata: //资源的元数据 name: string //name是必须的元数据项，用来标识pod的名称 namaspace: string //所属的命名空间，默认为default labels: //自定义标签 - name: string annotations: //自定义注解 - name: string spec: //pod中容器的详细定义 containers: //pod中的容器列表 - name: string //容器的名称 images: string //容器的镜像名称 imagePullPolice: [Always | Never | IfNotPresent] //获取镜像的策略，always表示每次都重新下载镜像；never表示表示仅使用本地镜像；ifnotpresent表示优先本地，如果没有再下载 command: [string] //容器的启动命令列表 args: [string] //容器启动命令参数 workingDir: string //容器的工作目录 volumeMounts: //挂载到容器内部的存储卷 - name: string //该卷的名称 mountPath: string //在容器内部挂载的绝对路径 readOnly: boolean //是否以只读模式挂载，默认是读写 ports: - name: string containerPort: int //容器需要监听的端口号 hostPort: int //容器所在主机所要监听的端口号 protocol: string //端口协议 env: //环境变量 - name: string //环境变量名 value: string //环境变量值 resources: //资源限制和资源请求的设置 limits: //限制的设置 cpu: string memory: string requests: //请求的限制 cpu: string memory: string livenessProbe: //容器健康检查的设置 exec: command: [string] //相应的命令或者是脚本 httpGet: //健康检查的设置，HTTPGet的方式，需要指定path，port path: string port: int host: string scheme: string httpHeaders: //头部的添加 - name: string value: string tcpSocket: port: int initialDelaySeconds: number timeoutSeconds: number periodSeconds: number //定期探测时间设置 successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] //重启策略 nodeSelector: object imagePullSecrets: - name: string hostNetwork: false volumes: //共享卷的列表 - name: string emptyDir: &#123;&#125; //类型为emptyDir的存储卷，表示与pod同生命周期的临时目录，值为一个空对象 hostPath: //类型为hostpath的存储卷，表示挂载pod所在宿主机的目录 path: string //该目录所在路径 secret: secretName: string items: - key: string path: string configMap: name: string items: - key: string path: string 关于pod的操作 操作 命令 创建 kubectl create -f frontend-localredis-pod.yaml 查询Pod运行状态 kubectl get pods --namespace=&lt;NAMESPACE&gt; 查询Pod详情 kebectl describe pod &lt;POD_NAME&gt; --namespace=&lt;NAMESPACE&gt; 删除 kubectl delete pod &lt;POD_NAME&gt; ; kubectl delete -f pod.yaml 更新 kubectl replace pod.yaml 查看容器的日志 kubectl logs &lt; pod-name &gt;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（二）]]></title>
    <url>%2F2019%2F05%2F10%2Fk8s-1%2F</url>
    <content type="text"><![CDATA[通过kubeadm安装部署 环境： 192.168.248.140 centos7 master 192.168.248.141 centos7 node1 192.168.248.142 centos7 node2 1、关闭防火墙和selinux（三台设备） 1234systemctl stop firewalldsystemctl disable firewalldsetenforce 0sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/g&apos; /etc/selinux/config 2、永久关闭交换分区（三台设备） 对于k8s而言，它的想法是将实例紧密包装到尽可能接近100%。所有的部署应该与CPU/内存限制固定在一起。所以如果调度程序发送一个pod到一台机器，他不应该使用交换。所以关闭swap主要是为了性能考虑。 123swapoff -asysctl -w vm.swappiness=0vim /etc/fstab #注释掉交换分区挂载 3、修改主机名（三台设备） 1234hostnamectl --static set-hostname masterhostnamectl --static set-hostname node1hostnamectl --static set-hostname node2#logout重新登陆即可生效 4、设置主机名的映射（三台设备） 1234vim /etc/hosts #添加以下内容并ping测试 192.168.191.138 master 192.168.191.139 worker1 192.168.191.140 worker2 5、安装docker（三台设备） 参考我的另一篇有关docker的文章。 6、安装kubelet、kubeadm、kubectl（三台设备） 12345678910cat&gt;&gt;/etc/yum.repos.d/kubrenetes.repo&lt;&lt;EOF[kubernetes]name=Kubernetes Repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpgEOFyum install -y kubelet kubeadm kubectlsystemctl enable kubelet &amp;&amp; systemctl start kubelet 此时我们查看一下kubectl版本(kubelet version），后面会用到这个版本号。 7、安装基础镜像（三台设备，这个的话需要翻墙） 1234567891011121314151617181920212223242526docker pull mirrorgooglecontainers/kube-apiserver:v1.14.1docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.1docker pull mirrorgooglecontainers/kube-scheduler:v1.14.1docker pull mirrorgooglecontainers/kube-proxy:v1.14.1docker pull mirrorgooglecontainers/pause:3.1docker pull mirrorgooglecontainers/etcd:3.3.10docker pull coredns/coredns:1.3.1docker pull docker.io/dockerofwj/flanneldocker tag mirrorgooglecontainers/kube-apiserver:v1.14.1 k8s.gcr.io/kube-apiserver:v1.14.1docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.1 k8s.gcr.io/kube-controller-manager:v1.14.1docker tag mirrorgooglecontainers/kube-scheduler:v1.14.1 k8s.gcr.io/kube-scheduler:v1.14.1docker tag mirrorgooglecontainers/kube-proxy:v1.14.1 k8s.gcr.io/kube-proxy:v1.14.1docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1docker tag docker.io/dockerofwj/flannel quay.io/coreos/flannel:v0.10.0-amd64docker image rm mirrorgooglecontainers/kube-apiserver:v1.14.1docker image rm mirrorgooglecontainers/kube-controller-manager:v1.14.1docker image rm mirrorgooglecontainers/kube-scheduler:v1.14.1docker image rm mirrorgooglecontainers/kube-proxy:v1.14.1docker image rm mirrorgooglecontainers/pause:3.1docker image rm mirrorgooglecontainers/etcd:3.3.10docker image rm coredns/coredns:1.3.1docker image rm docker.io/dockerofwj/flannel 8、初始化master 这里根据K8s版本初始化，并将api声明ip修改为master-ip。这里的pod网络因为我后面用的是flannel网络，所以这里必须是10.244.0.0/16 1kubeadm init --kubernetes-version=v1.14.1 --apiserver-advertise-address 192.168.248.140 --pod-network-cidr=10.244.0.0/16 如果成功的话，如图： 记得将* kubeadm join*这条命令记下，后面加入节点会用。 9、设置环境变量（master） 123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 10、配置kubectl并验证 123echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; /etc/profilesource /etc/profile echo $KUBECONFIG #查看是否输出正确内容 11、设置系统参数（master） 1sysctl net.bridge.bridge-nf-call-iptables=1 12、安装flannel网络（master） 有两种方法： 方法一： 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 方法二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473vim kube-flannel.yaml#添加以下内容kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelrules: - apiGroups: - &quot;&quot; resources: - pods verbs: - get - apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | &#123; &quot;name&quot;: &quot;cbr0&quot;, &quot;plugins&quot;: [ &#123; &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: &#123; &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true &#125; &#125;, &#123; &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: &#123; &quot;portMappings&quot;: true &#125; &#125; ] &#125; net-conf.json: | &#123; &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: &#123; &quot;Type&quot;: &quot;vxlan&quot; &#125; &#125;---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-arm64 namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-arm namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: arm tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-arm command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-arm command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-ppc64le namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: ppc64le tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-ppc64le command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kube-flannel-ds-s390x namespace: kube-system labels: tier: node app: flannelspec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: s390x tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.10.0-s390x command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-s390x command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfgkubectl apply -f kube-flannel.yaml 13、node加入集群 通过上面master初始化后生成的join命令输入： 如果说清屏了，没找到，可重新生成： 1kubeadm token create --print-join-command 最后我们可以通过kubectl get nodes查看加入的节点信息。 此时是因为每个节点都需要启动若干组件，这些节点都是在pod中运行，我们可以通过kubectl get pod --all-namespaces查看哪些组件没有处于running状态。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（一）]]></title>
    <url>%2F2019%2F05%2F09%2Fk8s%2F</url>
    <content type="text"><![CDATA[关于k8s 官方文档： Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。 k8s相关概念 参考于《kubernetes权威指南》（小声逼逼：贼tm厚，看崩了） 1、master 它的主要职责是调度，它决定了将应用放在哪里去运行，为了实现高可用，可以运行多个master。在master节点上运行着下面一组关键进程： kubernetes API server（kube-apiserver）：提供HTTP Restful接口的关键服务进程，是kubernetes 里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口。 kubernetes controller manager（kube-controller-manager）：所有资源对象的控制中心。 kubernetes schedule（kube-schedule）：负责（pod）调度的进程。 etcd：分布式的存储系统，用于配置共享和服务发现。在这里，kubernetes里的所有资源对象的数据都保存在etcd中。 2、node node的职责是运行容器应用，它由master管理，它负责监控并汇报容器的状态，同时根据master的要求去管理容器的生命周期。它也运行着一些关键的进程： kubelet：负责pod对应的容器的创建、启停等，同时与master紧密联系，实现集群管理的基本功能。默认情况下，它会向master注册自己。 kube-proxy：实现kubernetes service的通信与负载均衡机制的重要组件。 docker engine：负责本机的容器创建和管理工作。 12kubectl get nodes //查看节点kubectl describe node node1 //查看某一节点的详细信息 如图，我们获取node1节点的详细信息，其中包含了以下信息： node基本信息：名称、标签，创建时间 node当前的运行状态，当node启动后会进行一系列的自检，包括磁盘满了没有呀（满了就标记OutOfDisk=True ），然后检查内存够不够呀（不够就标记MemoryPressure=True），一直检查完如果没啥故障就标记Ready=True node的主机名和主机地址 node上的资源总量（在capacity里）：包括CPU、内存数量、最大可调度的pod数量等等 node上可分配的资源（Allocatable） 主机系统的信息，包括UUID号，系统、内核、docker版本、kubelet版本等等 当前运行的pod简单的信息 已经分配的资源（CPU的分配是以千分之一的配额为单位，用m表示；内存是用字节数为单位） node相关的event信息（也就是日志信息） 3、pod 容器提供了强大的隔离功能，所以有必要把为service提供服务的这组进程放入容器中进行隔离。为此，k8s设计了pod对象，它是一组容器的集合，通常会将紧密相关的一些容器放入到pod中，同一个pod中所有的容器共享IP地址和port空间，也就是说它们在同一个namespace里面，pod是k8s最小的调度单位，在同一个pod中的容器始终被一起调度。 pod运行在一个node节点中，可以是物理机，也可以是私有云或者公有云中的一个虚拟机，通常在一个节点上运行上百个pod，其次，每个pod里运行着一个特殊的被称作为pause的容器，用它的状态来代表整个容器组的状态；其他容器则为业务容器，这些业务容器共享pause容器的网络栈（如IP）和volume挂载卷。 此外，k8s为每个pod都分配了唯一的IP（pod IP），底层网络通过虚拟二层网络（flannel，open vswitch）来实现集群内的任意两个pod之间进行TCP/IP通信 。 static pod：不存放在etcd中，而是放在具体node上一个文件里面，只在这个node上运行 普通pod：一旦被创建，就放进etcd里面，然后被调度到某一个node上去binding，接着会被kubelet给实例化成一组docker。如果说它的其中一个docker挂了，k8s会自动去检测，并重启整个pod；如果是node挂了，则node所在的所有pod都会被调度给其他的node。 endpoint：pod IP加上容器端口，用来代表pod里的一个服务进程对外提供的一个通信地址。 4、label标签 一个label是一个key=value的键值对，我们可以通过给指定的资源对象捆绑一个或者多个label来实现多维度的资源分组管理，这样可以很灵活的进行资源的管理、分配、调度等等。 比如： 1234* 版本标签：“release”：“stable”，“release”：“canary”* 环境标签：“enviroment”：“dev”，“enviroment”：“production”* 架构标签：“tier”：“backend”，“tier”：“middleware”* 分区标签：“partition”：“customerA” 当给某个资源打了标签以后我们可以通过label selector进行查询和筛选，可以将它类比于sql语句中的where条件句。 比如： 12345678910基于等式的：* name=redis-slave //匹配所有具有标签为name=redis-slave的资源对象* env!=production //匹配所有不具有标签env=production的资源对象基于集合的：* name in (redis-master,redis-slave) //匹配所有具有标签为name=redis-master或者name=redis-slave的资源对象* env not in production //匹配所有不具有标签env=production的资源对象可以通过表达式的组合实现复杂的条件选择，用逗号隔开：name=redis-slave，env!=production 5、controller k8s并不会直接创建pod，而是通过controller来管理pod，controller中定义了pod的部署特性，比如有几个副本，在什么样的node上运行等。 deployment：最常用的controller，它可以管理pod的多个副本，并确保pod按照期望的状态运行。 replicaset：实现了pod的多副本管理。使用deployment时会自动创建replicaset，也就是说deployment时通过它来管理副本的。（后面可能会详细的讲一下这个） daemonset：用于每个node最多只运行一个pod副本，通常用于daemon。 statefulset：能够保证pod的每个副本在整个生命周期中保持名称不变，而其他controller不提供这个功能。当某个pod发生故障的时候需要删除并且重新启动，pod的名称会发生变化，同时statefulset会保证副本按照固定的顺序启动、更新或者删除。 job：用于运行结束就删除的应用，而其他controller中的pod通常是长期持续运行。 6、service 在kubernetes中，service是分布式集群架构的核心，一个service对象拥有如下特征： 拥有一个唯一指定的名字 拥有一个虚拟IP和端口号 能够提供某种远程服务能力 被映射到了提供这种服务能力的一种容器应用上 service的服务进程目前都基于socket通信方式对外提供服务 7、集群管理 k8s将集群中的机器划分为一个master节点和一群工作节点（node），其中，在master节点上运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager、kube-scheduler，这些进程是实现整个集群的资源管理、pod调度、弹性伸缩、安全控制、系统监控和纠错的功能，并且都是全自动生成的。node作为集群中的工作节点，运行真正的应用程序，node上运行着k8s的kubelet、kube-proxy服务进程，这些进程负责pod的创建、启动、监控、重启、销毁以及实现软件模式的负载均衡器。 8、namespace 通过namespace将物理的cluster逻辑上划分成多个虚拟的cluster，每个cluster就是一个namespace，不同的namespace里面的资源就是完全隔离的。k8s在启动以后，会创建一个名为“default”的namespace，我们可以通过kubectl get namespaces查看存在的namespace有哪些。如果不特别指出namespace，则我们所创建的pod、RC、service都将被放进这里面。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker(三)]]></title>
    <url>%2F2019%2F05%2F09%2Fdocker-2%2F</url>
    <content type="text"><![CDATA[关于docker的数据管理 直接在容器里面写入数据是不好的习惯，但是可以通过数据卷和数据卷容器的方式来写入。 1、数据卷 数据卷的使用和Linux挂载文件目录是很相似的。简单的说，数据卷就是一个可以供容器使用的特殊目录。 它提供了很多特性： 可以在容器之间共享和重用，使得数据在容器间进行传递变得高效和方便。 对数据卷内数据的修改马上生效。 对数据卷的更新不会影响镜像，将应用和数据分割。 卷会一直存在，直到没有容器使用 创建一个数据卷（run命令加参数-v） 1docker volume create -d local test 此时我们在/var/lib/docker/volumes路径下，会发现所创建的数据卷： 删除一个数据卷 数据卷是用来持久化数据的，所以数据卷的生命周期独立于容器。因此在容器结束后数据卷并不会被删除，如果需要删除数据卷，可以在docker rm删除容器的时候加上-v参数。 此外，如果删除挂载某个数据卷的容器没有使用-v参数清理，那么以后会很难清理。 挂载一个主机目录到容器 1docker run -ti --name volume2 -v /home/zsc/Music/:/myShare ubuntu:16.04 bash 以上指令会把宿主主机的目录 /home/zsc/Music 挂载到容器的 myShare 目录下，然后你可以发现我们容器内的 myShare 目录就会包含宿主主机对应目录下的文件。 直接挂载宿主主机目录作为数据卷到容器内的方式在测试的时候很有用，你可以在本地目录放置一些程序，用来测试容器工作是否正确。当然，Docker 也可以挂载宿主主机的一个文件到容器，但是这会出现很多问题，所以不推荐这样做。如果你要挂载某个文件，最简单的办法就是挂载它的父目录。 2、数据卷容器 所谓数据卷容器，其实就是一个普通的容器，只不过这个容器专门作为数据卷供其它容器挂载。 首先，在运行 docker run 指令的时候使用 -v 参数创建一个数据卷容器： 1sudo docker run -ti -d -v /dataVolume --name v0 ubuntu:16.04 然后，创建一个新的容器挂载刚才创建的数据卷容器中的数据卷：使用 --volumes-from 参数 1sudo docker run -ti --volumes-from v0 --name v1 ubuntu:16.04 bash 注意： 1、数据卷容器被挂载的时候不必保持运行！ 2、如果删除了容器 v0 和 v1，数据卷并不会被删除。如果想要删除数据卷，执行 docker rm 命令的时候使用 -v 参数。 迁移数据 可以利用数据卷容器对其中的数据卷进行备份、恢复以实现数据的迁移。 1、备份 1docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata 这条命名是先利用Ubuntu镜像创建一个worker容器使用–volumes-from dbdata 来让worker容器挂载数据卷容器的数据，然后使用-v $(pwd):/backup 来挂载本地的当前目录到容器的backup目录下，然后当容器启动后使用tar命令进行备份，最后同步到本地。 2、恢复 如果要恢复数据到一个容器，首先创建一个带有数据卷的容器 1docker run -v /dbdata --name dbdata2 ubuntu /bin/bash 然后创建另一个新的容器，挂载dbdata2的容器，并使用untar解压备份文件到所挂载的容器中 1docker run --volumes-from dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar 关于docker底层的一些东东 1、联合文件系统 docker将我们物理机的一些文件是以挂载的方式挂载在容器中，此时我们进行mount时使用只读模式，来降低恶意进程的攻击；其次，实现写入时复制，所有的容器可以先共享一个基本文件系统镜像，一旦需要向文件系统写入数据，就引导它写到与该容器相关的另一个特定文件系统中去。docker的存储文件系统是采用联合文件系统，其中它采用overlay和overlay2（现在默认是这个）存储文件驱动。overlayFS是一个类似于AUFS的现代联合文件系统，是内核提供的文件系统。 overlay原理： overlayFS将单个Linux主机上的两个目录合并成一个目录，这些目录被称为层，这一过程被称为联合挂载。OverlayFS底层目录称为lowerdir，高层目录称为upperdir。合并统一视图称为merged。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在docker中，底下的只读层就是image，可写层就是container。 overlay镜像结构 在/var/lib/docker/overlay/下（如果docker此时是overlay的话），每一个镜像都有一个对应的目录，包含了各层镜像的内容。而每一个镜像目录中包含的有：lower（用来记录下层镜像层，upper包含了容器层的内容，创建容器时将lower-id指向的镜像层目录以及upper目录联合挂载到merged目录。work用来完成如copy-on_write的操作） 对文件操作 读操作： 如果文件在容器层中不存在，则从lowerdir中读取 只在容器层存在，则直接从容器中读取该文件 文件存在容器和镜像层，容器层upperdir会覆盖lowerdir中的文件 写操作： 首次写入： 在upperdir中不存在，overlay和overlay2执行copy_up操作，把文件从lowdir拷贝到upperdir，由于overlayfs是文件级别的（即使文件只有很少的一点修改，也会产生的copy_up的行为），copy_up操作只发生在文件首次写入，以后都是只修改副本 ，overlayfs只适用两层，因此性能很好，查找搜索都更快 删除文件和目录： 当文件在容器被删除时，在容器层（upperdir）创建whiteout文件，镜像层的文件是不会被删除的，因为他们是只读的，但whiteout文件会阻止他们展现，当目录在容器内被删除时，在容器层（upperdir）一个不透明的目录，这个和上面whiteout原理一样，阻止用户继续访问，即便镜像层仍然存在 性能 页缓存：overlay支持页缓存共享，也就是说如果多个容器访问同一个文件，可以共享同一个页缓存，高效利用了内存 copy_up:aufs和overlayfs，由于第一次写入都会导致copy_up，尤其是大文件，会导致写延迟，以后的写入不会有问题。由于overlayfs层级 比aufs的多，所以ovelayfs的拷贝高于aufs inode限制：使用overlay存储驱动可能导致inode过度消耗，特别是当容器和镜像很多的情况下，所以建议使用overlay2。 关于overlay2 overlay2的镜像结构，在/var/lib/docker/overlay2下有每一层的镜像，另外与overlay相比多了一个l/ 的目录，里面包含了很多软链接，使用短名称指向了其它层。 overlay2的容器结构 启动一个容器，也是在/var/lib/docker/overlay2目录下生成一层容器层，目录包括diff，link，lower，merged，work，其中：diff记录了每一层自己内容的数据，link记录了该层链接目录，merged是挂载点 两个的区别： 本质区别是镜像层之间共享数据的方法不同： overlay共享数据库是通过硬链接，只挂载一层，其它层通过最高层通过硬链接形式共享（增加了iNode的负担），而overlay2则是通过每层的lower文件。 2、namespace 对于PID命名空间，docker会将全部没有运行在当前开发容器中的进程隐藏起来，让恶意程序看不见，如果终止Pid为1的进程命名空间，容器里面所有的进程都会被终止；对于网络命令空间，管理员通过路由规则和iptable来构建容器的网络环境，这样的话容器内部的进程只能使用管理员许可的特定网络。 3、cgroup机制（control group） 通过cgroup机制来防止恶意的进程通过占有系统全部的资源来进行攻击（这是一种资源控制机制），它保证所有在一个控制组内的进程组成一个私密、隔离的空间，各进程拥有自己的进程号，并且无法访问组外部的其他进程，这样就形成了若干个相互隔离的区域。 4、docker的缓存大致介绍 我们在通过dockerfile构建镜像的时候，docker会按照指定的顺序去执行相应的指令，这个过程中，他会先查看是否有可用的重复的子镜像。并且它不是通过查看容器内文件来确定缓存的匹配，而是通过查看某一命令字符串是否与之前的一致来判断是否匹配，如果一致，则说明是由相同的命令来创建的子镜像，一旦没有，则生成新的。 5、关于docker的分层存储 docker镜像是一个特殊的文件系统。镜像提供了容器运行时所需的程序、库、资源、配置等资源，还包含了一些为运行时准备的一些配置参数。镜像是一个静态的概念。docker的设计者充分利用unions FS技术，把docker设计为分层存储的结构。也就是说，镜像是分层构建的，每一层是上面一层的基础，每一层在构建完成之后都不会在发生变化。这就说明了，构建镜像的时候我们要保证每一层都只包含我们的应用所需要的东西，不要包含不需要的文件，因为每一层在构建之后不再变化。分层存储还使得不同的镜像可以共享某些层，便于镜像的复用。 其中最底层是bootfs：包括引导系统的文件系统，包括bootloader和kernel，容器启动完成后会被卸载以节约内存资源。然后是rootfs：是docker的根文件系统，传统模式中，系统启动的时候，内核挂载rootfs时会首先将其挂载为“只读模式”，自检完成后重新挂载为读写模式，docker中，通过联合挂载技术为其额外挂载一个“可写层”。 最直观的就是我们在pull和commit的时候，镜像是分层处理的。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible（二）]]></title>
    <url>%2F2019%2F04%2F19%2Fansible-1%2F</url>
    <content type="text"><![CDATA[关于Ansible的主配置文件 Ansible安装完成后会默认在/etc/ansible目录下生成一个主配置文件ansible.cfg： 12345678910111213141516[defaults]# some basic default values...#inventory = /etc/ansible/hosts 主机列表配置文件#library = /usr/share/my_modules/ 库文件存放位置#module_utils = /usr/share/my_module_utils/ #remote_tmp = ~/.ansible/tmp 生成的临时py命令文件存放在远程主机的目录#local_tmp = ~/.ansible/tmp 本机的临时命令执行目录#forks = 5 默认的并发数#poll_interval = 15 默认的线程池#sudo_user = root 默认sudo用户#ask_sudo_pass = True#ask_pass = True#transport = smart#remote_port = 22 指定连接被管节点的管理接口#module_lang = C#module_set_locale = False 注：Ansible系统默认是不记录日志的，如果想把ansible系统的输出记录到日志文件中，需要设置log_path来指定一个存储ansible日志的文件，比如： log_path = /var/log/ansible.log 另外，执行ansible的用户需要有写入日志的权限，模块将会调用被控端的syslog来记录。 关于Inventory Ansible可同时操作属于一个组的多台主机，组和主机之间的关系通过inventory文件配置，默认的文件路径为/etc/ansible/hosts。当然除默认文件以外，还可以同时使用多个inventory文件，也可以从动态源或者云上拉取inventory配置信息。 主机与组 其中，方括号[]中是组名，用于对系统进行分类，便于对不同系统进行个别的管理。一个系统可以属于不同的组，比如一个服务器可以同时属于两个组。这时属于两个组的变量都可以为这台主机所用，当然会牵涉到优先级的情况。 如果有主机的SSH端口不是22的话，可在主机名之后加上端口号，用冒号进行分隔。SSH配置文件中列出的端口号不会在paramiko连接中使用，会在openssh连接中使用。 假如有一些静态IP地址，希望设置一些别名，但是不是在系统的host文件中设置，又或者是通过隧道在连接，那么可以通过jumper来设置： jumper ansible_ssh_port=5555 ansible_ssh_host=192.168.1.50 这个例子中，通过jumper别名，会连接到192.168.1.50:5555，这是利用了inventory文件的特性功能设置的变量。 对于每一个host，还可以选择连接类型和连接用户名： 主机变量 直接在/etc/ansible/hosts文件中，主机的后边设置key=value的格式，这些变量定义后可在playbooks中使用。 主机组变量 分文件来定义我们的host和group变量 这些文件相互独立，但是与inventory文件保持关联，这样易于管理和区分，不同的是，这些文件的语法是基于YAML。 假设有一个主机名为 ‘foosball’, 主机同时属于两个组,一个是 ‘raleigh’, 另一个是 ‘webservers’. 那么以下配置文件(YAML 格式)中的变量可以为 ‘foosball’ 主机所用.依次为 ‘raleigh’ 的组变量,’webservers’ 的组变量,’foosball’ 的主机变量: 举例来说，假设你有一些主机，属于不同的数据中心，并依次进行划分。每一个数据中心使用一些不同的服务器。比如 ntp 服务器, database 服务器等等。 那么 ‘raleigh’ 这个组的组变量定义在文件 ‘/etc/ansible/group_vars/raleigh’ 之中,可能类似这样: 当然，还可以为一个主机或一个组，创建一个目录，目录名就是主机名或者组名，目录中可以创建多个文件，文件中的变量都会被读取为主机或组的变量。如下 ‘raleigh’ 组对应于 /etc/ansible/group_vars/raleigh/ 目录,其下有两个文件 db_settings 和 cluster_settings, 其中分别设置不同的变量: 这样的话当变量很多的时候便于我们进行管理。那么我们怎么去使用这些变量呢，比如我们在执行playbook的时候，可以通过指定变量文件来获取变量，比如我们在一个变量文件（server_vars.yml）中定义了一个变量（apache_config: labs.conf），此时playbook中是： 1234567---- hosts: webservers vars_files: - vars/server_vars.yml tasks: - name: deploy haproxy config template: src=&#123;&#123; apache_config &#125;&#125; dest=/etc/httpd/conf.d/&#123;&#123; apache_config &#125;&#125; 需要注意的是：Ansible 1.2 及以上的版本中,group_vars/ 和 host_vars/ 目录可放在 inventory 目录下，或是 playbook 目录下。如果两个目录下都存在，那么 playbook 目录下的配置会覆盖 inventory 目录的配置。 多个Inventory文件 比如我们在etc/ansible下创建一个inventory目录，下面包含两个目录，一个是docker，一个是hosts，我们用不同的文件来存放不同的主机，只需要修改配置文件中inventory的值，让它指向/etc/ansible/inventory这个目录就行。这样做的话，我觉得可以把不同业务的主机分开管理，包括其变量值这些。 关于inventory的参数 ansible_ssh_host 将要连接的远程主机名。与你想要设定的主机的别名不同的话，可通过此变量设置。 ansible_ssh_port ssh端口号。如果不是默认的端口号，通过此变量设置。 ansible_ssh_user 默认的 ssh 用户名。 ansible_ssh_pass ssh 密码(这种方式并不安全，我们强烈建议使用 --ask-pass 或 SSH 密钥)。 ansible_sudo_pass sudo 密码(这种方式并不安全，我们强烈建议使用 --ask-sudo-pass)。 ansible_sudo_exe (new in version 1.8) sudo 命令路径(适用于1.8及以上版本)。 ansible_connection 与主机的连接类型。比如:local, ssh 或者 paramiko。Ansible 1.2 以前默认使用 paramiko，1.2 以后默认使用 ‘smart’，‘smart’ 方式会根据是否支持 ControlPersist， 来判断’ssh’ 方式是否可行。 ansible_ssh_private_key_file ssh 使用的私钥文件。适用于有多个密钥，而你不想使用 SSH 代理的情况。 ansible_shell_type 目标系统的shell类型。默认情况下，命令的执行使用 ‘sh’ 语法，可设置为 ‘csh’ 或 ‘fish’。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible（一）]]></title>
    <url>%2F2019%2F04%2F18%2Fansible%2F</url>
    <content type="text"><![CDATA[开始之前先聊聊：这久太忙，都么得整理笔记和学自己想学的东西。真的Netty和OneNet直接要命，自己又忙着投简历等等，都么得时间弄自己的事，唉。关于Ansible的话，感觉东西也很多，慢慢学吧。 现在进入正题吧。 关于Ansible我找到的可以学的网站的话，一个是：Ansible中文权威指南，一个是W3Cschool，顺便一提，这个网站是个神奇的网站，以前只是用来学前端，但是现在感觉啥都能找到。 关于Ansible 是这样说的： 1、Ansible是个与puppet、saltstack、chef一样功能强大的组态设定工具。 2、Ansible提供一种最简单的方式用于发布、管理和编排计算机系统的工具，使得管理者可以在数分钟内搞定。 3、Ansible是一个模型驱动的配置管理器，支持多节点发布，远程任务执行。默认使用SSH进行远程连接，无需在被管理节点上安装附加软件，可使用各种编程语言进行扩展。 特性 1、拥有模块化的设计，Ansible能够调用特定的模块来完成特定的任务，本身是核心组件。 2、Ansible基于python语言实现，由paramiko（python的一个可并发连接ssh主机功能库），PyYAML和Jinja2三个关键模块实现。 3、Ansible的部署比较简单，被控端不需要安装客户端工具。 4、以主从模式工作。 5、支持自定义模块功能。 6、支持playbook剧本，连续任务先后设置顺序完成。 7、每个命令具有幂等性。 Ansible的一些组件 从上图我们可以看见，它大致包含这些东西： 1、连接插件connection plugins：负责和被控端实现通信。 2、host inventory：指定操作的主机，是一个配置文件里面定义监控的主机。 3、各种模块包括核心模块、command模块（默认），自定义模块等。 4、借助于插件完成记录日志邮件等功能。 5、playbooks 安装（centos）和公钥下发 安装 12yum install -y epel-releaseyum install -y ansible 公钥下发 12ssh-keygen -P &quot;&quot;ssh-copy-id -i /root/.ssh/id_rsa.pub root@[node2] Ansible工具默认主目录为/etc/ansible，此时我们 ls -l 一下： 其中： 1、ansible.cfg：是Ansible的主配置文件，以ini格式存储配置数据，在Ansible中，几乎所有的配置项都可以通过Ansible的playbook或环境变量来重新赋值。在运行命令的时候，命令将会按照预先设定的顺序来查找配置文件： * ANSIBLE_CONFIG：会先检查环境变量，以及这个变量指向的配置文件。 * /.ansible.cfg：当前目录下的ansible.cfg配置文件。 * ~/.ansible.cfg：当前用户家目录（home）下的ansible.cfg文件。 * /etc/ansible.cfg：最后是安装时自动生成的配置文件。 2、hosts：是Ansible的inventory档案，里面配置了被控的主机列表，可以配置分组，可以定义IP以及相关规则。 3、roles：为角色或者插件路径，默认空的。 现在，我们简单测试一下： 12echo -e &quot;[local]\n localhost ansible_connection=local&quot; &gt;&gt; /etc/ansible/hostsansible localhost -m command -a &quot;echo hello world&quot; 此时，会看见输出： Ansible命令的执行过程 1、加载自己的配置文件 2、加载自己对应的模块文件 3、通过ansible将模块或者命令生成对应的临时py文件，并将该文件传输至远程服务器 4、对应执行用户的家目录的.ansible/tmp/XXX/XXX.py文件 5、赋予文件执行权限，执行并返回结果 6、删除临时py文件，sleep 0退出]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（七）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-6%2F</url>
    <content type="text"><![CDATA[关于IS-IS实验的某些思考 1、拓扑如图： 我们配置完基础配置后，此时通过show ip route查看时，发现：R2和R3的lookback接口来自本区域，因此为L1的路由这个没有问题，但是R2-R4、R3-R4直连链路也进来了？ 这是因为R2和R3的直连接口。我们要想在49.0001里面不要通告直连链路的LSP，可以通过在R2的s1/0以及R3的s1/0配置为level-2，也就是接口级别： isis circuittype level2 2、关于汇总 IS-IS汇总要在本区域的ABR上进行汇总，如果有多个ABR，则都要进行，此时会下发一条汇总路由给其他L2，而本区域内的L1依然有明细路由。否则的话明细路由会通过其他没有汇总的ABR下发出去。 3、关于IS-IS的认证 这里有份官方文档 分为三种类型： 接口认证：是指使能IS-IS协议的接口以指定方式和密码对Level-1和Level-2的Hello报文进行认证。 区域认证：是指运行IS-IS的区域以指定方式和密码对Level-1的SNP和LSP报文进行认证。 路由域认证：是指运行IS-IS的路由域以指定方式和密码对Level-2的SNP和LSP报文进行认证。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（六）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-5%2F</url>
    <content type="text"><![CDATA[IS-IS的综合实验 拓扑如图： 要求： 1234567891011121314151、根据拓扑环境，配置直连IP地址，保证底层的联通性。2、在不同路由器对应接口和环回接口上开启集成ISIS。3、R1-R2-R3处于区域49.0001、R4处于区域49.0002、R5-R6处于区域49.0003、R7-R8处于49.0004 。ISIS路由器的system id规划为：R1=0000.0000.0001，R2=0000.0000.0002其余的依次类推。4、要求在所有路由器上开启ISIS的日志功能用来检测运行。5、修改对应路由器的层次类型。6、通过在接口下定义电路类型来限制ISIS没必要的分组。7、通过修改接口或者链路的开销来实现选路的优化，要求R2和R3到达对方时都走以太网链路。8、R7和R8通过帧中继链路运行ISIS，DLCI号分别为708和807 。9、在R3上重新创建环回口loop back100，配置从网段地址：172.16.1.0/24，172.16.2.0/24，172.16.3.0/24，172.16.4.0/24然后汇总。10、要求在R3上将其LSP的泛洪周期从默认的15分钟&lt;900s&gt;修改为&lt;1800s&gt;，将其老化时间从默认的&lt;1200s&gt;修改为&lt;2000s&gt;；将R4的hello间隔改为30s。11、在R4上通告动态默认路由。12、在R5和R6之间建立基于链路的认证并要求采用MD5；要求在区域49.0001上采用区域认证&lt;L1认证&gt;；要求L2全部开启域认证&lt;L2&gt;。13、要求R5上实施路由泄露，将4.4.4.4/32泄露到L1，使得R6可以学习到。14、要求在整个实验机器上开启CLNS路由功能，并在接口上开启ISIS，并查看CLNS路由表。15、要求将区域49.0003和49.0004合并。 配置（可能不全，还是以实验现象为主）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291R1#show runhostname R1clns routing //clns路由功能key chain area_auth //配置区域认证 key 1 key-string huaweiinterface Loopback0 ip address 1.1.1.1 255.255.255.255 ip router isis clns router isis interface Ethernet1/0 ip address 192.168.1.1 255.255.255.0 ip router isis clns router isis router isis net 49.0001.0000.0000.0001.00 is-type level-1 authentication mode md5 level-1 authentication key-chain area_auth level-1 log-adjacency-changes //开启日志功能R2#show runhostname R2 clns routingkey chain area_auth key 1 key-string huaweikey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 2.2.2.2 255.255.255.255 ip router isis clns router isis interface Serial0/0 ip address 10.1.23.2 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis metric 20 //更改isis的metricinterface Serial0/1 ip address 10.1.24.2 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-onlyinterface Ethernet1/0 ip address 192.168.1.2 255.255.255.0 ip router isis half-duplex clns router isis isis circuit-type level-1router isis net 49.0001.0000.0000.0002.00 authentication mode md5 authentication key-chain area_auth level-1 authentication key-chain bone_auth level-2 log-adjacency-changes summary-address 172.16.0.0 255.255.252.0 //汇总R3#show runhostname R3clns routingkey chain area_auth key 1 key-string huaweikey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 3.3.3.3 255.255.255.255 ip router isis clns router isis interface Loopback100 ip address 172.16.1.1 255.255.255.0 secondary ip address 172.16.2.1 255.255.255.0 secondary ip address 172.16.3.1 255.255.255.0 secondary ip address 172.16.0.1 255.255.255.0 ip router isis interface Serial0/0 ip address 10.1.23.3 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis metric 20interface Serial0/1 ip address 10.1.37.3 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-onlyinterface Ethernet1/0 ip address 192.168.1.3 255.255.255.0 ip router isis half-duplex clns router isis isis circuit-type level-1router isis net 49.0001.0000.0000.0003.00 authentication mode md5 authentication key-chain area_auth level-1 authentication key-chain bone_auth level-2 max-lsp-lifetime 2000 //更改isis的老化时间 lsp-refresh-interval 1800 //更改isis的泛洪周期 log-adjacency-changes summary-address 172.16.0.0 255.255.252.0R4#show runhostname R4clns routingkey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 4.4.4.4 255.255.255.255 ip router isis clns router isis interface Serial0/0 ip address 10.1.24.4 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis hello-interval 30interface Serial0/1 ip address 10.1.45.4 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-only isis hello-interval 30router isis net 49.0002.0000.0000.0004.00 is-type level-2-only authentication mode md5 level-2 authentication key-chain bone_auth level-2 log-adjacency-changes default-information originate //通告动态默认路由R5#show runhostname R5clns routingkey chain link_auth key 1 key-string ciscokey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 5.5.5.5 255.255.255.255 ip router isis clns router isis interface Serial0/0 ip address 10.1.57.5 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-onlyinterface Serial0/1 ip address 10.1.45.5 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-onlyinterface Serial0/2 ip address 10.1.58.5 255.255.255.0 ip router isis serial restart-delay 0 clns router isis isis circuit-type level-2-onlyinterface Ethernet1/0 ip address 10.1.56.5 255.255.255.0 ip router isis half-duplex clns router isis isis circuit-type level-1 isis authentication mode md5 isis authentication key-chain link_authrouter isis net 49.0003.0000.0000.0005.00 authentication mode md5 authentication key-chain bone_auth level-2 log-adjacency-changes redistribute isis ip level-2 into level-1 distribute-list 100 //路由泄漏access-list 100 permit ip host 4.4.4.4 any //路由泄露R6#show runhostname R6clns routingkey chain link_auth key 1 key-string ciscointerface Loopback0 ip address 6.6.6.6 255.255.255.255 ip router isis clns router isis interface Ethernet1/0 ip address 10.1.56.6 255.255.255.0 ip router isis half-duplex clns router isis isis authentication mode md5 isis authentication key-chain link_authrouter isis net 49.0003.0000.0000.0006.00 is-type level-1 log-adjacency-changesR7#show runhostname R7clns routingkey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 7.7.7.7 255.255.255.255 ip router isis clns router isis interface Serial0/0 ip address 10.1.37.7 255.255.255.0 ip router isis serial restart-delay 0 clns router isis interface Serial0/1 ip address 10.1.78.7 255.255.255.0 ip router isis encapsulation frame-relay serial restart-delay 0 clns router isis frame-relay map ip 10.1.78.8 101 broadcast //帧中继 frame-relay map clns 101 broadcast no frame-relay inverse-arpinterface Serial0/2 ip address 10.1.57.7 255.255.255.0 ip router isis serial restart-delay 0router isis net 49.0004.0000.0000.0007.00 is-type level-2-only authentication mode md5 level-2 authentication key-chain bone_auth level-2 log-adjacency-changesR8#show runhostname R8 clns routingkey chain bone_auth key 1 key-string ciscointerface Loopback0 ip address 8.8.8.8 255.255.255.255 ip router isis clns router isis interface Serial0/0 ip address 10.1.78.8 255.255.255.0 ip router isis encapsulation frame-relay serial restart-delay 0 clns router isis frame-relay map clns 202 broadcast frame-relay map ip 10.1.78.7 202 broadcast no frame-relay inverse-arpinterface Serial0/1 ip address 10.1.58.8 255.255.255.0 ip router isis serial restart-delay 0 clns router isis router isis net 49.0004.0000.0000.0008.00 is-type level-2-only authentication mode md5 level-2 authentication key-chain bone_auth level-2 log-adjacency-changes]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（五）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-4%2F</url>
    <content type="text"><![CDATA[IS-IS区域的合并、分割、迁移 区域合并 使用多宿主功能，可以将不同区域合并到一个区域。一台L1路由器只在本区域内扩散链路状态信息，如果这台路由器连接了两个区域，那么可以实现在多个区域内扩散链路状态信息，使用这个机制，可以有效的完成区域的合并。 比如：RTA与RTB都为L1/2路由器。RTA与RTB分别属于不同的区域49.0001和49.0002，之间建立了L2邻接关系，RTA与RTB都向L2骨干区域通告链路状态信息。现在需要将这两个区域合并为一个区域。这时可以为RTA赋予两个NET地址，这两个NET地址包含不同的区域地址，分别为 49.0001和49.0002，但是SysID是相同的。由于RTA也具有了区域地址为49.0002的NET地址，与R2的区域地址相同，这时根据 IS-IS建立邻接关系的规则，RTA与RTB之间也建立了一个L1邻接关系，并且拥有一个合并的L1链路数据库。最后可以将RTA原先的49.0001的 NET地址删除，这样就完成了区域的合并。 区域分离 区域分离的操作与区域合并的正好相反。区域分离可以将原有的一个区域分离为两个不同的区域。 比如：RTA与RTB都为L1/2路由器。起初RTA和RTB属于同一个区域中，都拥有相同的区域地址49.0001，之间形成了L1和L2邻接关系，共享相同的L1和L2链路状态数据库。现在需要将这两个区域分离开。 与区域合并一样，可以先赋予RTB两个NET地址，区域地址分别为49.0001和49.0002。之后再将RTB原先区域地址为49.0001的NET地址 删除，这时由于RTA和RTB处于不同的区域，L1邻接关系将不存在，但L2邻接关系和L2链路状态数据将保留，此时便完成了区域分离。 重编址 重编址过程与区域合并、区域分离相似，重编址可能需要清除一些或者全部路由器的区域前缀，用新的区域前缀代替。 比如，现在希望将原先的49.0001 区域迁移到49.0002区域，这就需要更改路由器上的区域地址。RTA和RTB属于同一个区域49.0001中，要将RA和RB迁移到49.0002区域 中，可以为RTA和RTB都赋予两个NET地址，两个NET地址包含不同的区域地址，49.0001和49.0002，然后依次删除RTA和RTB的包含 49.0001区域地址的NET地址，这样就实现了路由器新的NSAP地址的无缝、无冲突的重新配置。 注意，IS-IS多宿主与IP中的辅助地址（secondanary IP）是不同的，辅助地址可以在同一条链路上创建多个隔离的逻辑子网。另外，辅助IP地址是在一条链路上配置多个子网。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（四）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-3%2F</url>
    <content type="text"><![CDATA[IS-IS形成邻居关系的条件 同一层次 同一区域 同一网段 相同网络类型 不同层次类型的路由器不能形成邻居关系，即Level-2路由器不能和Level-1路由器形成邻居关系，但是Level-1-2路由器既能和同一区域的Level-1路由器形成Level-1邻居关系又能和相同或者不同区域Level-2路由器形成Level-2邻居关系。 由于IS-IS是直接运行在数据链路层上的协议，并且最早设计是给CLNP使用的，IS-IS邻居关系的形成与IP地址无关，所以容易导致相互形成邻居关系的路由器间处于不同的IP网段。但在实际的实现中，由于只在IP上运行IS-IS，所以是要检查对方的IP地址的。为了解决这一问题，华为设备进行同一网段检查，保证邻居关系的正确建立。如果网络需要不检查子网掩码，在P2P网络中，可以配置接口忽略IP 地址检查；在广播网络中，需要将以太网接口模拟成P2P接口，然后才可以配置接口忽略IP地址检查。 邻居建立过程 两台运行IS-IS的路由器在交互协议报文实现路由功能之前必须首先建立邻居关系。在不同类型的网络上，IS-IS 的邻居建立方式并不相同。 在广播网络上 在广播网络上，使用LAN IIH报文来建立邻接关系。有两种类型的LAN IIH：L1 LAN IIH（组播MAC:01-80-C2-00-00-14）和L2 LAN IIH （组播MAC:01-80-C2-00-00-15） 。Level-1路由器通过交互L1 LAN IIH报文来建立邻接关系；Level-2路由器通过交互L2 LAN IIH报文来建立邻接关系；Level-1-2路由器会同时交互L1 LAN IIH报文和L2 LAN IIH报文来建立邻接关系。 以两台L2路由器在广播链路上建立邻居关系为例： 1、R1组播发送Level-2 LAN IIH（组播MAC:01-80-C2-00-00-15），此报文中无邻居标识。 2、R2收到此报文后，将自己和R1的邻居状态标识为Initial。然后，R2再组播向R1回复Level-2 LAN IIH，此报文中标识R1为R2的邻居。 3、R1收到此报文后，将自己与R2的邻居状态标识为Up。然后R1再组播向R2发送一个标识R2为R1邻居的Level-2 LAN IIH。 4、R2收到此报文后，将自己与R1的邻居状态标识为Up。这样，两个路由器成功建立了邻居关系。 5、 因为是广播网络，需要选举DIS，所以在邻居关系建立后，路由器会等待两个Hello报文间隔再进行DIS的选举。Hello报文中包含Priority 字段，Priority值最大的将被选举为该广播网的DIS。若优先级相同，接口MAC地址较大的被选举为DIS。IS-IS中DIS发送Hello时间间隔为10/3秒，而其他非DIS路由器发送Hello间隔为10秒。 在p2p链路上 在P2P链路上，邻居关系的建立不同于广播链路。分为两次握手机制和三次握手机制。 1、两次握手 只要路由器收到对端发来的Hello报文，就单方面宣布邻居为Up状态，建立邻居关系。但是容易存在单通风险。 3、三次握手 此方式通过三次发送P2P的IS-IS Hello PDU最终建立起邻居关系，类似广播邻居关系的建立。 IS-IS和OSPF关于邻接关系的区别 1、IS-IS两个邻居路由器只要相互交换HELLO数据包就认为相互形成了邻接关系；而OSPF中，两台路由器进入2-Way状态则认为形成了邻居关系，但是只有进入Full状态才被认为进入完全邻接关系。 2、IS-IS中，优先级为0的路由器亦然可以参与DIS选举；而OSPF汇总优先级为0表示不参与选举。 3、IS-IS中，DIS是基于抢占的；OSPF中DR/BDR已经选举不得抢占。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（三）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-2%2F</url>
    <content type="text"><![CDATA[IS-IS的路由选择层次 1、IS-IS area IS-IS允许将整个domain划分为多个区域，采用两级的分层结构 区域之间只能通过L2或者L1/L2路由器进行互联 一台路由器最多可以有254个area ID。当然，一般一台路由器就属于一个区域，多区域的情况，可能用于区域的合并、分割或变更 和OSPF不同的是，一台路由器必须整台属于一个区域，区域的边界不能在路由器上，也就说，不能一个接口属于某个区域，另一个接口属于其他区域 区域内的所有路由器（除边界路由器之外）只能和本区域内的路由器建立邻居关系 2、Nodes level and area level L1 router L2 router L1/L2 router Level 1area 是L1 routers和L1/L2routers的集合，相对于单个区域的概念，是由本区域中的L1 router构成，其路由信息发布到backbone中 Level2（backbone） area是L2routers和L1/L2 routers的集合，是连续的L2 router的集合（含L1/L2router）；backbone必须是连续的，它不是某个特定的什么区域 一个IS-IS路由域并不一定需要有两个层次，如果只部署一个区域的话，可能全都是L1或者L2，推荐用L2，已得到比较好的扩展性 DIS（designated IS）：类似OSPF中的DR概念 在广播多路访问的网络中，一台router会被选举为DIS，点对点网络不需要DIS level 1 有level 1的DIS，level 2有level 2的DIS，选举结果有可能不一样 与OSPF不同的是，DIS是可抢占的，并且不存在备份DIS，当一个DIS挂掉了，直接再选举 DIS发送hello数据包的时间间隔是普通router的1/3（默认是3.3s）这样可以确保DIS出现故障的时候能够被更快的被发现 选举DIS的顺序： 接口优先级（默认64），注意与OSPF不同的是，优先级为0的IS也可以参与DIS的选举，优先级数值越大越优 系统ID比大 PSN（pseudonode）：也叫伪节点 是广播多路访问网络中的一台虚拟路由器，由DIS创建 DIS在伪节点LSP中通告LAN中的所有邻居 LAN中的所有路由器在它们的LSP中通告自己与伪节点的邻接关系 PSN可以减少泛洪扩散和数据库同步的数量 如果没有PSN，任意两个之间都要建立邻接关系，并且建立LSPDB，并且数量庞大，但是引入PSN后，只需要与PSN进行连接。 LSP的泛洪扩散和同步 1、每一个LSP都拥有自己的一个4字节的序列号，在路由器启动时所发送的第一个LSP报文中的序列号为1，以后当需要生成新的LSP时，新LSP的序列号在前一个LSP序列号的基础上加1，更高的序列号意味着更新的LSP 2、每个LSP在LSDB中都有一个最大经历时间（maxage），当这个时间到达后如果没有接收到新的LSP来更新LSDB，则这个LSP会从LSDB中清除。在旧的LSP被从LSDB中清除后，他还会再保留一段时间（zeroagelifetime），当这个时间也到达时它将会被真正删除。 3、一般缺省的最大时间为1200s，zero时间为60s，当一个IS所发出的LSP的序列号达到0XFFFFFFFF时，这个路由器会将IS-IS进程暂停两个时间，以便在整个路由域内和这个路由器对应的LSP都被删除掉。随后IS-IS进程再重新启动]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（二）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis-1%2F</url>
    <content type="text"><![CDATA[IS-IS路由器的分类 level-1路由器 Level-1路由器负责区域内的路由，它只与属于同一区域的Level-1和Level-1-2路由器形成邻居关系，属于不同区域的Level-1路由器不能形成邻居关系。Level-1路由器只负责维护Level-1的链路状态数据库LSDB（Link State Database），该LSDB包含本区域的路由信息，到本区域外的报文转发给最近的Level-1-2路由器。 level-2路由器 Level-2路由器负责区域间的路由，它可以与同一或者不同区域的Level-2路由器或者其它区域的Level-1-2路由器形成邻居关系。Level-2路由器维护一个Level-2的LSDB，该LSDB包含区域间的路由信息。 所有Level-2级别（即形成Level-2邻居关系）的路由器组成路由域的骨干网，负责在不同区域间通信。路由域中Level-2级别的路由器必须是物理连续的，以保证骨干网的连续性。只有Level-2级别的路由器才能直接与区域外的路由器交换数据报文或路由信息。 level-1-2 同时属于Level-1和Level-2的路由器称为Level-1-2路由器，它可以与同一区域的Level-1和Level-1-2路由器形成Level-1邻居关系，也可以与其他区域的Level-2和Level-1-2路由器形成Level-2的邻居关系。Level-1路由器必须通过Level-1-2路由器才能连接至其他区域。 Level-1-2路由器维护两个LSDB，Level-1的LSDB用于区域内路由，Level-2的LSDB用于区域间路由。 IS-IS的编址 OSI网络层编制是通过使用两类层次化地址： 1、NSAP 每一个传输层的实体都会分配一个NSAP地址。NSAP地址是CLNS分组的网络层地址。它用来标识设备。它由初始域部分IDP和域内自定义部分DSP组成，这两部分又做了详细的设定： 其中： AFI（Authority and Format Identifier）：1字节的授权和格式标识符。AFI字段标识与NSAP相关的高层寻址域和DSP部分的语法。AFI字段的取值范围为0-99的十进制数。高层地址域提供各种各样的子域，其值由IDI字段分配。每一个高层域定义自己的IDI字段格式。 IDI（Initial Domain Identifier）：可变长的初始域标识符，标识AFI下的子域AFI+IDI 用于标识Domain。 Area：2字节的area标识符，也称为Domain内自定义部分的高位部分，（HODSP）用来把Domain细分为area， 大致类同IP中的子网 System ID：6字节的系统ID，ES或IS的标识符，类似于OSPF的router ID，每台设备都有一个系统ID，而在IP网络中每个接口都有一个IP，这是区别之一。要注意：SystemID必须在整个Area和主干（Level2）上保持唯一。 N-SEL：1字节的选择符，英文：NSAP-Selector类似TCP/IP中的端口，识别设备上的进程（或服务），在NET中为00 。 对于IP应用程序而言，在NSAP地址中，1字节定义AFI，最少2字节定义实际区域信息，6字节定义系统ID和1字节定义NSEL。因此NSAP地址最少为10字节。在CISCO IOS 中，NSAP配置为点分16进制形式。AFI值为49的只能本地使用，是RFC1618定义的预留私有地址空间。 2、NET 用NSEL值为0的NSAP地址用来标识设备，这就是这个设备的网络地址NET。因此NET由区域Id和系统ID所决定。总的来说，NSAP编址风格和IP编址风格之间的最大区别就是NSAP仅使用一个地址标识一台router，而IP则是每个端口都分配一IP地址。 关于NET的分配 一个中间系统（可以理解为路由器）至少有一个NET（最多可有254个）且系统ID必须相同。Cisco路由器默认支持最多三个NET地址，可使用如下命令修改： Router(config-router)# max-area-address xx 在一个路由选择区中的全部IS和ES必须有相同长度的系统ID 在一个区域中的所有router必须有相同的区域ID 所有的2层router必须有域内唯一的系统ID 所有的1层router必须有区域内唯一的系统ID 如果ES和IS有相同的区域ID，那么同一个区域的所有ES都会和它同在一段共享媒介质上的1层router建立毗邻关系 如果一个router上分配了多个NET, 则这些NET的系统ID必须是相同的。 举个栗子： Cisco IOS软件从右边开始解释NSAP地址（这里使用NSAP 的OSI格式）。首先从右边数起，第一个B是NSEL，往左的6B是SystemID，剩下的部分是AreaID： 关于NSAP地址到主机名的映射 本质上就是hostname到systemid的映射，有静态映射和动态映射两个方法，可通过： show isis hostname 进行查看。 1、静态主机名映射（只在本地设备有效，不会发出去） 对于Router A： 12345router isis net 49.0001.1111.2222.3333.00 exitclns host RouterA 49.0001.1111.2222.3333.00clns host RouterB 49.0001.4444.5555.6666.00 对于RouterB： 12345router isis net 49.0001.1111.2222.3333.00 exitclns host RouterA 49.0001.1111.2222.3333.00clns host RouterB 49.0001.4444.5555.6666.00 2、动态映射 IS-IS动态主机名的信息在LSP中以137号TLV的形式发布给其他IS-IS设备，在其他设备上使用IS-IS相关显示命令查看IS-IS信息时，本地设备的System ID将被设置的主机名所代替。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[isis（一）]]></title>
    <url>%2F2019%2F04%2F07%2Fisis%2F</url>
    <content type="text"><![CDATA[关于IS-IS 1、相关概念 ISIS（中间系统到中间系统）是一个链路状态的、内部网关协议。它被设计成适用在OSI无连接网络服务（CLNS）的环境中。为了提供对IP的路由支持，后面对它进行扩充和修改，使得能够同时能应用在TCP/IP和OSI环境中，称为集成化IS-IS。 管理距离为115 OSI参考模型中的网络服务规范定义了网络设备之间使用无连接通信的功能，也就是CLNS，此时，无需在发送数据之间建立端到端的路径。下图，展示的是CLNS中所包括的协议组件： 其中： CLNP：等价于TCP/IP模型中的IP协议，它提供尽力而为的传输，为ISO传输层提供服务的。 ES-IS：终端系统到中间系统的协议，类似TCP/IP中的ARP、ICMP等协议 IS-IS：中间系统到中间系统，路由选择协议。IS-IS传递的是CLNP的路由信息；在CLNS中，CLNP、IS-IS、ES-IS都是独立的网络层协议，他们都直接被封装到数据链路层的帧中进行传输；在ISO协议族可以把ES终端系统理解为主机，把IS中间系统理解为路由器，因此IS-IS的出生，其实是为CLNS服务的，往后发展成了集成性IS-IS，也就可以支持IP路由了。 2、特性 路由器与路由器之间的通信使用IIH（is-is hello）报文。该协议报文的设计主要是为了满足CLNS网络中的如下需求： 在路由域内执行路由选择功能 为网络提供最佳路由 网络出现故障后，能够快速的收敛 提供无环路的网络 提供网络的稳定性 提供网络的可扩展性 合理利用网络资源 所以被设计成一种链路状态路由协议，并且使用SPF最短路径优先算法实现快速的收敛和无环路网络 关于集成IS-IS 1、集成IS-IS使得IS-IS协议可以传播除CLNP之外的其他协议的路由信息。 2、IS-IS能在混合模式下同时路由CLNP和IP 3、IS-IS可以纯粹地用做IP路由选择，也可以纯粹地用作ISO路由选择，或同时用于两者。 4、即使只为IP提供路由选择功能，也需要CLNS地址。 相关术语 邻接关系建立 IS-IS建立邻接关系需要遵循的基本原则： 只有同一层次的相邻路由器才有可能建立邻接关系 对于level 1 router来说要求area号一致 要进行同一网段检查 隐含MTU检查 关于IS-IS报文 我这个的话对比SPF的报文（hello，DBD，LSR，LSAck，LSU）来看的，我觉得应该可以这样类比： 其中： 1、Hello PDU，包括ESH，ISH，IIH 用于建立和维护毗邻关系 ESH是ES发送到IS的 ISH是由IS发送到ES的 IIH则是IS之间传送的 2、LSP 用来发布链路状态信息，有点类似于OSPF的LSA 包含： 区域信息 邻接路由器 IP子网 度量值 认证信息 3、CSNP（完全序列号PDU） 用来发布一台router上的完整的链路状态数据库，CSNP用来告知其他router他们自己的数据库可能过时或者丢失的LSP 4、PSNP（部分序列号PDU） 用来确认和请求链路状态信息 度量值 default metric所有IS-IS路由器都支持，经常被解释成跟带宽成反比的度量方式，越小越优先，IOS无法自动分配基于带宽的链路（接口）度量，无论链路的带宽多少，所有的接口默认IS-IS度量都是10，一完整路径的最大度量值是1023 。 可通过在接口下更改度量值： 12int s0/0 isis metric 20]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>IS-IS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（七）]]></title>
    <url>%2F2019%2F04%2F04%2Fnginx-7%2F</url>
    <content type="text"><![CDATA[关于nginx的日志分析 nginx默认的日志路径是在nginx目录下的logs里（我这里是/usr/local/nginx/logs），包括访问日志access.log和错误日志error.log。 比如我们查看一下： 1cat /usr/local/nginx/logs/access.log | more 我们可以在主配置文件里面（/nginx.conf）通过log_format自定义日志的打印格式。语法： 1log_format name type 其中，name为我们为这个模板定义的名字，type为日志的类型。 关于type变量说明： $remote_addr：记录客户端的IP地址。 $server_name：虚拟主机的名称 。 $http_x_forwarded_for：HTTP请求端真实的IP。 $request：请求的URL和HTTP协议。 $remote_name：客户端的名称。 $status：记录返回HTTP请求的状态。 $upstream_status：upstream的状态。 $ssl_protocol：ssl的协议版本。 $body_bytes_sent：发送给客户端的字节数，不包括头部的大小。 $bytes_sent：发送给客户端的字节总数。 $http_referer：记录从哪个页面链接访问过来的。 $http_user_agent：记录客户端浏览器相关信息。 $request_length：请求的长度，包括请求行，请求头和请求正文。 $msec：日志写入时间。 $request_time：请求处理时间，单位为s。 $upstream_response_time：应用程序响应时间，nginx向后端服务建立连接开始到接受完数据然后关闭连接的总时间。 比如我们在/nginx.conf中配置一个main 的log_format： 123log_format main &apos;$remote_addr $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;$http_user_agent $http_x_forwarded_for $request_time $upstream_response_time $upstream_addr $upstream_status&apos;; 然后我们在需要使用的文件后面调用，比如是nginx的日志文件(access.log)或者是虚拟主机的日志文件(./vhost/*.conf)： 1access_log logs/access.log main; 然后我们就可以通过awk等工具来进行分析。 比如： 统计nginx服务器独立IP数 1awk &apos;&#123;print $1&#125;&apos; access.log |sort -r|uniq -c|wc -l]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（七）]]></title>
    <url>%2F2019%2F04%2F04%2Fnginx-6%2F</url>
    <content type="text"><![CDATA[关于nginx的rewrite规则 rewrite的主要功能是实现URL地址的重定向，比如访问nowamaic.net，自动跳转到www.nowamaic.net， 或者是反过来。它依赖于pcre组件。语法格式为： 1rewrite &lt;regex&gt; &lt;replacement&gt; [flag]; 根据regex正则表达式，会重定向到replacement，flag标识表示规则的执行属性。 1、flag标记有： last：相当于apache里面的(L)，表示完成匹配。 break：本条规则匹配完成后，终止匹配，不在匹配后面的规则。 redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址。 permanent：返回301永久重定向，浏览器地址会显示跳转后的URL地址。 其中last和break用来实现URL重写时，浏览器地址不会改变。 2、rewrite常用于匹配HTTP请求头信息，浏览器主机名、URL等，具体内容如下： HTTP headers：HTTP_USER_AGENT，HTTP_REFERER，HTTP_COOKIE，HTTP_HOST，HTTP_ACCEPT connection &amp; request：REMOTE_ADDR，QUERY_STRING。 server internals：DOCUMENT_ROOT，SERVER_PORT，SERVER_PROTOCOL。 system stuff：TIME_YEAR，TIME_MON，TIME_DAY。 解释： HTTP_USER_AGENT：用户使用的代理，例如浏览器。 HTTP_REFERER：告知服务器，从哪个页面来访问的。 HTTP_COOKIE：客户端缓存，主要用于存储用户名和密码等信息。 HTTP_HOST：匹配服务器servername的域名。 HTTP_ACCEPT：客户端的浏览器支持的MIME类型。 REMOTE_ADDR：客户端的IP地址。 QUERY_STRING：URL访问的字符串。 DOCUMENT_ROOT：服务器的发布目录。 SERVER_PORT：服务器端口。 SERVER_PROTOCOL：服务器的协议。 TIME_YEAR：年。 TIME_MON：月。 TIME_DAY：日。 3、regex的相关说明 \：转义符标志。 ^：匹配输入字符串的起始位置。 $：匹配输入字符串的结束位置。 *：匹配前面的字符零次或者多次。 +：匹配前面的字符1次到多次。 ？：匹配前面的字符零次或一次。 （pattern）：匹配括号内并可以在后面获取对应的匹配，常用$0…$9属性获取小括号中的匹配内容。 例子： 将dyh.com跳转到www.dyh.com 123if( $host != &apos;www.dyh.com&apos; )&#123; rewrite ^/(.*)$ http://www.dyh.com/$1 permanent; &#125; 访问/dyh/test1/ 跳转到/newindex.html，浏览器的位置不变 1rewrite ^/dyh/test1/$ /newindex.html last; 访问文件和目录不存在时跳转到index.html 123if ( ! -e $request_name )&#123; rewrite ^/(.*)$ /index.html last;&#125; 目录对换，如/xxxx/1234变为/xxxx?id=1234 1rewrite ^/(.+)/(\d+) /$1?id=$2 判断浏览的使用的代理 123if ( $http_user_agent ~ MSIE )&#123; rewrite ^(.*)$ /it/$1 break;&#125;]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（六）]]></title>
    <url>%2F2019%2F04%2F03%2Fnginx-5%2F</url>
    <content type="text"><![CDATA[日志分割 服务器每天都会产生大量的访问日志，而且不会自动的进行切割，如果持续数天访问，将会导致access.log日志文件很大，不便查看分析。所以可以用shell脚本结合crontab对该日志进行自动快速的切割。 比如： 1234567891011121314#!/bin/bash#日志切割S_LOG=/usr/local/nginx/logs/access.logD_LOG=/data/backup/`date +%Y%m%d`echo -e &quot;\033[32mplease wait shell script!\033[1m&quot;sleep 2if [ ! -d $D_LOG ];then mkdir -p $D_LOGfimv $S_LOG $D_LOGkill -USR1 &apos;cat /usr/local/nginx/logs/nginx.pid&apos;echo &quot;-------------------------------------------&quot;echo &quot;Successful!&quot;echo &quot;You can access backup nginx log $D_LOG/access.log&quot; 然后加入到crontab计划任务中。 防盗链 盗链的含义是网站内容本身不在自己公司的服务器上，而通过技术手段，直接调用其他公司的服务器网站数据，而向最终用户提供此内容。这样是极其消耗被盗链的服务器的资源。所以我们可以配置防盗链技术来防止。 referer是http请求的header的一部分，当浏览器或者模拟浏览器行为向web发送请求的时候，头信息里包含它。比如在 www.dyh.com 里面有一个 www.baidu.com 的连接，那么点击www.baidu.com ，它的header信息里面就有，referer=http://www.dyh.com 。所以我们需要对不信任的网站进行防御。 12345678910111213141516171819server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; #防盗链 location ~* \.(gif|jpg|png|swf|flv)$ &#123; #valid_referers none|blocked|server_names|string ...; valid_referers localhost; if ( $invalid_referers )&#123; #rewrite ^/localhost/403.html; return 403; &#125; &#125;&#125; 或者： 1234if ( $host != &apos;server_name[自己的网站]&apos; )&#123; #rewrite ^/localhost/403.html; return 403;&#125; gif|jpg|png|swf|flv ：设置防盗链的文件类型。 valid_referers表示可用的referers设置，也就是白名单，允许文件链出的域名白名单。none表示没有referers，直接通过浏览器或者其他工具访问；blocked表示有referers，但是被代理服务器或者是防火墙隐藏；server_names设置一个或多个URL,检测Referer头域的值是否是这些URL中的某个。 比如： 1234567location ~* \.(gif|jpg|png|bmp)$ &#123; valid_referers none blocked *.dyh.com *.baidu.com *.google.com; if ($invalid_referer) &#123; return 403; #rewrite ^/ http://www.wangshibo.com/403.jpg; &#125;&#125;]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（五）]]></title>
    <url>%2F2019%2F04%2F03%2Fnginx-4%2F</url>
    <content type="text"><![CDATA[配置nginx的负载均衡 nginx负载均衡是通过upstream模块来实现的，内置实现了三种负载策略。 1、轮询（默认） nginx根据请求次数，将每个请求均匀分配到每台服务器。 2、最少连接 将请求分配给连接数最少的服务器。nginx会统计哪些服务器的连接数最少。 3、IP hash 绑定处理请求的服务器。第一次请求时，根据该客户端的IP算出一个hash值，将请求分配到集群中的某一台服务器上。后面该客户端的所有请求，都将通过这台服务器处理。 配置轮询 思路：通过upstream定义一组主机，然后通过proxy_pass转发给这组主机。 1234567891011121314http &#123; upstream tomcats&#123; server 192.168.1.1:8080; server 192.168.1.2:8080; server 192.168.1.3:8080; &#125; server &#123; listen 80; location / &#123; proxy_pass http://tomcats ; &#125; &#125;&#125; 我们可以通过修改权重weight，实现非等价的负载均衡。 12345upstream tomcats&#123; server 192.168.1.1:8080 weight=2; #2/6次 server 192.168.1.2:8080 weight=1; #1/6次 server 192.168.1.3:8080 weight=3; #3/6次&#125; max_fails 默认为1，某台server允许请求失败的次数，超过这个次数，在fail_timeout时间内，新的请求不会发送给它，表示无法到达；如果为0，则会标记它会永久无效的状态。 fail_timeout 默认为10秒，某台服务器达到max_fail次失败后，在这个时间内，不会将请求分配给它。 12345upstream tomcats&#123; server 192.168.1.1:8080 weight=2 max_fails=3 fail_timeout=15; server 192.168.1.2:8080; server 192.168.1.3:8080; &#125; max_conns 限制分配给服务器的最大连接数量（1.5.9以后才有），为0表示不限制。 123upstream tomcats&#123; server 192.168.1.1:8080 max_conns=1000;&#125; resolve 指定DNS服务器，需要在HTTP模块下配置resolver指令。 123456789http &#123; resolver 10.0.0.1; upstream tomcats &#123; zone ....; server example.com resolve; &#125;&#125; 表示example.com域名，由10.0.0.1服务器负责解析。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（四）]]></title>
    <url>%2F2019%2F04%2F03%2Fnginx-3%2F</url>
    <content type="text"><![CDATA[proxy模块和upstream模块 proxy模块 1、proxy_pass URL： 设置代理服务器所使用的协议、上游服务器地址和一个可选的URL被映射到一个位置。这个指令一般用于location中。如： 123location /name/ &#123; proxy_pass http://192.168.1.1/remote/;&#125; 这里的proxy_pass指定的是一个URL（注意：remote后有一个/），当用户请求中匹配到“/name/&quot;，被代理到后端后路径被更改成上游服务器的”/remote/“。又如： 123location /name/ &#123; proxy_pass http://192.168.1.1/remote;&#125; 此时proxy_pass指定的不是一个URL（/remote后面没有/），当用户的请求匹配到”/name/“时，被代理到后端的时候，会把”/name&quot;添加到proxy_pass路径后面，此时就会被转发到 http: //192.168.1.1/name/ 。 如果location中采用了正则表达式，那proxy_pass一定不能指定成一个URL，而llocation中匹配到的URL将被直接传递给上游服务器。比如： 123location ~ ^/name/&#123; proxy_pass http://192.168.1.1;&#125; /name/将被代理为http: //192.168.1.1/name/ 。 2、proxy_connect——timeout TIME； 定义nginx作为代理接收到客户端的请求后，需要把这个请求转发到上游服务器的最大等待时长，默认是60秒，建议不要超过75秒。 3、proxy_cookie_domain off; proxy_cookie_domain domain replacement; 将upstream server通过Set-Cookie首部设定的domain属性修改为指定的值，其值可以为一个字符串、正则表达式的模式或一个引用的变量。 4、proxy_cookie_path off; proxy_cookie_path path replacement; 将upstream server通过Set-Cookie首部设定的path属性修改为指定的值，其值可以为一个字符串、正则表达式的模式或一个引用的变量。 5、proxy_hide_header FIELD; 默认情况下nginx不会将上游服务器的“Date”, “Server”, “X-Pad”, and “X-Accel-…”这些头部字段转发给客户端，使用proxy_hide_header后可以自定义哪些头部字段不被转发。 6、proxy_pass_header FIELD; 与proxy_hide_header相反，定义显式的指定上游服务器的哪些头部字段会转发给客户端。 7、proxy_set_header FILED VALUE; 把客户端发送给nginx代理的首部进行重新定义或附加一个首部，然后传递给上游服务器，VALUE可以是文本、变量或是文本和变量的组合。例如： 123456789101112server&#123; listen 80; server_name www.dyh.com; location / &#123; root /web/dyh.com; index index.html index.htm; &#125; location /dyh/ &#123; proxy_pass http://192.168.1.1/test/; proxy_set_header X-Real-IP $remote_addr; &#125;&#125; 这里定义了一个“X-Real-IP”的首部，这个首部的值是“$remote_addr”，即是客户端的IP地址，这样上游服务器就可以收到这个自定义的首部，可以利用此手段让上游服务器在访问日志中能够记录真实请求服务的IP地址，而不是记录的全是来自nginx代理的IP地址，有利于做日志分析。 8、proxy_redirect [ default|off|redirect replacement ]; 重写location并刷新从upstream server收到的报文的首部，然后才发送给客户端。 9、proxy_send_timeout TIME; 在连接断开之前两次连续发送至upstream server的写操作的最大间隔时长，默认是60秒。 10、proxy_read_timeout TIME; 在连接断开之前两次从接收upstream server接收读操作的最大间隔时长，默认是60秒。 11、proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 | http_403 | http_404 | off …; 设置当上游服务器出现哪此错误时，将下一个请求转发到下一个上游服务器。 upstream模块 upstream server {…} 用于设置一个服务器组，组内的服务器可以是主机名，也可以是IP地址，服务器支持端口映射功能。只能用户http上下文。 里面参数： 1、weight=number； 设置服务器的权重，默认为1。 2、max_fails=number 默认为1，与参数fail_timeout配合使用，表示在fail_timeout（默认为10s）指定的时间内，向上游服务器转发失败的最大次数，超过则认为上游服务器不可用。 3、backup 定义一个备用服务器，只有当其他所有的服务器都不可用时，这个备用的服务器才启用。 4、down 标记此服务器永久不可用，可作停机维护使用。 5、max_conns=NUMBER 在1.5.9以后，此参数可设置上游服务器的最大并发数，默认为0，表示不作限制。 更多的参数说明可查看这里。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（三）]]></title>
    <url>%2F2019%2F04%2F02%2Fnginx-2%2F</url>
    <content type="text"><![CDATA[关于nginx反向代理 Nginx反向代理服务器是指用代理服务器来接受Internet上的连接要求，然后将请求转发给内部上的服务器；并将从服务器上得到的结果反馈给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 通常的代理服务器，只用于代理内部网络对Internet的连接请求，客户机必须指定代理服务器，并将本来要直接发送到web服务器上的请求发送到代理服务器中。但是，当一个代理服务器能够代理外部网络上的主机，访问内部网络时，这种代理的方式称为反向代理服务器。 适用范围 作为内容服务器的替身 如果你的内容服务器具有必须保持安全的敏感信息，可在防火墙外部设置一个代理服务器作为内容服务器的替身。当外部客户机尝试访问内容服务器时，会将其送到代理服务器。实际内容位于内容服务器上，在防火墙内部受到安全保护。代理服务器位于防火墙外部，在客户机看来就是像是内容服务器。 当客户机向站点发出请求的时候，请求将转发到代理服务器。然后，代理服务器通过防火墙中的特定通路，将客户机的请求转发到内容服务器。内容服务器再通过该通道将结果回传给代理服务器，代理服务器将检索到的信息发给客户机。 如果内容服务器返回错误信息，代理服务器会先行截取该信息并更改标头中列出的任何URL，然后再将消息发送给客户机。如此可防止外部客户机获取内部服务器的重定向URL。 作为内容服务器的负载均衡器 可以在一个组织内部使用多个代理服务器来平衡各web服务器间的网络负载。在此模型中，可以利用代理服务器的高速缓存特性，创建一个用于负载平衡的服务器池。此时，代理服务器可以位于防火墙的任意一侧。如果web服务器每天都会接收大量的请求，则可以使用代理服务器分担web服务器的负载并提高网络访问效率。 对于客户机发往真正服务器的请求，代理服务器起着中间调停的作用。代理服务器会将所请求的文档存入高速缓存。如果有不止一个代理服务器，DNS可以采用“循环复用法”选择其IP地址，随机为请求选择路由。客户机每次都使用同一个URL，但请求所采取的路由每次都可能经过不同的代理服务器。 可以使用多个代理服务器来处理对一个高用量内容服务器的请求，这样做的好处是内容服务器可以处理更高的负载，并且比其独自工作时更有效率。 配置： 不需要增加额外的模块，用自己的proxy_pass转发就行 1234567891011121314upstream my_server &#123; server 10.0.0.2:8080; keepalive 2000;&#125;server &#123; listen 80; server_name 10.0.0.1; client_max_body_size 1024M; location / &#123; proxy_pass http://my_server/; proxy_set_header Host $host:$server_port; &#125;&#125; 通过使用upstream配置服务地址，使用server的location配置代理映射。结果就是，访问nginx地址10.0.0.1:80的请求会被转发到my_server的服务器地址10.0.0.2:8080 重定向报文代理 上面的配置只是简单的配置，当服务返回重定向报文的时候（301或者302），此时会将重定向的目标URL放进http 里面的response报文的header的location字段内。用户收到这个重定向时，会解析出该字段进行跳转，此时新的请求报文会直接发给内容服务器，而不是nginx代理服务器。为了避免这种情况，我们必须修改重定向报文的location信息。 123456location /my/ &#123; proxy_pass http://my_server; proxy_set_header Host $host:$server_port; proxy_redirect / /my/;&#125; 题外话！！！ （注：当proxy_pass 后面是 http://my_server 的时候，此时是转发到host:port/my，如果是 http://my_server/ 时，这时是转发到 host:port ，这是因为proxy_pass参数中如果不包含url的路径，则会将location的pattern识别的路径作为绝对路径） 正事！！！ 使用proxy_redirect可以修改重定向报文的location字段，比如上面的例子，它会将所有根路径下的url代理到nginx的/my/路径下返回给用户。就是说，如果服务器返回的重定向报文的location原始值是/login，那么经过nginx代理后，用户收到的报文的location字段是/my/login。这时，浏览器将会跳转到nginx的/my/login地址进行访问。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（二）]]></title>
    <url>%2F2019%2F04%2F01%2Fnginx-1%2F</url>
    <content type="text"><![CDATA[nginx的安装 yum源安装（可根据官方的提示来做：官方教程） 1234567891011121314151617181920212223242526yum install -y yum-utils创建/etc/yum.repos.d/nginx.repo，内容为：[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.key[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.key然后：yum makecache fastyum install -y nginx启动测试：systemctl start nginxsystemctl enable nginxfirewall-cmd --add-port=80/tcp --permanent firewall-cmd --reload 源码安装 安装pcre依赖环境 12345yum install -y gcc gcc-c++ zlib-develwget https://ftp.pcre.org/pub/pcre/pcre-8.43.tar.gztar -zxvf pcre-8.43.tar.gz ./configure --prefix=/usr/local/pcre //进入到pcre的解压缩文件夹下make &amp;&amp; make install 下载源码包并解压： 12wget http://mirrors.sohu.com/nginx/nginx-1.9.8.tar.gztar -zxvf nginx-1.9.8.tar.gz 创建一个nginx用户： 1useradd -s /sbin/nologin nginx 编译： 12./configure --prefix=/usr/local/nginx --with-pcre=/usr/local/src/pcre-8.43 --user=nginxmake &amp;&amp; make install 启动并放行防火墙： 123/usr/local/nginx/sbin/nginxfirewall-cmd --add-port=80/tcp --permanent firewall-cmd --reload 给nginx配置https 我们此时需要一个证书,可以去申请，这里的话我用openssl自己签发一个。 1、安装openssl 1yum install -y openssl openssl-devel 2、创建服务器私钥，默认会让输入一个口令，以后重启nginx都需要，但是可以等创建好后移除 1openssl genrsa -des3 -out server.key 1024 3、创建签名请求的证书（csr），颁发证书 1234567891011openssl req -new -key server.key -out server.csr 会出现：Country Name (2 letter code) [XX]:CN //您所在国家的ISO标准代号，中国为CNState or Province Name (full name) []:Jiangsu //您单位所在地省/自治区/直辖市Locality Name (eg, city) [Default City]:Zhenjiang //您单位所在地的市/县/区Organization Name (eg, company) [Default Company Ltd]:ujs // //您单位/机构/企业合法的名称 Organizational Unit Name (eg, section) []:ujs //部门名称 Common Name (eg, your name or your server&apos;s hostname) []:www.dyh.com 通用名，例如：www.itrus.com.cn。此项必须与您访问提供SSL服务的服务器时所应用的域名完全匹配Email Address []: //邮箱名后面的都可以直接回车 4、去除生成私钥时的所需要的密码 12cp server.key server.key.org openssl rsa -in server.key.org -out server.key 5、自签发根证书 1openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt 6、配置nginx配置文件，让其包含证书和私钥（针对yum安装的，源码安装的下面会说） 1234567891011121314151617#vim /etc/nginx/nginx.conf(在http里面添加)server &#123; listen 443; //监听端口为443 server_name www.dyh.com; ssl on; //开启ssl ssl_certificate /etc/nginx/key/server.crt; //证书位置 ssl_certificate_key /etc/nginx/key/server.key; //私钥位置 ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; //指定密码为openssl支持的格式 ssl_ciphers HIGH:!aNULL:!MD5; //密码加密方式 ssl_prefer_server_ciphers on; //依赖SSLv3和TLSv1协议的服务器密码将优先于客户端密码 location / &#123; root html; //根目录的相对位置 index index.html index.htm; &#125; &#125; 然后重启nginx服务。 这个时候虽然是https，但是由于我们没有购买证书，所以对于浏览器还是不信任的。我们可以自己弄一个CA证书，然后安装在本地，使浏览器对该网站信任，但是对于别人而言还是不信任的。所以我们可以找一个ssl证书的颁发机构，去弄一个。 7、关于源码安装的nginx的配置 我们在生成私钥以及公钥后，对于源码安装的服务器而言，需要重新编译一遍。 我们先看以前编译的选项： 1/usr/local/nginx/sbin/nginx -V 然后进入我们解压的nginx源码包 123./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-file-aio --with-http_realip_modulemake //这里不需要make install否则会覆盖 这个时候，本路径下的objs文件夹下将出现新的nginx启动程序，我们先备份以前的，然后使用新的启动程序。（先停止nginx服务） 123456/usr/local/nginx/sbin/nginx -s stopcp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bakcp objs/nginx /usr/local/nginx/sbin/nginx/usr/local/nginx/sbin/nginx -t/usr/local/nginx/sbin/nginx -V/usr/local/nginx/sbin/nginx 修改配置文件如上面就行。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx（一）]]></title>
    <url>%2F2019%2F04%2F01%2Fnginx%2F</url>
    <content type="text"><![CDATA[关于nginx nginx是一个高性能、轻量级的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP代理服务器。 相对于apache所具有的优点 高并发性能好 负载均衡以及反向代理能力好 系统内存和CPU的占用率低 可对后端服务进行健康检查 支持PHP fcgi和fast fcgi 可以作为缓存服务器，邮件代理服务器 配置简单 关于nginx的模块（按照功能来分） 基础模块，看这官方文档。 handles（处理器模块） 此类模块直接处理请求，并进行输出内容和修改handers信息等操作，此类模板一般只能有一个。 filters（过滤器模块） 此类模块主要对其他处理器模块输出的内容进行修改操作。 proxies（代理器模块） 此类模块是upstream之类的模块，这类模块主要是与后端一些服务进行交互，实现服务代理和负载均衡。 关于epoll模型（默认在linux内核版本2.6以上支持） nginx的高并发得益于它的epoll模型，与传统的服务器架构不同的是，epoll是Linux后面才有的异步非阻塞，而apache是select模型（传统的I/O多路复用）。 关于两者的区别：apache的select特点是在选择句柄的时候需要遍历所有的句柄，效率很低；而nginx的epoll则是当句柄上有事件响应的时候就把它立即选出来，不需要遍历全部。 关于php-fpm 先说说cgi、fast-cgi cgi协议是为了解决不同的语言解释器，像：php，python与webserver的通信；而fast-cgi则是cgi的改良版本，为了解决：解释器与webserver通信的时候，当webserver每收到一个请求就fork一个cgi进程，请求结束后在kill进程，当请求增多的时候，这样的事也就增多从而带来的资源浪费的问题。fast-cgi不会杀死这个进程，一直保留，使它可以处理多个请求。 php-fpm 也就是php-fastcgi process manager。它提供了进程管理的功能。它包含两个进程，一个是master负责监听端口，接收来自web server的请求，另一个是worker（可以有多个），每个进程内部都嵌入了一个PHP解释器，是PHP代码真正的执行器。 对于apache而言，php是作为它的一个模块进行工作、处理；但是对于nginx，php-fpm与nginx则是相互独立的。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS]]></title>
    <url>%2F2019%2F04%2F01%2Fhttp%2F</url>
    <content type="text"><![CDATA[关于http和https 1、简介 HTTP HTTP（超文本传输协议），用于从万维网服务器传输超文本到本地浏览器的传送协议。 HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件,图片文件, 查询结果等）的应用层协议。 HTTP协议工作于C/S或C/B架构。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。 HTTPS HTTPS（超文本安全传输协议），是一种透过计算机网络进行安全通信的传输协议。 HTTPS经由HTTP进行通信，但是利用了SSL/TLS来加密数据包，也就是在HTTP上加了一层S加密模块。 作用是为了对网站服务器进行身份认证，保护数据的安全和完整。 工作流程 客户端发起HTTPS请求 服务端传送数字证书(也就是自己的公钥) 客户端解析证书，并判断是否有效 客户端用服务端证书对自己产生的一个随机值（私钥）进行加密并传输过去 服务端用自己的私钥进行解密，得到客户端的随机值（私钥），以后通过这个私钥进行通信 服务端利用客户端的私钥进行对称加密并传输数据 客户端进行解密并获取信息 关于网页的状态码 1XX：指示信息，表示请求已经接受，继续处理 2XX：成功，表示请求已经被成功接收 3XX：重定向，要完成请求必须进行更进一步的操作 4XX：客户端错误，请求的语法有错误或者请求无法实现 5XX：服务端错误，服务器未能实现合法的请求！ 1234567891. 200：请求已经正常处理完毕2. 301：URL永久重定向3. 302：URL临时重定向4. 400：客户端请求存在语法错误5. 401：客户端请求没有经过授权6. 403：客户端的请求被服务器拒绝7. 404：客户端请求的URL不存在8. 500：服务器发生永久错误9. 503：服务器发生临时错误]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postfix]]></title>
    <url>%2F2019%2F03%2F30%2Fpostfix%2F</url>
    <content type="text"><![CDATA[搭建postfix邮件服务器 一、配置DNS服务（假如我的域名为：mail.dyh.com，服务器地址为：192.168.248.128） 1、安装 1yum install -y bind bind-utils.x86_64 2、编辑配置文件（/etc/named.conf） 通过named-checkconf检查文件语法 3、编辑区域配置文件（/etc/named.rfc1912.zones） 在后面加入： 通过named-checkconf检查文件语法 4、编辑区域数据配置文件 先创建正向区域数据配置文件 12cd /var/namedcp -p named.localhost dyh.com.zone 参照上面创建反向区域数据配置文件 5、启动服务以及放行防火墙 6、配置/etc/resolv.conf文件，在首行加入（只要保证自己所配地址为第一DNS） 1nameserver [自己的地址] 7、测试 二、配置postfix 1、安装 默认安装，如果没有的话自行安装吧。 2、修改主配置文件（/etc/postfix/main.cf） 123456mydomain = dyh.com //邮件域myhostname = mail.dyh.com //服务器的主机名myorigin= $mydomain //发送的邮件域inet_interfaces = 192.168.248.128 127.0.0.1 //监听的网卡mydestionation = $myhostname, localhost.$mydomain, localhost //服务对象home_mailbox = Maildir/ //邮件目录 通过postfix进行检查，然后重启服务。 3、配置mailx发送邮件（借助外部的邮箱进行发送） 安装 1yum install -y mailx 修改主配置文件（/etc/mail.rc） 其中： set from：是外部能够发送邮件的并开启smtp服务的邮箱，我这里是选择qq邮箱（默认没有开启smtp，需要自己设置，并获取smtp授权码） smtp-auth-user：smtp邮箱账号，和上面那个是一样的 smtp-auth-password：这是是smtp的授权码，和邮箱的登录密码在qq上并不一样 4、测试 1echo &quot;zabbix test mail&quot;|mail -s &quot;zabbix&quot; [要发送的邮箱地址] 上面的意思是： 标题：zabbix 邮件内容：zabbix test mail]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>postfix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（八）]]></title>
    <url>%2F2019%2F03%2F29%2Fzabbix-7%2F</url>
    <content type="text"><![CDATA[关于zabbix的邮件报警 Zabbix服务端设置邮件报警，当被监控主机宕机或者达到触发器预设值时，会自动发送报警邮件到指定邮箱。 在这里配置的思路是这样的：使用一个可以收发的邮件的邮箱，通过在zabbix服务端中设置，使其能够发送报警邮件到指定邮箱。（也就是从服务器指定发送邮箱——&gt;管理员邮箱） 1、安装postfix环境 看这！！！ 2、配置zabbix 关于这里，我们可以通过type选择媒介类型，可以是email（上图），此时需要配置SMTP服务器的相关信息，包括： Smtp server：设置smtp服务器来处理传出的消息 SMTP server port：SMTP服务器的端口 SMTP helo：通常是域名 SMTP Email：此处输入的地址将被用作是发送消息的from地址。 Connection security：选择连接的安全级别，如果使用ssl/tls，需要设置smtp服务器的ssl证书 Authentication：选择认证级别 当然也可以是脚本： 我们在zabbix配置文件中可以看见，zabbix的相关脚本信息都放在下面这个路径 1AlertScriptsPath=/usr/lib/zabbix/alertscripts 所以我们在这个路径下创建一个mail.sh的脚本，这个脚本里的内容为： 其中包含几个变量，message（邮件信息），subject（邮件主题）这两个可以从zabbix的宏所传递的，我们在选择脚本作为媒介的时候，设置了三个宏： {ALERT.SENDTO}：发给谁 {ALERT.SUBJECT}：主题 {ALERT.SUBJECT}：内容 然后分别对应我们的参数$ 1，$ 2，$ 3 。 此外将错误输出到/tmp/mailx.log文件中。 3、设置触发报警的动作，也就是触发器 比如说，我们创建一个当发现agent工作不正常的时候触发报警的trigger。 关于condition，触发条件的话，可以查看官方文档 其中： Default subject（邮件主题，也可以用默认的） 故障{TRIGGER.STATUS},服务器{HOSTNAME1}发生:{TRIGGER.NAME}故障! Default message（邮件内容，可以使用默认的） 告警主机:{HOSTNAME} 告警事件:{EVENT.DATE} {EVENT.TIME} 告警等级:{TRIGGER.SEVERITY} 告警信息:{TRIGGER.NAME} 告警项目:{TRIGGER.KEY1} 问题详情:{ITEM.NAME}:{ITEM.VALUE} 当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1} 事件ID:{EVENT.ID} Operations 我们在这里设置邮件所发送的对象。 这里是problem恢复后的邮件信息，可以参考上面。 这时我们弄好了，可以看见我们所创建的东东。这个时候应该显示为disable我们点击一下让它变为enable开启状态（其中第二个为默认存在的）。 4、配置接收者信息 send to：为所要发送的目的邮箱。 5、测试 我们将其中一台agent关闭。 然后查看出现的problem。 点击时间，我们可以查看详细的信息，包括邮件的发送情况 这个时候，我们可以查看邮箱，看邮件信息。 如果问题解决后，此时会发送recovery operation的内容，并且将状态从PROBLEM置为RESOLVED。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ospf（四）]]></title>
    <url>%2F2019%2F03%2F28%2Fospf-3%2F</url>
    <content type="text"><![CDATA[关于OSPF的综合实验 这个的话在IOU上做的，配置导出可能有问题，一切以最终的实现结果为准吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147R1：interface Loopback0 ip address 1.1.1.1 255.255.255.255 ip ospf authentication-key cisco ip ospf 1 area 0interface Ethernet0/0 ip address 192.168.18.1 255.255.255.0 ip ospf priority 5 ip ospf 1 area 3interface Ethernet0/1 ip address 192.168.12.1 255.255.255.0 ip ospf authentication-key cisco ip ospf priority 5 ip ospf 1 area 0interface Ethernet0/2 ip address 192.168.13.1 255.255.255.0 ip ospf authentication-key cisco ip ospf priority 5 ip ospf 1 area 0router ospf 1 area 0 authentication area 3 stub no-summary area 3 range 172.16.0.0 255.255.252.0R2：interface Loopback0 ip address 2.2.2.2 255.255.255.255 ip ospf authentication-key cisco ip ospf 1 area 0interface Ethernet0/0 ip address 192.168.12.2 255.255.255.0 ip ospf authentication-key cisco ip ospf 1 area 0router ospf 1 area 0 authenticationR3:interface Loopback0 ip address 3.3.3.3 255.255.255.255 ip ospf authentication-key cisco ip ospf 1 area 0interface Ethernet0/0 ip address 192.168.13.3 255.255.255.0 ip ospf authentication-key cisco ip ospf 1 area 0interface Ethernet0/1 ip address 192.168.34.3 255.255.255.0 ip ospf message-digest-key 1 md5 ospf ip ospf network point-to-point ip ospf 1 area 1router ospf 1 area 0 authentication area 1 authentication message-digest area 1 virtual-link 4.4.4.4 authentication authentication-key huaweiR4:interface Loopback0 ip address 4.4.4.4 255.255.255.255 ip ospf 1 area 1interface Ethernet0/0 ip address 192.168.34.4 255.255.255.0 ip ospf message-digest-key 1 md5 ospf ip ospf network point-to-point ip ospf 1 area 1interface Ethernet0/1 ip address 192.168.45.4 255.255.255.0 ip ospf network point-to-point ip ospf 1 area 2router ospf 1 area 1 authentication message-digest area 1 virtual-link 3.3.3.3 authentication authentication-key huawei area 2 nssa no-summaryR5:interface Loopback0 ip address 5.5.5.5 255.255.255.255 ip ospf 1 area 2 interface Ethernet0/0 ip address 192.168.45.5 255.255.255.0 ip ospf network point-to-point ip ospf 1 area 2interface Ethernet0/1 ip address 192.168.56.5 255.255.255.0router ospf 1 area 2 nssa summary-address 6.6.0.0 255.255.0.0 redistribute rip metric 5 subnetsrouter rip version 2 redistribute ospf 2 metric 5 network 192.168.56.0 no auto-summaryR6:interface Loopback0 ip address 6.6.6.6 255.255.255.255interface Loopback1 ip address 6.6.1.1 255.255.255.0interface Loopback2 ip address 6.6.2.1 255.255.255.0interface Loopback3 ip address 6.6.3.1 255.255.255.0interface Ethernet0/0 ip address 192.168.56.6 255.255.255.0interface Ethernet0/3 no ip address shutdown router rip version 2 network 6.0.0.0 network 192.168.56.0 no auto-summaryip route 0.0.0.0 0.0.0.0 192.168.56.5R7:interface Loopback0 ip address 7.7.7.7 255.255.255.255 ip ospf 1 area 3 interface Loopback1 ip address 172.16.1.1 255.255.255.0 ip ospf 1 area 3interface Loopback2 ip address 172.16.2.1 255.255.255.0 ip ospf 1 area 3interface Loopback3 ip address 172.16.3.1 255.255.255.0 ip ospf 1 area 3interface Ethernet0/0 ip address 192.168.78.7 255.255.255.0 ip ospf network point-to-point ip ospf 1 area 3router ospf 1 area 3 stub no-summaryR8:interface Loopback0 ip address 8.8.8.8 255.255.255.255 ip ospf 1 area 3interface Ethernet0/0 ip address 192.168.78.8 255.255.255.0 ip ospf network point-to-point ip ospf 1 area 3interface Ethernet0/1 ip address 192.168.18.8 255.255.255.0 ip ospf 1 area 3router ospf 1 area 3 stub]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>ospf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ospf（三）]]></title>
    <url>%2F2019%2F03%2F28%2Fospf-2%2F</url>
    <content type="text"><![CDATA[关于ospf的LSA 路由器LSA(Router LSA Type-1) 区域内router产生：描述了路由器所有接口，链路和cost值，只能在本区域内泛洪。 网络LSA(Network LSA Type-2) 由DR产生，报文包括了其连接的所有router的router-ID，其中包含自己的router-id，只在本区域泛洪。通过LSA1、LSA2在区域内泛洪，使区域内每个路由器的LSDB达到同步，计算生成标识为&quot;O&quot;的路由，解决区域内部的通信问题。 网络汇总LSA(Network summary LSA Type-3) 可以通知本区域内的路由器外的路由信息，默认路由也被通告。link-id为目标网段的id。如果一台ABR路由器在与他本身相连的区域内有多条路由可以到达目的地，那么他将只会事发单一的一条网络汇总LSA到骨干区域，而且这条网络汇总LSA是上述多条路由器中代价最低的。当ABR收到来自其他区域的LSA3，然后会重新生成新的属于自己的LSA3，然后在整个OSPF系统内扩散。 ASBR汇总LSA（ASBR summary Type-4） 也是有ABR产生，但是它是一条主机LSA，指向ASBR路由器。 自治系统外部汇总LSA（AS external Type-5） 由ASBR产生，告诉本自治区的路由器通往外部自治区的路由器。外部路由以重分发的方法引入 OSPF路由域，相应信息由ASBR以LSA5的形式生成然后进入。缺省情况下，LSA5生成路由用OE2表示，可强制指定为OE1。 注： OE2开销=外部开销； OE1开销=外部开销+内部开销 LSA5不允许进入特殊区域——stub存根区和NSSA（非完全存根区）区域。 NSSA外部LSA(Type-7) 由ASBR产生，几乎和LSA5通告是相同的，但NSSA外部LSA通告仅仅在始发这个NSSA外部LSA通告的非纯末梢区域内部进行泛洪。 LSA7只能在NSSA区域内泛洪，到达NSSA区域ABR后，ABR将其转换成LSA类型5外部路由，传播到area0，从而传播到整个OSPF路由域，生成路由缺省用ON2表示，也可指定为ON1。 认证 支持明文认证和MD5认证 1、明文认证 在接口下配置认证密码： 在进程中开启认证： 2、MD5认证 在接口下配置密钥以及认证类型： 在进程中开启： 关于ospf的区域过滤 1、可以用acl+route-map进行 2、也可以使用area PID filter-list prefix +name in|out 比如： 1234router ospf 1 area 25 filter-list prefix area25outbount out ip prefix-list area25outbount seq 10 deny 192.168.1.0/24 //拒绝该地址由类型3的LSA通告出去。 ip prefix-list area25outbount seq 20 permit 0.0.0.0/0 le 32 //允许所有其他地址通过，从掩码长度0位到32位地址。 该列表过滤了由区域25发起的，并发送到非区域25的区域的地址的LSA。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>ospf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ospf（二）]]></title>
    <url>%2F2019%2F03%2F28%2Fospf-1%2F</url>
    <content type="text"><![CDATA[OSPF的五种包 hello包：用于路由器之间发现和维护邻居关系//协商邻接关系等 DBD包：用于向邻居表述自己已经知道的LSA，以建立LSDB LSR：用于请求邻接的路由器发送链路状态更新包（LSU） LSU：用于回应链路状态请求包LSR而发送的更新包 LSACK：用于对邻接的路由器发送过来的LSU包的确认回复 路由器的类型 内部路由器（IR） 是指所有接口都属于同一个区域的路由器 区域边界路由器（ABR） 是指连接一个或者多个区域到骨干区域的路由器，并且这些 路由器会作为域间通信量的路由网关。因此，ABR至少有一个接口是属于骨干区域，而且必须为每一个与之相连的区域维护不同的数据链路状态数据库。 骨干路由器（BR） 是指至少有一个接口是和骨干区域相连的路由器。这就意味着ABR路由器可以是骨干路由器。但是，并不是所有的骨干路由器都是ABR。 自主系统边界路由器（ASBR） 可以认为是OSPF域外部的通信量进入OSPF域的网关路由器。也就是说，ASBR就是用来把其他路由选择协议学习到的路由，通过路由重分配的方式注入到OSPF域的路由器。 关于OSPF的区域 为什么要实行多区域？ 因为单区域： 收到的LSA通告太多，OSPF路由器的负担很大 内部动荡会引起全网路由器的完全SPF计算 资源消耗过多，LSDB庞大，设备性能下降，影响数据转发 每台路由器都需要维护的路由表越来越大，单区域内路由无法汇总 解决方案： 把大型网络分隔为多个较小，可管理的单元-区域 减少了LSA泛洪的范围，有效地把拓扑变化控制在区域内，达到网络优化的目的 充分利用OSPF特殊区域的特性 ，进一步减少LSA泛洪，从而优化路由 多区域提高了网络的扩展性，有利于组建大规模的网络 特殊区域 stub（末梢区域）： Stub区域是一些特定的区域，该区域的ABR会将区域间的路由信息传递到本区域，但不会引入自治系统外部路由，区域中路由器的路由表规模以及LSA数量都会大大减少。为保证到自治系统外的路由依旧可达，该区域的ABR将生成一条缺省路由Type-3 LSA，发布给本区域中的其他非ABR路由器。 123R1/R2：router ospf 1 area 100 stub 结果是：由于R2既是处于area 100，又处于area 0，所以，当“show ip route ospf”的时候，只有R1上的OSPF路由条目（OE1、OE2）会被替换成默认路由指向骨干路由，而R2上的路由条目是不会被替换的。此图右边使用的是EIGRP，也可以使用除OSPF外的其他路由协议，如果我们要引入自治系统外的路由，我们要在R3上做“路由重分布”。 totally-stub（完全末梢区域）： 使用的前提条件和stub的一样，只是totally-stub要更“狠”，它的作用是：将从它路由协议重分布到OSPF的路由条目（OE1、OE2）及OIA（区域间学习到的路由）全部替换成默认路由指向骨干区域。 12router ospf 1 area 100 stub no-summary NSSA区域 NSSA（Not-So-Stubby Area）区域是Stub区域的变形，与Stub区域的区别在于NSSA区域允许引入自治系统外部路由，由ASBR发布Type-7 LSA通告给本区域。当Type-7 LSA到达NSSA的ABR时，由ABR将Type-7 LSA转换成Type-5 LSA，传播到其他区域。该区域使用的前提是，当其他协议区域跨非骨干区域连接到骨干区域时，如下图所示，RIP跨了area 10连接到了area 0。 12router ospf 1 area 10 nssa totally-nssa(完全非纯末梢区域) 该区域的ABR不会将区域间的路由信息传递到本区域。为保证到本自治系统的其他区域的路由依旧可达，该区域的ABR将生成一条缺省路由Type-3 LSA，发布给本区域中的其他非ABR路由器。 12 router ospf 1 area 10 nssa no-summary 网路类型 1、broadcast（multi-access）：广播型（多路访问）网络，链路协议是以太网时默认为这个网路类型，允许多个设备连接，访问相同的网络，而且提供广播的能力，在这样的网络中必须要有一个DR和一个BDR。 2、NBMA：非广播型多路访问网络，虽然从一个接口可以达到多个目的的节点，但是网络本身不支持广播功能，当链路层协议是帧中继、ATM或X.25时，ospf缺省默认网络是NBMA，此时ospf的邻居需要管理员手工指定。在该类型的网络中，以单播方式发送协议报文。 3、Point-to-point：点到点网络，是指该接口通过点到点的方式与一台路由器相连，此类型网络不需要进行OSPF的DR、BDR的选举。当链路层协议是PPP或者HDLC时，ospf缺省认为网络类型是P2P，此时以组播的方式（224.0.0.5）发送协议报文。 4、Point-to-multipoint：点到多点网络，是指该接口通过点到多点必须是由其他网络类型强制更改而来。常用做法时间NBMA网络改为点到多点的网络，在该类型的网络中，缺省是组播的方式（224.0.0.5）发送协议报文，也可以根据用户需要，以单播形式发送协议报文。 虚链路 当非主干区域与主干区域之间不连续时，必须建立虚链路，也就是说两者之间相隔其他area区域。 配置：（两端都要配置） 12router ospf [pid] area [area_id] virtual-link [router_id] 其中： area_id：指虚链路经过的中转区域的id router_id：指虚链路另一端的router_id]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>ospf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ospf（一）]]></title>
    <url>%2F2019%2F03%2F28%2Fospf%2F</url>
    <content type="text"><![CDATA[基础概念 OSPF（开放最短路径优先）是一种链路状态路由协议，无路由循环，属于IGP。 OSPF协议包直接封装于ip,协议号为89 OSPF使用的组播地址： 所有OSPF路由器——224.0.0.5；DR、BDR——224.0.0.6 管理距离：110 Router-ID：在一个OSPF域中，唯一地标识一台OSPF路由器 在未手工指定的情况下，如果本地有激活的look back接口，则取look back接口中IP最大值，如果没有LP接口，则取激活的物理接口IP 中的最大值 为了提高路由器的RID的稳定性和网络的稳定性，建议手动的设置路由器的Router-ID（在OSPF的进程下修改） 项目实施中，一般是建立look back口，并且手工指定look back口地址为router-id DR、BDR DR的作用：多路访问中为了减少邻接关系和LSA的泛洪，采用DR机制，BDR提供了备份 MA网络上的所有路由器均与DR、BDR建立邻居关系 选举比较顺序： 接口优先级数字越大越优先（优先级为0不能参与DR的选举，此时处于two-way状态） Router-ID越大越好 非抢占 通过控制接口优先级是控制DR选举的好办法 DR的选举是基于接口的，如果说某个路由器是DR，这种说法是错误的 在实际的网络环境中，可能会将接口的OSPF网络类型进行更改，已跳过DR、BDR的选举过程，加快OSPF邻居的建立过程 COST值=参考带宽（10的8次方）/接口带宽 接口带宽为接口逻辑带宽，可以使用bandwith命令调整，主要用于路由计算，而不是接口物理带宽，但一般情况：接口逻辑带宽=接口物理带宽 手工修改的方法： 建立邻居的条件 相邻两台路由器运行OSPF协议 两台路由器直接相连 再同一自治系统内 HELLO/DEAD时间一致 区域ID一致 认证密码一样 MTU值一致 网络类型一致 链路两端接口掩码一致 建立邻居的过程 在初始的情况下，A、B在某个接口激活了OSPF后都会在这个接口上发组播的OSPF hello包，目的是发现OSPF邻居。hello包里，有个active neighbor字段，用来存储路由器在某个OSPF接口上发现的邻居。 当OSPF路由器（B）在某个OSPF接口上收到邻居发来的HELLO包，他会记录下A，并且将A 的状态记为init，然后将A的Router-ID存储在自己将要发送 的hello包里的active neighbor字段发送出去，这样A就会收到这个hello包，并且在这个包里面找到自己的router-id，那么此时A就会以为自己与B已经建立了关系，因此A会将B 的邻居状态置为 two-way。同时，A也会继续发送hello包，将B的router-id放在active neighbor字段里面，而B收到这个hello包并看见了自己的router-id后，B也会将A的状态two-way，此时到达第一个稳态。 接下来A、B会进入ex-start状态并开始进行master-slave的协商，目的是为了决定在后面的LSA交互中，谁来决定DD报文的序列号，而router-id大的那个会成为master，这个协商过程由不含LSA的DBD包来完成。当OSPF接口收到一个DBD包并且其中的I为0的时候，此时他就知道ex-start状态已经过去了，于是将邻居的状态置为ex-change，并存储对端发送过来的DBD包所包含的LSA信息，同时他也要发送自己OSPF DATABASE的摘要给邻居。 当A收到一个M位为0的DBD包时，他就知道这个是邻居发来的最后一个DBD包，此时如果A发现这些DBD包中有自己感兴趣的LSA信息时，它会将B置为loading状态，并且发送LSR报文去请求更详细的报文信息。当B收到LSR后，会以LSU包含所需信息回复。收到LSU后，A会将有关信息存储在自己的LSDB中，并且发送一个LSACK确认，当所有网络上的LSA全部的到更新后，它会将邻居置为FULL。 关于各状态说明 down：尚未收到邻居的hello，或者开始发送hello给邻居 attempt：尝试发送hello信息给邻居，但还没有收到任何信息（仅仅在NBMA模式下有效） INIT：收到了来自邻居的hello，但是邻居的hello中没有本路由的ID（说明邻居还没有收到来自本地发送的hello） two-way：双向邻居建立（在hello包中可以看见自己的router-id），如果是多点访问网络，本阶段同时完成DR/BDR的选举 exstart：DD报文交互的准备阶段，协商master和slave关系 exchange：DD报文的交互阶段 loading：通过LSR和LSU报文的交互获取尚未发现的详细的链路状态信息 FULL：路由器之间完成了数据库的同步]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>ospf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EIGRP（四）]]></title>
    <url>%2F2019%2F03%2F27%2FEIGRP-3%2F</url>
    <content type="text"><![CDATA[关于EIGRP的综合实验 这个的话我是用iou做的，最后给出的配置可能在导出的时候有点小错误，所以就只是参考一下，一切以实验结果现象为主吧。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788配置：R1:interface Loopback0 ip address 1.1.1.1 255.255.255.255interface Ethernet0/0 ip address 192.168.12.1 255.255.255.0ip route 0.0.0.0 0.0.0.0 192.168.12.2R2:key chain cisco key 1 key-string ciscointerface Loopback0 ip address 2.2.2.2 255.255.255.255interface Ethernet0/0 ip address 192.168.12.2 255.255.255.0 ip summary-address eigrp 100 172.16.0.0 255.255.0.0 no shinterface Ethernet0/1 ip address 192.168.32.2 255.255.255.0 ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 cisco ip summary-address eigrp 100 0.0.0.0 0.0.0.0interface Serial1/0 ip address 172.16.1.1 255.255.255.0 secondary ip address 172.16.2.1 255.255.255.0 secondary ip address 172.16.3.1 255.255.255.0 secondary ip address 192.168.23.2 255.255.255.0 no sh ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 cisco ip summary-address eigrp 100 172.16.0.0 255.255.0.0router eigrp 100 variance 6 network 2.0.0.0 network 172.16.0.0 network 192.168.23.0 network 192.168.32.0 passive-interface Loopback0ip route 0.0.0.0 0.0.0.0 192.168.12.1R3:key chain cisco key 1 key-string cisco key 2 key-string huaweiinterface Loopback0 ip address 3.3.3.3 255.255.255.255interface Ethernet0/0 ip address 192.168.32.3 255.255.255.0 ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 ciscono shinterface Ethernet0/2 ip address 192.168.34.3 255.255.255.0 ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 cisco no shinterface Serial1/0 ip address 192.168.23.3 255.255.255.0 ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 cisco serial restart-delay 0no shrouter eigrp 100 network 3.0.0.0 network 192.168.23.0 network 192.168.32.0 network 192.168.34.0 offset-list 1 in 30 passive-interface Loopback0access-list 1 permit 4.4.4.4R4:key chain huawei key 1 key-string cisco key 2 key-string huaweiinterface Loopback0 ip address 4.4.4.4 255.255.255.255no shinterface Ethernet0/0 ip address 192.168.34.4 255.255.255.0 ip authentication mode eigrp 100 md5 ip authentication key-chain eigrp 100 huawei no sh]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>EIGRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EIGRP（三）]]></title>
    <url>%2F2019%2F03%2F27%2FEIGRP-2%2F</url>
    <content type="text"><![CDATA[认证 EIGRP只支持MD5认证。 123456key chain +name key 1（2、3…..） key-string +口令的nameinterface + 端口 ip authentication key-chain eigrp [pid] +name ip authentication mode eigrp [pid] md5 当有多个key的时候，会先从第一个开始匹配，如果第一个不成功，就算后面相同也不行，但是如果第一个成功，后面匹配不成功也可以；因为eigrp默认发送的是key1，如果没有定义key1那就发送key2，如果对端有多个key，会默认用key1协商。 默认路由的下发 有4种方式可以下发默认路由： 1、network 0.0.0.0： 1234ip route 0.0.0.0 0.0.0.0 +出接口 //（只能接出接口不能用下一跳，想象成该路由是本地的一个接口）router eigrp 100 network 0.0.0.0 //（宣告了所有接口，就把上面想象成本地接口的路由也宣告到EIGRP进程里了，所以其他路由器引入的是D路由） 2、redistribute static： 123ip route 0.0.0.0 0.0.0.0 接出接口或下一跳router eigrp 100 redistribute static 此时，其他路由器引入的是DEX。 3、ip summary eigrp AS号 0.0.0.0 0.0.0.0； 因为EIGRP支持CIDR，与RIP不同，可以直接汇总到0.0.0.0，然后传播该路由给其他路由器，不用再取一个静态默认路由做/0到/0的汇总。 4、全局模式下ip default-network 默认路由（出口所在网络）//不推荐使用 与RIP相比，在EIGRP中使用本命令要满足一下条件： 接口IP地址必须要是主类，不能是子网（或者利用auto summary） 接口必须宣告进EIGRP 使用命令下发时也必须写上这个接口的主类网络号 此外这种方法需要手工写一条指向null0的路由来防止环路。 优化 原因： 1、大型eigrp会有以下问题： 需要处理的网络路由条目很多 大量的邻居，需要维持大量的拓扑表 需要发送大量的路由更新，网络充斥着大量的查询和应答 2、这使得影响网络可扩展性的因素增多： 邻居间交换的信息量 路由器的数量 拓扑深度 网络中的替代路径数 3、陷入主动状态（SIA） 路由器陷入SIA状态后会发起查询状态，仅当收到每个查询的回复后，该路由器才会脱离SIA状态进入到被动状态。 如果路由器在3分钟内没有收到查询应答，会陷入SIA状态，此时路由器会重置与为应答路由之间的邻接关系。 导致路由器进入SIA状态的原因： 路由器太忙无法回复查询 路由器之间的链路质量低劣 单向链路 防范路由器进入SIA，路由器新增了SIA-查询、SIA-应答 解决方法： 限制路由查询： 在合适的路由器上进行路由汇总 将远程路由器设置为末梢路由器 在中央-分支网络中，stub路由器会将所有非本地数据流转发给hub路由器，而不需保存完整的路由表 对于hub路由器来说，不应将stub路由器作为中转路由器，禁止stub路由器将hub路由器通告给其他hub路由器 stub不会收到查询，与stub区域相连接的hub路由器会代替stub路由器对查询做出应答 配置]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>EIGRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EIGRP（二）]]></title>
    <url>%2F2019%2F03%2F27%2FEIGRP-1%2F</url>
    <content type="text"><![CDATA[Metric计算 链路的相关衡量参数： 而对于EIGRP而言，只需要参考网络链路的延迟和带宽： 注：5K默认为10100，可以通过metric weights tos K1 K2 K3 K4 K5,其中tos值默认为0 比如： 关于EIGRP的五个包和三张表 EIGRP协议的所有数据包通过IP头部的协议号88来进行标识。 五个包： Hello：用于邻居的发现和恢复进程，使用组播的方式进行发送，是不可靠的 ACK：对Update、Reply、Query包进行确认，使用单播和不可靠的方式 Update：用于传递路由更新信息，使用单播或者组播的方式并且可靠的方式进行发送 Query：查询消息，使用单播或者组播 Reply：回复消息，使用单播，可靠 如果任何数据包通过可靠的方式发送组播出去，而没有从邻居那里收到一个ACK数据包，那个这个数据包会以单播的形式重新发送给那个没有相应的邻居，如果经过16次单播重传没有收到ACK，那么这个邻居会被宣告无效。 三张表： 路由表，将拓扑表中计算得到的最佳路径存放在路由表中。 拓扑表：存放着从EIGRP邻居学习到的路由信息，不管这些路由条目是否满足FC条件，所有的路径都可以放在这张表中 其中： 12345Show ip eigrp topology //查看拓扑表,只能看到满足FC条件的路由信息（最佳路径和备份路径）Show ip eigrp topo all-links //所有的路由信息（最佳路径，备份路径，不满足FC路径）p：passive，表示这个路由条目处于一个正常主状态A：表示这个路由条目处于一个不正常的状态（SIA），和query包有关1 success 表示有一个继承者 查看某去往某条路由的详细信息： 邻居表：保存着已经学习到的邻居关系。 建立邻居的过程 非等价的负载均衡 在CEF（快速转发机制）下，EIGRP支持最多16条等价的路由路径上实现等价的负载均衡。在不同的是，它也支持非等价的负载均衡。其中variance定义了一个倍数因子，用来表示一条路由的度量值和最小代价路由的差异程度。variance默认是1，表示要实现负载均衡，多条路由的度量值必须是相同的。 注意： 增加到负载共享“组”中的路由条目不能超过最大路径条数的限制。 下一跳路由器必须在度量值上更接近目的网络。（满足FC可行条件） 最小路由代价的度量值乘以variance后，必须大于所增加的非最小路由代价的度量值。 如果数据包转发是快速交换或只是缺省配置的CEF交换，就按照每个目的地进行负载均衡；如果数据包转发是处理交换或者是更改的CEF交换(no ip cef and no ip route-cache)，就按照每个数据包进行负载均衡。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>EIGRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EIGRP（一）]]></title>
    <url>%2F2019%2F03%2F27%2FEIGRP%2F</url>
    <content type="text"><![CDATA[特性： 1.Cisco私有的高级距离矢量协议 2.支持无类路由，支持VLSM 3.DUAL，EIGRP的核心算法，避免环路 4.快速收敛 为什么能进行快速收敛？ EIGRP内部有一个机制，除了最优路径以外还有备份路径，当最有路径不可用的时候，回去查找是否有备选路径，如果有的话则自动切换到备选路径，如果没有的话，则重新进行计算，生成路径。 5.支持增量更新 当网络发生变化的时候（新增了路由条目），则只会向邻居发送新增的网络路由信息 周期性更新 触发更新 6.支持多种网络层协议 IP IPX Apple Talk 7.有灵活的网络设计 扁平式的网络结构，没有中心点（OSPF是层次性，有中心区域，对中心区域的设备负载要求很高，但是网络易于拓展），网络难以拓展，最大跳数（用来限制网络规模的大小，不作为度量值的计算）默认为100，可用metric maximum-hops 配置成1-255之间的数值。 8.用组播和单播来代替广播 EIGRP发送路由信息的时候会先以组播的形式来发送路由，且等待邻居的确认，如果没有收到邻居的确认会认为组播发送失败，此时会以单播的形式单独发给没有收到确认的那台路由器，并且存在单播重传机制。组播地址：224.0.0.10。 9.支持在任意点进行手工汇总 对比OSPF在区域间进行汇总，EIGRP基于端口的汇总，支持在任意一个端口做手工汇总。 10.支持等价和非等价的负载均衡 EIGRP的查询过程： 1.当路由器丢失最优路径，且没有备份时，会向所有的邻居（除了stub）发送query查询，丢失的路由条目会置为active state（passive） 2.query被从路由器所有启用eigrp的接口上，发送到除继任者（最优路径的下一跳）之外的所有eigrp邻居 3.如果eigrp邻居没有关于丢失路由的信息，query会被发送给该邻居的其他的eigrp邻居 4.如果路由器有关于丢失路径的替代路由，将以reply响应 5.当受到整个分支网络的响应时，停止查询 DUAL EIGRP的核心算法（自动扩散更新算法），保证100%没有环路，计算出最优路径和备份路径，并且能够进行快速收敛。 相关术语： FD：可行距离，到达每一个目的地的最小度量值作为该目的网络的可行距离。 AD：通告距离，下一跳路由器到达目的网络的距离 FC：可行性条件，本地路由器的一个邻居路由器的通告距离AD值要小于本地路由器的FD FS：可行后继路由器，在满足FC条件下，邻居路由器就会成为改目的网络的一个可行后继路由器。 S：后继路由器，对于在拓扑表中的每一个目的网络，将选用拥有最小度量值的路由并放置到路由表中，通告这条路由的邻居就会成为一个后继路由器，或者是到达目的网络的数据包的下一跳路由器。也就是最佳路径的下一跳路由器。 其中，FC和FS是避免环路的一项核心技术。 比如： 从R1到R5有三条路径： R1------R3-------R5 FD 20 S R3 AD 10 R1------R2-------R5 FD 30 FS R2 AD 10 R1------R4-------R5 FD 45 AD 25 为什么能够放环？ 对于R1—R6—R1—R3----R5 这条路径而言，AD值为为25，不满足FC条件，此时这条路就不会被使用。]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>EIGRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rip]]></title>
    <url>%2F2019%2F03%2F27%2Frip%2F</url>
    <content type="text"><![CDATA[RIP 路由选择信息协议，典型的距离矢量协议 特性： 1、基于UDP协议的520端口 2、收适用于小型网络，RIP的metric值使用跳数，最大跳数为15跳（16跳默认为不可达） 3、cisco管理平台上的管理距离为120 4、周期性的泛洪整张路由表 5、依照传闻的更新 6、逐跳更新 版本： 1.RIP v1 使用广播更新路由表 有类路由协议 不支持VLSM 没有认证功能 不支持路由标记（Tag） 更新的路由条目中没有Next-hop信息 2.RIP v2 使用组播（224.0.0.9）更新路由表 无类路由协议 支持VLSM 支持手工汇总 支持路由标记功能 更新的路由条目中有Next-hop信息 3.RIPng 支持IPv6协议，基于UDP端口的521 被动接口(passive-interface)： 配置了被动接口命令的路由器可是说是在特定的数据链路上，将该路由器作为一台“静”主机看待，他只是在监听这条特定链路上相应的RIP广播，从而更新自己的路由表而不会在被动接口上响应收到的请求消息。这样子做可以在一定程度上降低广播风暴。 1passive-interface +接口 配置单播路由： 在RIP协议进程下配置neighbor + x.x.x.x ，能够使RIP协议能够以单播的方式向（x.x.x.x）的路由器发送通告。 相关调试命令： 1debug ip rip events 偏移列表（offset-list）： 可以通过offset-list来改变路由的度量值，该命令指定一个数值来加大路由的度量值，并且参照访问列表（ACL）来决定哪些路由条目需要修改。 123access-list x permit x.x.x.x x.x.x.xrouter rip offset-list x in|out y（所加值）+ 接口 最小化更新的影响： 普通的RIP的hello更新时间为30秒，更新内容为完整的路由表，对于较低带宽的链路，路由选择更新会对网络流量产生很大的影响，我们可以调整路由选择协议的计时器降低更新的频率，也可以配置触发扩展特性来消除周期性的RIP更新。 使用接口模式下的ip rip triggered可以启动RIP协议的触发扩展特性，这条命令仅仅在串行链路上有效，并且必须在链路的两端同时配置才会有效，配置后，链路上的更新将会变得最少，仅仅包括路由表最初的交换信息和路由表发生变化时的更新信息。 RIPv2的链路认证： 支持简单的明文认证以及MD5认证,默认是明文认证。 步骤： 定义一个带名字的钥匙链 定义在钥匙链上的钥匙 在接口上启动认证并指定使用的钥匙链 123456key chain +name key 1（2、3…..） key-string +口令的nameinterface + 端口 ip rip authentication key-chain +name ip rip authentication mode md5]]></content>
      <categories>
        <category>NA &amp;&amp; NP</category>
      </categories>
      <tags>
        <tag>Rip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（七）]]></title>
    <url>%2F2019%2F03%2F26%2Fzabbix-6%2F</url>
    <content type="text"><![CDATA[关于zabbix的触发器（triggers） 官方解释说，触发器是“评估”由监控项采集的数据并表示当前系统状况的逻辑表达式。具体的话看官方文档 关于这个的话，相信大家对触发器应该有一点概念，我们一般是通过触发器来设置一下被监控项目的阀值，当达到这个阀值或者是超过时（也即是触发器的值变成了PROBLEM），则代表我们所监控的遇到了问题，此时需要去解决。 每当zabbix server接收到作为表达式一部分的新值时，都会去重新计算触发器表达式，得到新的状态。 1、创建Triggers 然后点击右上角的 create triggers name：触发器名称 severity：严重性，通过点击不同的按钮来设定 expression：用于异常条件的逻辑表达式（在表达式构造器列出了所有单个表达式。打开测试窗口，点击在表达式列表下方测试） ok event generation：事件成功的选项 expression：ok事件基于与问题事件相同的表达式生成 恢复表达式：如果问题表达式计算为false，恢复表达式计算为true，则生成Ok事件 none：在这种情况下，触发器将永远不会返回到ok状态。 recovery expression（如果上面选择这个的话）：逻辑表达式用于定义问题解决的条件。只有在问题表达式为false才对恢复表达式进行评估，如果此时问题条件仍然存在，则不可能通过恢复表达式来解决问题。 PROBLEM event generation mode（异常事件生成模式）：生成异常事件的模式 single（单个）：当触发器第一次进入“异常”状态的时候，生成一条单个事件 multiple（多重）：每一个触发器“异常”，评估都会生成一条事件 更多的话看文档吧 关于触发器的表达式 触发器表达式： 触发器中使用的表达式很灵活，通过使用他们去创建关于监控统计的复杂逻辑测试。 1&#123;&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)&#125;&lt;operator&gt;&lt;constant&gt; 其中： function（函数） 触发器函数允许引用采集的值、当前时间以及其他。关于触发器支持的函数 operator（运算符） 触发器支持下列运算符 其中： 1、not，and 和or运算符区分大小写，而且必须小写，他们也必须被空格或者是括号包围。 2、所有运算符中，除了 - 和 not ，都有左到右的关联性。这两个是非结合的(意味着-(-1)和not (not 1)应该用- -1 and not not 1代替).) 缓存值 触发器评估所需的值由Zabbix server缓存。由于此触发器评估在服务器重新启动后一段时间导致较高的数据库负载。当监控项历史数据被移除（手动或housekeeper）时，缓存值不会被清除，因此服务器将使用缓存的值，直到它们比触发器函数中定义的时间段或服务器重启的时间长。 官方示例： 关于其中的监控项的键值，我们可以在item里面找到。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（六）]]></title>
    <url>%2F2019%2F03%2F22%2Fzabbix-5%2F</url>
    <content type="text"><![CDATA[关于zabbix的screens（展示器） 关于这个东东，简单地说就是我们可以通过它了解某台设备地运行情况及相关监控参数，也可以是某几台设备某些监控项地对比情况。 安装后默认存在一张zabbix server的sreens，如图： 创建screen 我们在这里输入我们需要生成screen的名字（最好是与我们所要监控的内容命令），然后列与行（把它想成一个网格，每个格子里面是我们需要添加的监控选项） 然后我们选中刚才创建的screen，点击编辑 网格可通过边缘的加减号进行增删，通过点击每个网格的change我们可以来添加需要的服务 其中： Resource：表示元素的来源 Graph：则是选择相应的主机的监控项 从图中我们可以看见，在右上角我们可以选择不同的主机，也就是说我们可以对一台主机的不同参数进行同时显示，也可以不同主机的不同监控项（比如监控这个网内所有主机的cpu负载等等） Horizontal align： 水平对齐方式 Vertical align：垂直对齐方式 Column span：列跨度，当前元素占用几个列 Row span：行跨度，当前元素占用几个行 依次添加好就行 关于Maps（拓扑） 在Zabbix中配置拓扑图，首先需要创建拓扑图，并定义关于拓扑图的常规参数，然后就可以使用图像等元素填充实际拓扑图。 你可以使用主机、主机组、触发器、图像或其他拓扑图元素填充拓扑图。 图标用来表示拓扑图元素。你可以定义与图标一起显示的信息，并设置以特殊方式显示最近的问题。与此同时，你可以链接图标并定义在此链接上显示的信息。 你可以通过点击图标来添加可访问的自定义URLs。因此，你可以将主机图标链接到主机属性，或将拓扑图图标链接到其他拓扑图。 通过 Monitoring → Maps对拓扑图进行管理，可以对其进行配置、管理和浏览。在监控视图中，你可以点击图标，并利用一些脚本和URLs的链接。 系统默认存在一个Local network的网络拓扑 选中它我们可以看见： 当比如像CPU负载70%以上，它会在图标上显示提醒，和图中一样，如果我们选中某台设备，单击： 此时我们可以看见，我们可以通过脚本进行管理，或者是进行其他一些服务，比如查看相应的监控数据。 关于如何创建一个map，以及管理，我觉得还是看官方文档吧 顺便提一下，前面忘记说了，如果觉得英文不好看懂，可以将语言换成中文，包括主题背景的更改，在Administrator下的user用户，找到admin，然后进行修改]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（五）]]></title>
    <url>%2F2019%2F03%2F22%2Fzabbix-4%2F</url>
    <content type="text"><![CDATA[zabbix自动发现和注册 当网络中存在很多设备需要我们去监控的时候，一台一台地添加很累，所以，我们可以利用server端的某些规则，让它自己去发现局域网中存在的设备，自动地去把它们添加进监控平台里面。zabbix中有个叫Discovery模块，来实现这个需求。 其中： Name：规则的名字 Discovery by proxy：通过代理搜索（不同的代理发现的之际被认为是不同的主机） IP range：zabbix_server搜索的范围 Delay：搜索一次的时间间隔 Checks：检测方式，如ping或者fping Device uniqueness criteria：以IP地址作为被发现主机的标识 Update interval：多久检测一次 zabbix网络发现可基于以下信息： IP范围 某些外部服务（FTP、SSH、WEB、IMAP、POP3） 来自zabbix客户端的信息（仅支持未加密模式） 来自SNMP客户端的信息 一条发现规则始终由一个发现进程处理，IP范围不会在多个发现进程之间分割。并且发现检测与其他检测独立处理。如果一些检测没有找到服务，那么其他的检测仍会处理。 Action 当我们服务器发现IP主机后，并不会自动添加到监控列表，添加这个动作是在被发现后所触发的一个Action。所有的Action都是基于发现这个事件。 比如说我们发现以后可以做到： 发送通知 添加/删除主机 启用/禁用主机 添加主机到组 从组中删除主机 将主机链接到/取消链接模板 执行远程脚本命令 比如我们先配置一条发现规则，或者修改默认的规则： 然后创建一个Action： 或者是编辑默认的规则： 此时如果发生以下情况，动作将被激活： “Zabbix agent”服务是“up” system.uname(规则中定义的Zabbix agent键值)包含“Linux” IP地址位于192.168.248.4-254之间 该动作会执行这样的操作： 添加主机 将发现的主机添加到“Linux servers”组 链接主机到“Template OS Linux”模板。Zabbix将自动开始使用“Template OS Linux”模板中的项目和触发器来监控主机 此时我们通过Monitoring下的Discovery来查看通过发现规则找到的主机 这里，由于192.168.248.1是我windows，而我们前面所修改的action是针对Linux（我们上面是添加linux的模块），所以我们在创建一个新的action来针对windows。 其他设置看官方文档]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（四）]]></title>
    <url>%2F2019%2F03%2F22%2Fzabbix-3%2F</url>
    <content type="text"><![CDATA[关于zabbix的监控项（item） 关于zabbix的item，就是一个监控的特定指标，并且通过这个指标来搜集相关的监控数据。 可参考 官方文档 这是新建一个item项的页面： 1、所有带 * 号的都是必填，关于type（类型）： 监控类型： zabbix agent zabbix自带的客户端程序（被动模式），zabbix server主动向它收集监控数据。agent提供丰富的key，包括不限于CPU、内存、网络等。一般是首选的，此外，需要注意的是，server检索数据有超时限制，如果检索数据经常超过30s，那么建议使用agent(active） agent （active） 主动模式，和上一个相同，但是数据由zabbix agent主动提交给server simple check 基本的检测，可以检测网络、端口、fping，功能很少并且不需安装客户端 snmp check 有V1、V2、V3，推荐如下场景 客户基于安全考虑，不同意安装agent 路由器、打印机等设备无法安装，但是支持SNMP协议 不喜欢频繁对agent升级 zabbix internal zabbix系统内部使用，比如趋势数据记录数量、历史记录数量等，日常业务监控用不上 zabbix trapper 也需要安装agent，借助/bin/zabbix_sender 将数据提交至zabbix server，适合以下情况： 检索数据时间较长 同一时间有大量的数据要提交 zabbix aggretage 这是一个聚合的检测，例如我想知道某个组的host负载平均值，硬盘剩余总量，或者某几台机器的这些数据，简单的说，这个方法就是用来了解一个整体，而不需要我们逐台去看，这个方法的数据全部来与数据库 external check server运行脚本或者二进制文件来执行外部检测，外部检测不需要在被监控端运行任何agent，但是如果过度使用外部检测，会严重降低zabbix系统性能 database monitor zabbix通过调用ODBC来获取数据库的数据以及数据库状态等等信息 IPMI agent 用于监控硬件设备 SSH agent zabbix使用提供的ssh信息登录服务器，执行指定的脚本来检索数据 telnet agent 用于Windows端 2、关于key(键值)： zabbix服务器在与被监控端通信时，会通过某些协议去查询我们设置的这个key值，监控端会返回相关信息。 3、Host interface（主机接口） 4、Update interval（更新间隔） 5、History storage period（历史数据保留时长） 在数据库中保存详细历史的持续时间，按秒存储，支持时间后缀。 6、Trend storage period（趋势存储时间） 在数据库中保持聚合的历史时间 其他的参数，可在官方文档中去看吧]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker(二)]]></title>
    <url>%2F2019%2F03%2F22%2Fdocker-1%2F</url>
    <content type="text"><![CDATA[关于DockerFile dockerfile是一种能被docker程序解释的脚本，dockerfile由多条指令组成，每条指令对应Linux系统中不同的命令，基于dockerfile可以自定义创建生产环境所需的docker镜像，通过镜像可以启动所需的docker容器。 docker程序将这些docerfile指令翻译为真正的Linux命令，dockerfile有特定的书写格式和支持命令。 参数： FROM &lt; image &gt;:&lt; tag &gt;：FROM指令标识指定一个基本的镜像源，或从公共库拉取一个镜像源，dockerfile文件第一行必须指定FROM基础镜像源。 MAINTAINER：设置dockerfile编写人或者维护者的信息 LABER &lt; key &gt;=&lt; value &gt;:设置标签，采用键值对的方式 RUN &lt; command &gt;：核心指令，表示运行的Linux指令，每条指令在当前基础镜像上执行，并且提交成为新的镜像 EXPOSE &lt; port &gt; [&lt; port &gt;/&lt; protocol &gt;]：用来指定docker容器中监听的端口，用于外界宿主机互联访问，启动docker时，可以通过-P（或者 -p HOST_PORT:CONTAINER_PORT），主机会自动分配一个端口号转发到指定的端口 注：该指令只是起声明作用，并不会自动完成端口映射 ENV &lt; key &gt;=&lt; value &gt;：设置环境变量，执行RUN指令及docker启动时被引用 WORKDIR /path/to/workdir：设置工作目录，执行RUN，ADD，COPY，ENV指令时的基础路径 注：可以使用多个WORKDIR指令，如果后续指令参数是相对路径，则会基于之前命令指定的路径。比如： 1234WORKDIR /aWORKDIR bWORKDIR cRUN pwd 最终路径为/a/b/c，但是最好还是使用绝对路径 COPY 和ADD ：Linux系统复制和增加命令，ADD在和COPY相同的基础上，ADD允许是一个URL，同时ADD的是一个压缩格式文档，将会解压复制 CMD和ENTRYPOINT：配置docker容器启动后执行的命令，每个dockerfile至少指定一个CMD命令或ENTERPOINT，两者都可以指定shell或exec函数调用的方式执行命令，默认dockerfile run启动镜像之后便会退出容器，需要一个长时间运行的命令，使得它一直运行。 CMD [“executable”,“param1”,“param2”]：运行一个可执行的文件并提供参数 CMD [“param1”,“param2”]：为ENTRYPOINT指定参数 CMD command param1 param2：以/bin/sh -c的方式执行命令 ENTRYPOINT [“executable”,“param1”,“param2”]：首选的执行格式 ENTRYPOINT command param1 param2：以/bin/sh -c的方式执行命令 关于两者： 每个dockerfile只能有一个CMD/ENTRYPOINT，超过一个CMD只有最后一个生效 dockerfile中同时设置CMD和ENTERPOINT，docker在build过程中会将CMD中指定的内容作为ENTERPOINT的参数 如果docker启动需要运行多个启动命令，彼此之间可以使用*&amp;&amp;*分开，最后一个命令必须为无限运行的命令，否则启动的容器最后会被退出 VOLUME [DIR]:设置本地挂载目录，用于存放数据库和需要保持的数据 USER daemon：指定docker运行时的用户名或UID，后续的RUN也会使用指定用户 ONBUILD [INSTRUCTION]：配置当前所创建的镜像作为其他新创建镜像的基础镜像时，所执行的操作指令，也就是说当我们创建一个基于这个镜像的子镜像时，会先执行ONBUILD的指令]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker（一）]]></title>
    <url>%2F2019%2F03%2F21%2Fdocker%2F</url>
    <content type="text"><![CDATA[什么是docker docker是基于GO语言和遵循apache2.0协议实现的开源容器引擎。docker可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化 应用场景： 1、web应用的自动化打包和发布 2、自动化测试和持续集成、发布 3、在服务型环境中部署和调整数据库或者其他的后台应用 4、从头编译或者扩展现有的openshift或者cloud foundry平台来搭建自己的PaaS 优点： 1、更快速的交付和部署 使用docker，开发人员可以使用镜像来快速构建一套标准开发环境，然后使用完全相同的环境来部署代码。可以快速创建和删除容器，节约开发、测试、部署的大量时间。 2、更高效的资源利用 运行docker容器不需要额外的虚拟化管理程序的支持，它本身就是内核级的虚拟化，可以实现更高的性能，同时对资源的额外的需求很低。 3、更轻松的迁移和扩展 docker几乎可以在任意的平台上运行，包括物理机、虚拟机、公有云、私有云、个人PC，服务器等，同时支持主流的操作系统。具有高兼容性 4、更简单的更新管理 使用dockerfile，只需要进行小小的配置修改，就可以代替以往大量的更新工作，所有修改都已增量的方式被分布和更新，从而实现自动化并且高效的容器管理。 关于docker的安装 12345yum install -y yum-util device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum makecache fastyum install -y docker-cesystemctl enable docker &amp;&amp; systemctl start docker 配置docker的加速： 12systemctl daemon-reloadsystemctl restart docker 6种namespaces 1、UTS：实现主机名和域名隔离 2、User：实现用户组和用户隔离 3、Mount：实现挂载点（文件系统）隔离 类似于chroot，mount命名空间可以将一个进程的根文件系统限制到一个特定的目录下。 挂载命名空间允许不同命名空间的进程看到的本地文件位于宿主机不同路径下，每个命名空间中的进程所看到的的文件目录彼此是隔离的。也就是，不同命名空间中的进程，都认为自己独自占了一个完整的根文件系统（rootfs），但是实际上，不同命名空间中的文件彼此隔离，不会造成互相影响。 4、IPC：实现信号量、消息队列和共享内存隔离 5、Pid：进程隔离 Linux通过进程命名空间来 管理进程号，对于同一个进程，在不同的命名空间里有不同的进程号，每个进程命名空间有一套自己的进程号管理方法。进程命名空间是一个父子关系结构，子空间中的进程对于父空间是可见的。新fork出来的一个进程，在父命名空间和子命名空间将分别对应不同的进程号： 然后我们用pstree，查看进程树： 6、Net：网络设备，网络栈、端口等隔离 一个网络命名空间为进程提供了一个完全独立的网络协议栈的视图，包括ipv4和ipv6协议栈，ip路由表，防火墙规则等。这样每个容器的网络就能隔离开来。可以使用docker network ls查看当前主机上存在的网桥。 使用brctl工具还可以看见连接在网桥上的虚拟网口的信息。每个容器默认分配了一个网桥上的虚拟网口，并将docker0的IP地址设置为默认的网关。容器发起的网络流量通过宿主机的iptables规则进行转发。 在内核3.8以后，用户就可以在proc/[pid]/ns文件下看到指向不同namespace号的文件，如下图，其中$$表示当前运行的进程号： 如果两个进程指向的namespace相同，就说明他们在同一个namespace下，否则不是。 关于docker的四种网络模式 基于docker run创建docker容器的时候，可以使用–net选项来指定容器的网络模式，默认有以下四种网络模式： host模式，使用–net=host指定 容器将不会获得一个独立的network namespace，而是与宿主机共用一个network namespace，容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。 container模式，使用–net=container:NAME_OR_ID指定 模式指定新创建的容器和已经存在的一个容器共享一个network namespace，除了网络方面相同一样，其他的像文件系统、进程列表还是隔离的，两个容器的进程通过lo网卡进行通信。 none模式，使用–net=none指定 如果docker处于这个模式下，docker容器拥有自己的network namespace，但是并不为docker容器进行任何网络配置。该容器没有网卡、IP、路由等信息，需要手工添加。 bridge模式，使用–net=bridge指定，默认设置 该模式会为每一个容器分配network namespace，设置IP、路由等，默认会将docker容器连接到一个虚拟网桥交换机docker0上。 关于桥接模式 1、启动docker容器，首先会在docker宿主机上创建一对虚拟网卡veth pare设备，veth设备总是成对出现的，组成了一条数据通道，数据从一段设备进入，就会从另一端设备出来，veth设备常用来连接两个网络设备 2、docker将veth pair设备的一端放在新创建的容器中，并取名为eth0，然后将另一端放在宿主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中 3、从docker0子网中分配一个IP给容器使用，并设置docker0IP的地址为容器的默认网关 4、此时，容器IP与宿主机能够通信，宿主机也可访问容器中的IP地址，在bridge模式下，连在同一网桥上的容器之间可以互相通信，同时也可以访问外网，但是其他宿主机不能访问docker容器IP，需要通过NAT将容器IP的port映射为宿主机的IP和port才可以 docker的架构 docker目前采用了标准的C/S架构，包括客户端、服务器两大核心组件，同时通过镜像仓库来存储镜像。客户端和服务端既可以运行在一个机器上，也可以通过socket或者是RESTful API来进行通信。 1、服务端 docker服务端一般在宿主主机后台运行，dockerd作为服务端接收来自客户的请求，并通过containerd具体处理与容器相关的请求，包括创建、运行、删减容器等。主要包括四个组件： dockerd：为客户端提供RESTful API接口，响应来自客户端的请求，采用模块化的架构，通过专门的engine模块来分发管理来自客户端的任务。 docker-proxy：是dockerd的子进程，当需要进行容器端口映射的时候，docker-proxy完成网络映射配置。 containerd：是dockerd的子进程，提供GRPC接口响应来自dockerd的请求，对下管理runC容器和镜像环境。 containerd-shim：是containerd的子进程，为runC容器提供支持，同时作为容器内进程的根进程。 dockerd默认监听本地的unix:///var/run/docker.sock套接字，只允许本地的root用户或docker用户组成员访问。可以通过dockerd -H 127.0.0.1：port 来指定监听的端口。 2、客户端 docker客户端为用户提供一系列可执行命令，使用这些命令可实现与docker服务端进行交互。我们使用的docker可执行命令即为客户端程序。与docker服务端保持运行方式不同，客户端发送命令后，等待服务端返回；一旦收到返回后，客户端立即执行结束并退出。客户执行新的命令，需要再次调用客户端命令。 客户端默认通过本地的默认监听本地的unix:///var/run/docker.sock套接字向服务端发送命令，如果服务端没有监听在该端口，需要显示的指出服务端地址。比如：假定服务端监听在本地的TCP连接1234端口上，那么我们可以这样： 1docker -H tcp://127.0.0.1:1234 info]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（三）]]></title>
    <url>%2F2019%2F03%2F20%2Fzabbix-2%2F</url>
    <content type="text"><![CDATA[客户端安装（yum） 同服务端安装类似，只不过这里安装的是zabbix-agent。 123rpm -Uvh https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpmyum makecache fastyum install -y zabbix-agent 修改配置文件*/etc/zabbix/zabbix_agent.conf* 123ListenPort=10050Server= [服务端地址]ServerActice= [服务端地址] 开启服务并放行10050端口 1234systemctl enable zabbix-agent.service systemctl start zabbix-agent.servicefirewall-cmd --add-port=10050/tcp --permanent firewall-cmd --reload 这里为啥是10050端口呢？因为是这样的，agent端是通过10050来与server端进行通信，而服务器端是监听10051端口。 源码安装(centos6) 下载安装包、解压 在这里下载源码包，然后解压。 创建安全用户 1useradd -s /sbin/nologin zabbix 进入到源码包进行编译、安装 12./configure --enable-agent --prefix=/usr/local/zabbix make install 如果过程中报错（pcre的缺失，则安装） 1yum install -y pcre* 编辑主配置文件（参照上面的yum安装） 关闭selinux和放行防火墙 123setenforce 0iptables -I INPUT 35 -p tcp -m tcp --dport 10050 -m comment --comment &quot;zabbix_agentd listen &quot; -j ACCEPTservice iptables save 设置开机启动 复制源码中的启动文件： 1cp /usr/local/src/zabbix-4.2.3/misc/init.d/tru64/* /etc/init.d/ 修改/etc/init.d/zabbix_agentd，将运行路径改为安装后的路径： 这里将DAEMON的路径修改为*/usr/local/zabbix/sbin/zabbix_agentd* 修改权限： 1chmod 755 /etc/init.d/zabbix_agentd 设置开机启动： 12chkconfig --add zabbix_agentdchkconfig --level 12345 zabbix_agentd on 如果报错说，不支持redhat，则在上面那个文件的#!/bin/sh 后加入如下两行注释： 12# chkconfig: - 95 95# description: Zabbix Server 安装proxy（Ubuntu）（为啥环境不统一呢，因为我想多熟悉不同的环境，所以别骂我） 添加软件仓库 123sudo wget https://repo.zabbix.com/zabbix/4.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_4.0-2+bionic_all.debsudo dpkg -i zabbix-release_4.0-2+bionic_all.debsudo apt update 安装 12sudo apt-get install zabbix-proxy-mysql sudo mysql_secure_installation 配置数据库进行相关设置参照server设置 导入数据库 1sudo zcat /usr/share/doc/zabbix-proxy-mysql/schema.sql.gz | sudo mysql -uroot -p zabbix 编辑配置文件 12345Server=192.168.248.128Hostname=zabbix_proxyDBName=zabbix_proxyDBUser=zabbixDBPassword=zabbix 启动服务、设置开机启动并放行防火墙 123service zabbix-proxy startsudo update-rc.d zabbix-proxy enablesudo ufw allow 10051/tcp 在server的web界面上配置 修改agent的配置文件（Server/ServerActive），让其指向proxy代理 添加主机，然后在最下面的选项选择代理 zabbix上添加监控主机 我们先点击到Configuration，可以看见首页出现一些host主机组，假设我们这里添加一台主机到Linux Servers组，当然，我们也可以自己创建一个，在右上角有一个Create host group。 点击Configuration下的Host，点击右上角的Create host 其中Host name填需要监控的主机agent配置文件中的HostName（个人喜欢配置本地的IP地址），Visible name是显示的易理解名字，可以自己想，Agent interfaces是需要监控的主机IP地址。Groups选Linux servers，或者是自己创建的那个组。 为创建的主机添加模板（假设我们这添加的模板为Template os Linux，这一步也可以在创建主机的时候进行） 选中刚才创建的主机，点击Templates，找到我们需要的，然后依次点击Add——Update。 此时，我们刷新页面可以看见 一个是我们所创建的主机，一个是本地，绿色图标表示健康运行，红色为错误，此时我们将鼠标移到上面可查看原因。 图表分析 我们此时进入到Monitoring下的Graphs页面，进行选择 选择刚才我们创建的主机，此时我们然后在下面就可以看见我们需要的东东啦 关于Windows下部署agent来受监控 去官网下载Windows agent包进行解压 比如我们在C盘下创建一个zabbix文件夹，然后将刚才解压的文件移过去 修改配置文件 将server改成我们的server主机IP，Hostname改成自己需要的。 服务注册 管理员权限打开power shell进入到zabbix（刚才我们在C盘下创建的那个文件夹） 配置本机防火墙 放行本机上的10050，允许流量进来，也就是我们在windows防火墙上配置入站规则 当然除了放行端口，你也可以放行zabbix.exe程序 配置Windows主机监控 1、我们新建一个Windows组 2、然后添加主机 3、模板选择Template OS Windows 这时我们就可以监控我们Windows上的东西啦 注：关于zabbix的其他东西在下章]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（二）]]></title>
    <url>%2F2019%2F03%2F20%2Fzabbix-1%2F</url>
    <content type="text"><![CDATA[安装zabbix 1、我们需要准备好LAMP环境，这里就用yum源一键安装 123456yum install -y httpd mariadb mariadb-server.x86_64 php php-mysqlmysql_secure_installation systemctl enable httpd.service mariadb.service systemctl start httpd.service mariadb.service firewall-cmd --add-service=http --permanent firewall-cmd --reload 2、安装zabbix及配置 可以去官网，上面有yum的简单安装教程 导入zabbix源并安装 123rpm -Uvh https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpmyum clean allyum install -y zabbix-server-mysql zabbix-web-mysql zabbix-agent 配置数据库 导入初始文件 修改配置文件 /etc/zabbix/zabbix_server.conf 在里面找到： 12345ListenPort=10051DBHost=localhostDBName=zabbixDBUser=zabbixDBPassword= [接数据库的密码] 编辑zabbix前端配置文件，去除时区的注释以及修改为本地的时区 通过NTP使时间同步 12yum install -y ntpdate.x86_64 ntpdate cn.pool.ntp.org 关闭selinux以及开启服务、放行防火墙 123456setenforce 0systemctl enable zabbix-server.service systemctl start zabbix-server.service systemctl restart httpd.servicefirewall-cmd --add-port=10051/tcp --permanent firewall-cmd --reload web页面安装zabbix 此时，打开网站页面，进行安装zabbix，有点像discuz的安装 输入相关信息点击下一步，接着会让你输入zabbix管理员的姓名，自行输入，直到完成安装，出现以下界面： 这里需要注意以下，这里的用户名和密码，是默认为：用户名（Admin）,密码（zabbix) 登录成功后如图： 注：客户机安装以及相应添加主机监控在下一章]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix（一）]]></title>
    <url>%2F2019%2F03%2F19%2Fzabbix%2F</url>
    <content type="text"><![CDATA[什么是zabbix呢？ zabbix是一个基于web界面的提供分布式系统监控的企业级开源解决方法，zabbix能监视各种网络参数，保证服务器系统安全稳定地运行，并提供灵活的通知机制以让SA快速定位并解决存在的各种问题。 优点 在《曝光》这本书 里面是这样说的： 支持自动发现服务器和网络设备 支持底层自动发现 分布式的监控系统和集中式web管理 支持主动监控和被动监控模式 服务器支持多种操作系统 agent客户端支持多种操作系统 基于SNMP、IPMI接口，zabbix agent方式监控客户端 基于web的管理方法 高水平的业务视图监控资源，支持日志审计、资产管理等功能 支持高水平的API二次开发、脚本监控、自key开发，自动化运维整合调用 我感觉很不错的就是： 1、能自己定义一个时间间隔，通过IPMI、agent、SNMP等协议去采集信息 2、能及时将数据放进数据库存储 3、能够进行数据绘图，多种图形展示出来 4、支持微信报警、邮件报警这些。只不过微信报警需要进行企业认证，所以我现在没用过。 相关组件 这图的话，根据我的理解来画的，大概是这样： 最后通过web来对进行管理。 说明： zabbix server：核心组件，对数据进行处理，负责执行数据的主动轮询和被动获取，计算触发器条件，向用户发送通知。它是zabbix agent和proxy报告系统可用性和完整性的核心组件。它自身可以通过简单服务远程检查网络服务（像web服务器和邮件服务器），它是所有配置、统计和操作数据的中央存储中心，也是监控系统的告警中心。 database：存储相关数据信息和配置信息 proxy：在分布式监控中，代理server，将获得的数据发送到server端 agent：收集本地信息，将数据发送到server或者proxy web：提供一个GUI的管理平台 核心进程 核心进程：是服务端的守护进程，其它进程的最终数据最后都提交给他 客户端的守护进程，负责收集客户端的相关信息 数据获取（get） 数据发送（sender） 分布式代理守护进程，分布式架构需要（proxy） 监控流程 首先我们将监控服务部署在客户端，定期收集相关信息返回给server端，当server端收到数据以后，将数据存储在数据库上，并在web上生成相应的图表信息，监控管理人员通过web来实时查看。 当zabbix监控某个项目的时候，我们设置一个阀值，一旦某些所监控的指标达到或者是超过的时候，监控机制就会启动报警或者是其他措施。 监控方式 agent：基于自身zabbix_agent客户端插件监控OS的状态 可以通过简单网络管理协议监控网络设备，通过设定SNMP的参数将相关监控数据传送至服务器端 智能平台管理接口即主要应用于设备的物理特性。最大的优点就是不管是否开关机，只要通电就可以监控 两种模式 主动模式 agent主动请求server端获取主动的监控列表，并主动将监控项内需要检测的数据提交给server或者proxy。 被动模式 server向 agent请求监控项目的数据，agent收到后发送相应的请求数据。 zabbix监控客户端默认为被动模式，关闭被动模式的方法是在配置文件中加入*startagents=0 *。 注：关于安装的话在下一章]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主从复制]]></title>
    <url>%2F2019%2F03%2F19%2F%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[这样做的的好处 增强服务的健壮性 实现了主从服务器之间的数据的同步，增加了数据库系统的可用性，当主服务器出行问题，可以在从服务器接管服务。 实现负载均衡 我们通过主从服务器之间实现读写分离，可以更快的对客户端的请求进行相应。比如说，我们可以尝试将主服务器实现数据的更新，让从服务器相应所有的请求，这样做则可以提高系统的性能和效率。 能实现数据的备份 过程 进行复制的时候，我们对复制中表的更新在主服务器上进行。其中，会涉及到三个线程来执行复制功能。其中一个在主服务器上，另外两个在从服务器上；其次还有两个日志文件，一个是bin-log，这个是用来记录在主库中存在的增删改数据的更新操作，另一个是relay-log，这个是从库上保存来自于主库中的操作信息。 1、当我们在从库上执行slave start命令时，也就是开启复制，这个时候，从库的I/O线程会通过我们提前在主库上创建的一个授权用户连接上主库，并请求主库发送指定位置（position点）后的bin-log日志内容。 2、主库收到消息之后呢，它的I/O线程就根据请求，发送从库想要的信息给它，并且更新自己的position点。 3、返回的信息会传回到从库上的relay-log文件中，当从库上的SQL线程发现中继日志发生变化了，它会去查看，并根据内容对自己数据库进行更改，从而是主从数据库保持一致。 搭建 假设我们这里有两台相同版本的数据库环境。（一主一从） 1、先为主库创建一个授权用户。 登录到数据库中： 12grant replication slave on *.* to &apos;dyh&apos;@&apos;%&apos; identified by &apos;12345&apos;;flush privileges; 2、修改数据库的配置文件/etc/my.cnf。设置好server-id和log-bin。 此时我们可以查看主库的相关信息： 3、配置从库 修改配置文件，只需要设置一下它的server-id 指定主库信息： 开启服务 1slave start; 查看是否配置成功 当两个进程均为YES时，说明完成。 4、测试 我们在主库上创建一个数据库和表来验证 然后进入到从库中进行查看：]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LNMP(二)]]></title>
    <url>%2F2019%2F03%2F19%2FLNMP-1%2F</url>
    <content type="text"><![CDATA[注：源码安装包我这都放在/usr/local/src里面，安装后的文件位置放在/usr/local下 MySQL 关于MySQL的安装见LAMP PHP 1、下载安装包并解压 2、创建php用户 1useradd php 2、环境检查并编译安装 12./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php-fpm/etc --enable-fpm --with-fpm-user=php --with-fpm-group=php --with-opensslmake &amp;&amp; make install 注：过程中可能会报很多依赖包没安装的错，根据提示自行安装即可 3、配置文件 123cp php.ini-production /usr/local/php/etc/php.inicp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.confvim /usr/local/php/etc/php-fpm.conf 然后检查配置文件 1/usr/local/php/sbin/php-fpm -t 4、修改php.ini 编辑php.ini文件，找到cgi-fix_pathinfo配置项，去除注释，然后值改为0.官方的文档上说，这里是为了当文件不存在时，阻止Nginx将请求发送到后端的php-fpm模块，从而避免恶意脚本注入的攻击。 5、开启服务 1/usr/local/php/sbin/php-fpm 如果报： 说明没有这个组，创建即可 Nginx 1、下载安装包，并解压 2、编译安装 12./configure --prefix=/usr/local/nginxmake &amp;&amp; make install 3、编辑配置文件，相应的参数设置在我的另一篇有关nginx的有。 4、检查配置文件以及开启服务，放行防火墙 1234/usr/local/nginx/sbin/nginx -t/usr/local/nginx/sbin/nginx -tfirewall-cmd --add-port=80/tcp --permanentfirewall-cmd --reload]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>LNMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LNMP（一）]]></title>
    <url>%2F2019%2F03%2F18%2FLNMP%2F</url>
    <content type="text"><![CDATA[LNMP的工作原理 首先，浏览器发送http request请求到服务器上，服务器响应并处理web请求，将一些静态资源（图片、视频）保存在服务器上，然后将php脚本通过FCGI接口传输给php-fpm（不做处理），然后php-fpm调用php解析器解析。将解析后的脚本返回到php-fmp，在通过fast-cgi的形式将脚本信息传送给nginx服务器，在通过HTTP response的形式传送给浏览器。而对于LAMP，浏览器向服务器发送请求，服务器接收后，由于PHP这里是作为apache的模块一起启动的，所以它会调用PHP模块去处理，将结果返回给浏览器。 简单说说与LAMP的区别 1、在LNMP中，nginx本身对脚本不做处理，它是通过调用php-fpm对其进行处理，二者独立；但是作为LAMP而言，php作为apache的一个模块。 2、LNMP占用的资源较少，配置简单，利用FCGI来动态解析php脚本，但是负载能力有限，不稳定。所以现在也有LNAMP，将两者结合，利用nginx去做出静态脚本，并且利用nginx的转发特性，将动态脚本交给apache处理。 注：安装和配置见下一章]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>LNMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LAMP]]></title>
    <url>%2F2019%2F03%2F18%2FLAMP%2F</url>
    <content type="text"><![CDATA[LAMP企业主流架构最重要的三个环节：一是apache web服务器；二是php；三是mysql数据库。 Linux+apache+mysql/mariadb+php+python是一组用来搭建动态网站的开源软件架构，本身是各自独立的软件服务，放在一起使用，拥有了越来越高的兼容性，共同组成了一个强大的web应用程序平台。 yum源安装 12yum install -y apr apr-devel gcc gcc-c++ pcre pcre-develyum install -y httpd httpd-devel mysql mysql-server mysql-devel php php-devel php-mysql 然后开启相应的服务和放行防火墙，如果selinux影响，可暂时关闭。 源码安装 注：源码包都放在/usr/local/src下，安装后的路径均为/usr/local下 一、安装mysql 1、下载并解压源码包 可去官网进行下载，也可在这个内含很多源码的网站进行下载 2、创建一个用户，用于安装数据库 1useradd -s /sbin/nologin mysql 3、进入到数据库下创建文件夹存放数据库相关文件，并更改权限 1chown -R mysql:mysql data/mysql/ 4、编译，并且指定相应路径 如果此时遇到报错： 则安装依赖包 1yum install -y autoconf 然后重新执行，结果为： 5、根据上图提示安装数据库和设置密码 1/usr/local/mysql/bin/mysql_secure_installation 6、修改配置文件 先拷贝原文件 1cp support-file/my-default.cnf /etc/my.cnf 然后进行修改 7、启动服务并放行防火墙 对于centos 6而言： 复制启动脚本，并修改权限 修改启动脚本 接下来将启动脚本加入系统服务项并设置开机启动： 对于centos 7而言 1/usr/local/mysql/support-files/mysql.server 然后登录数据库： 1mysql -uroot -p 此时如果报错的话： 可以这样: 因为系统会默认去/usr/bin目录下查找命令 二、安装apache 1、安装相应的依赖环境apr、apr-util、pcre（安装apr和apr-util有大坑，建议先阅读apache安装） 去apr官网下载安装包并进行解压 pcre下载 二选一 先编译apr，在编译apr-util 注：先安装gcc gcc-c++编译环境 进入到apr目录下： 1./configure --prefix=/usr/local/apr 如果报这个错： 则我们需修改apr目录中configure文件，找到RM=‘$RM’这一行 修改为RM=’$RM -f’，然后重新进行编译。 1make &amp;&amp; make install 进入到apr-util目录下 12./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/aprmake &amp;&amp; make install 如果报这个错： 则： 1yum install -y expat-devel 然后重新 进入到pcre下 12./configure --prefix=/usr/local/pcre --disable-shared --with-picmake &amp;&amp; make install 2、下载apache源码包解压，进行编译安装 12./configure --prefix=/usr/local/apache --with-apr-util=/usr/local/apr-util --enable-so --enable-rewrite --with-pcre=/usr/local/pcremake &amp;&amp; make install 如果报这个错： 则：我们一个很麻烦的解决过程 删除已经编译安装好的/usr/local/apr 和apr-util文件夹 将解压后的apr和apr-util放在你的解压httpd文件夹下的srclib下 重复 apr和apr-util的编译安装，但之前需要make clean一下 返回到解压的httpd文件夹下，make clean后，重新编译安装 3、编辑配置文件，开启服务，并放行80端口 1234vim /usr/local/apahce/conf/httpd.conf/usr/local/apahce/bin/httpd firewall-cmd --add-port=80/tcp --permanent firewall-cmd --reload 4、测试 打开本地的80端口页面 三、安装php 下载php安装包，解压 1、安装依赖包 12yum install -y libxml2-devel.x86_64 openssl openssl-devel.x86_64 libcurl-devel.x86_64 libjpeg libpng-devel.x86_64 yum insyall -y epel-release.noarch libmcrypt-devel 2、编译、安装php 12./configure --prefix=/usr/local/php5 --with-config-file-path=/usr/local/php5/etc --with-apxs2=/usr/local/apache/bin/apxsmake &amp;&amp; make install 3、文件拷贝 4、在apache中配置php模块 编辑在/usr/local/apache/conf/httpd.conf 5、重启服务即可]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>LAMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP]]></title>
    <url>%2F2019%2F03%2F18%2FFTP%2F</url>
    <content type="text"><![CDATA[文件传输协议（FTP）是一个用于在计算机网络上在客户端和服务器之间进行文件传输的应用层协议。文件传送（file transfer）和文件访问（file access）之间的区别在于：前者由FTP提供，后者由如NFS等应用系统提供。 FTP服务器中的文件按目录结构进行组织，用户通过网络与服务器建立连接，FTP仅基于TCP服务，不支持UDP。分为两个端口：20（数据端口），21（控制端口）。 FTP分为主动模式和被动模式： 主动FTP 主动方式的FTP客户端从一个任意的非特权端口N（N&gt;1024）连接到FTP服务器的命令端口21，然后客户端开始监听端口N+1，并发送FTP命令“port+1”到FTP服务器，接着服务器会从自己的数据端口（20）连接到客户端指定的数据端口（N+1) 。主动模式下，服务器端开启的是20和21端口，客户端开启的是1024以上的端口。 被动FTP 为了解决服务器发起到客户的连接的问题采取了被动方式，或叫做PASV，当客户端通知服务器处于被动模式才启用，在被动方式FTP中，命令连接和数据连接都由客户端发起，当开启一个FTP连接时，客户端打开两个任意的非特权本地端口N和N+1，第一个端口连接服务器的21端口，但与主动方式的FTP不同，客户端不会提交port命令并允许服务器来回连接他的数据端口，而是提交PASV命令。这样做的结果是服务器会开启一个任意的非特权P（P&gt;1024），并发送port p 命令给客户端，然后客户端发起从本地端口N+1到服务器的端口P的连接，用来传送数据，此时服务端的数据端口不再是20端口，此时服务端开启的是21命令端口和大于1024的数据连接端口，客户端开启的是大于1024的两个端口。 1、安装 我这里安装的是vsftp，因为它小巧轻快，安全易用。 1yum install -y vsftp 2、创建用户 一般的话，我是用匿名用户的方式进行登录，为了安全使用虚拟ftp用户映射到真实系统用户的方式去使用ftp服务，从而避免ftp用户直接登陆系统。 1useradd -s /sbin/nologin xiaoming 3、修改主配置文件/etc/vsftpd/vsftpd.conf 此处guest_username=xiaoming是将虚拟用户映射为xiaoming Allow_writeable_chroot=YES 或者将匿名用户的家目录减去写权限，否则报错，新版本会检查这一点。 4、创建chroot_list 创建chroot_list文件并将xiaoming加入，目的是限制xiaoming只可以访问自己的目录，如果此处不设置，虚拟用户将无法登陆ftp。 5、创建虚拟用户并设置密码 注：奇数行写用户名，偶数行写密码，多个用户就写多组，中间不能有空格 6、生成虚拟用户密码库（基于pam模块认证） 123yum install -y db4*db_load -T -t hash -f /etc/vsftpd/vsftpd_login.txt /etc/vsftpd/vsftpd_login.dbchmod 600 vsftpd_login.db 7、修改PAM文件 注释前面的所有，在后面加上认证，路径不需要加.db 8、为虚拟用户创建配置文件目录和配置文件 注：配置文件的名字要与创建的虚拟用户名保持一致 关于配置文件参数说明 9、创建虚拟ftp用户的家目录，并创建测试文件 级联更改系统用户virftp家目录的属主属组： 添加测试文件： 10、放行防火墙以及开启服务 1234systemctl enable vsftpsystemctl start vsftpfirewall-cmd --add-service=ftp --permanentfirewall-cmd --reload 11、测试 在客户端或者pc机上访问该服务器上的ftp服务，登陆后即可看见。 注：如果出现500 oops或者530 登录失败，仔细检查有关配置文件，以及文件目录的权限]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>网络文件共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba]]></title>
    <url>%2F2019%2F03%2F18%2FSamba%2F</url>
    <content type="text"><![CDATA[Samba 是 SMB/CIFS 网络协议的重新实现, 可以在 Linux 和 Windows 系统间进行文件、打印机共享，和 NFS 的功能类似。 一、安装（基于源码安装 &amp;&amp; yum 安装） 注：基于源码安装，虽然操作上难度增加，但是我们可以灵活的对服务的路径等进行设置。 基于源码安装 1、安装编译环境 1234yum install -y gcc gcc-c++ glibc kernel-headers python-develyum install -y gnutls-devel gmp-devel gnutls-c++ libtasnl-develyum install -y nettle-devel pll-kit-devel zlib-devel libacl-develyum install -y openldap-devel cyrus-sasl-devel 2、下载源码并解压 去官网找： 12wget https://download.samba.org/pub/samba/stable/samba-4.9.5.tar.gztar xvf samba-4.9.5.tar.gz 3、进入到解压文件夹里面，进行系统环境检查和编译安装 12./configure --prefix=/usr/local/sambamake &amp;&amp; make install 注：–prefix选项是指定安装后的文件路径 4、关于/usr/local/samba/sbin里面相关说明 smbd：SMB服务器，为客户机如Windows等提供文件和打印服务 nmbd：NetBIOS名字服务器，可以提供浏览支持 smbclient：SMB客户程序，类似于FTP程序，用以从Linux或其他操作系统上访问SMB服务器上的资源 smbmount：SMB挂载工具，卸载为smbmount smbpasswd：用户增删登陆服务端的用户和密码 5、修改链接库文件 配置动态链接库文件，因为运行samba的进行smbd和nmbd需要用到目录/usr/local/samba/lib下的动态链接库文件，但是该目录不是系统默认的动态连接搜索路径，于是我们需要将该目录添加到文件ld.so.conf中。 然后进行更新 1ldconfig 6、创建用于特定用户访问的指定目录，并修改此目录的所属用户和组 创建用户目录并设置允许的用户名和密码，认证方式为系统用户认证，要添加的用户需要在/etc/passwd中存在，也就是说我们先创建一个系统用户，比如： 1useradd test1 然后加入到samba用户组中 1/usr/local/samba/bin/smbpasswd -a test1 如果需要查看当前samba用户成员，可以用： 1pdbedit -L 然后修改共享目录所属的用户以及权限 注：如果需要查看selinux有关samba的设置，可用： 1getsebool -a | grep samba 7、启动服务 12/usr/local/samba/sbin/smbd/usr/local/samba/sbin/nmbd 8、关闭防火墙或者放行 1systemctl stop firewalld.service 9、确定启动 1lsof -l 10、检测 基于yum源安装 1、安装 1yum install -y samba samba-client cifs-utils 2、配置相关文件 3、centos 7以后需要对selinux进行设置 12chcon -t samba_share_t [共享的目录]setsebool -P samba_export_all_ro on 4、添加samba用户和密码，修改相应的权限 这里用的是samba管理成员的工具 12useradd test1pdbedit -a test1 5、启动服务 12systemctl enable smb nmbsystemctl start smb nmb 6、放行防火墙 12firewall-cmd --permanent --add-service=sambafirewall-cmd --reload]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>网络文件共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS]]></title>
    <url>%2F2019%2F03%2F18%2FNFS%2F</url>
    <content type="text"><![CDATA[NFS是一种分布式文件系统,本质是文件系统，允许网络中不同操作系统的计算机间共享文件。NFS在文件传送或者信息传送过程中依赖于RPC。使用NFS时，用户端只需使用mount命令就可把远程文件系统挂接在自己的文件系统之下。 一、服务器的配置（通过yum源安装）： 1、安装并创建需要共享的目录 1yum install -y nfs-utils rpcbind 2、修改文件（showmount 查看所有已挂载的nfs共享目录） nfs.service服务是nfs服务启停控制单元，位于/usr/lib/stemd/system/nfs.service rpc.nfsd是基本的nfs守护进程，主要功能是控制客户端是否可以登录服务器。 exportfs 如果修改了/etc/exports文件后不需要重新激活NFS，只要重新扫描一次/etc/exports文件 配置/etc/exports，在该文件中，每一行代表一个共享目录，并且描述了改目录如何被共享。 &lt;共享目录&gt; [客户端1 选项] [客户端2 选项] /nfsshare *(rw,all_squash,sync,anonuid=1001) 共享目录：nfs系统中需要共享给客户端使用的目录 客户端：网络中可以访问这个nfs共享目录的计算机。 指定方式： 主机：192.168.1.1 指定子网中的所有主机：192.168.1.0/24 指定域名的主机：www.baidu.com 指定域中的所有主机：*.baidu.com 所有主机：* 注：当我们需要设置成all_squash的时候需要指定它的anonuid和anongid 比如： 它表示共享/nfsshare目录，192.168.206.0网段的所有主机都可以访问，具有可读写，并且所有的用户在访问时都映射成服务器上uid为1001，gid为1001的用户 3、开启服务，并放行防火墙 12345678systemctl enable nfssystemctl restart nfssystemctl enable rpcbindsystemctl restart rpcbindsystemctl status nfsfirewall-cmd --add-service=nfs --permanent firewall-cmd --reload 4、检查 1showmount -e 1rpcinfo -p 二、客户端配置 1、安装并创建本地挂载文件夹 2、修改/etc/fstab文件使其永久挂载 3、进行挂载 1mount -a 4、查看挂载情况 1df -Th 5、测试 进入到test目录（客户端的挂载文件夹）中，创建文件，但此时报错，因为all_squash和roor_squash是默认设置，会将远程访问的用户映射为nobody用户，而test是root用户,所以可将共享文件权限设为：no_root_squash，也可以将nfsshare更改为nobody用户。 客户端： 服务器端]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>网络文件共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsyslog日志系统]]></title>
    <url>%2F2019%2F03%2F18%2Frsyslog%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[rsyslog采用模块化的设计，是syslog的替代品 特点 实现了基本的syslog协议 直接兼容syslogd的syslog.conf 在同一台机器上支持多个rsyslogd进程 丰富的过滤功能，可将消息过滤后再进行转发 灵活的配置选项，配置文件中可写简单的逻辑判断 有现成的前端web展示 主配置文件（/etc/rsyslog.conf） 结构： 全局指令：设置全局参数，如主消息队列尺寸的设置，加载扩展模块的那个配置。 模块：指定记录的消息格式，也用于动态文件名称的生成。 输出通道：对用户最期望的消息进行预定义。 规则：指定消息的规则。在规则中可以引用之前定义的模板和输出通道。 如图： 规则配置语法 facility.priority action（设备.级别 动作） 其中， 设备级别有： authpriv：与安全、认证相关的信息 cron：与cron和at有关的信息 daemon：没有明确设备定 义的守护进程的信息 ftp：报告ftp守护进程的信息 kern：报告与内核有关的信息 lpr：报告与打印服务有关的信息 mail：报告与邮件服务有关的信息 news：报告与网络新闻服务有关的信息 syslog：报告由syslog生成的信息 user：报告一般的用户级别信息 uucp：由UUCP子系统生成的信息 local0-local7：保留给本地其他应用程序使用 日志级别： 0EMERG（紧急）：会导致主机系统不可用的情况 1ALERT（警告）：必须马上采取措施解决的问题 2CRIT（严重）：比较严重的情况 3ERR（错误）：运行出现错误 4WARNING （提醒）：可能会影响系统功能的事件 5NOTICE（注意）：不会影响系统但值得注意 6INFO（信息）：一般信息 7DEBUG（调试）：程序或系统调试信息等 none：用于禁止任何日志消息 facility.priority语法 可以在同一行中设置多个“设备.级别”组合，每组之间用分号隔开 可以同时指定多个设备或级别，用逗号隔开 可以使用通配符*标识所有的设备或级别 级别的指定 直接指定：记录比指定级别高的所有级别的消息 使用=前缀：只记录指定级别的日志消息 使用！前缀：不记录比指定级别高的所有级别的消息 使用！=前缀：不记录指定级别的日志消息 none：用于禁止任何日志消息 目标动作（也就是发送消息给谁） 常规文件 将日志记录于文件：以“/”开始的全路径文件名 [-]/path/filename 文件名之前的减号（-）表示对文件的写入同时不立即同步到磁盘，这样可以加快日志写入速度 终端设备 将日志发送到终端：/dev/console，/dev/ttyX 命名管道 将日志记录到命令管道，用于日志调试非常方便 | named_pipe @host 远程的日志服务器 username 发送信息到本机的指定用户终端中，前提是该用户必须已经登录到系统中 举例 把邮件除info级别外都写入到mail文件中 1mail.*;mail.!=info /var/adm/mail 仅把邮件的通知性消息发送到tty12终端设备 1mail.=info /dev/tty12 n如果root和Joey用户已经登录到系统，则把所有紧急信息通知给他们 1*.alert root,joey 把所有信息都发送到192.168.3.100主机 1*.* @192.168.3.100]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github+Typora搭建自己的博客]]></title>
    <url>%2F2019%2F03%2F18%2Fhexo%2F</url>
    <content type="text"><![CDATA[一、部署Hexo（后面操作基本在git bash里进行） 1、找个合适的位置，本地创建blog的家文件夹 2、进入到这个文件夹 3、安装并且检查 12npm install hexo-cli -ghexo -v 4、初始化 1hexo init 5、安装相应的组件 1hexo install 6、生成本地静态文件 1hexo g 7、启动本地web浏览 1hexo s 注：通过本地的4000端口可查看，若一直无法加载，可查看本地4000端口是否被占用，若被占用，则映射到其他端口 1hexo -s -p [端口] 此时，出现初始页面： 8、设置主题 网上有很多hexo的主题，这里我用的是GitHub里iissnan的。 在家目录文件夹下： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 此时我们查看当前的列表： 当前目录下的__config是你的站点的相关配置信息， theme文件夹，里面的next文件夹里也有一个__config.yml，这个是你的主题相关设置文件。 克隆完成后，编辑当前目录下的__config.yml文件，找到theme字段，并将其值改为next。 9、更新配置 清除原先缓存 1hexo clean 进行本地部署并查看 1hexo g &amp;&amp; hexo s 反复进行调试、浏览后，推送到GitHub上（后文会讲解） 1hexo d 注：关于hexo相关命令： 12345hexo generate(hexo g) //生成本地静态文件，并创建一个public文件夹hexo server（hexo s） //启动本地web浏览hexo deploy（hexo d） //将博客部署到远端hexo new &quot;postname&quot; //新建文章hexo new page &quot;pagename&quot; //新建页面 二、关于GitHub Pages 1、一般GitHub Pages用于托管个人的项目，但是现在我们可以用来搭建自己的私人博客。但是需要注意的是。由于GitHub特殊的命名规定：每个账号只能有一个仓库存放个人主页，名字格式必须是：username.github.io。如图： 2、关联Hexo和GitHub Page 在根目录下找到主配置文件：__config.yml，进行修改 1234deploy: type: git repo: git@github.com:username/username.github.io branch: master 注：这里的username换成自己的，并且hexo的配置中冒号(:)与后文要有空格 3、安装扩展模板 1npm install hexo-deployer-git –save 4、将本地部署推送到远端 1hexo d -g 此时输入你的GitHub Page地址就可以访问了 关于主题页面优化以及创建文章和标签，请参考以下网址： 主题设置 高级使用技巧 添加分类和标签 发布文章 到这基本就结束了，一些其他优化和功能，大家自己去摸索 三、关于Typora 这个东西是我用来进行Markdown编辑的一款非常简洁的编辑器，当然大家也可以去使用Visual Code，主要我是看中了它的简洁美观、实时预览、免费而且还支持跨平台。大家可以去官网进行下载。 另外我们知道，Markdown很难受的一点就是它的图片插入和管理，所以我们需要一个稳定的地方来存放我们需要插入的图片也就是图床，这里我使用的是 路过图床，当你上传图片后自动给你生成很多链接包括markdown链接，或者大家也可以用微博的，但是它有个不好的就是所有图片都会给你转为jpg，而且有水印吧，或者可以试试 postimage。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hexo]]></title>
    <url>%2F2019%2F03%2F18%2F%E5%85%B3%E4%BA%8Ehexo%2F</url>
    <content type="text"><![CDATA[Hexo是什么？ 根据官方文档的解释：这是一个基于Node.js，能快速、简洁且高效的博客框架。Hexo使用Markdown(或者是其他的渲染引擎)来解析文章，在几秒内能够生成相应的静态网页。 优点 飞速渲染 由于Hexo基于Node.js，它能够在几秒内瞬间渲染上百个页面，拥有超快的生成速度 对Markdown的支持 支持Flavored Markdown的所有功能，可以整合Octopress的大多数的插件 部署快捷 通过简洁的指令就可以部署到GitHub Pages等网站 插件丰富 Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。 怎么安装Hexo？ 在安装之前，我们需要做一些准备工作，安装Git和Node.js 1、安装Git 如果有360软件管家，直接搜索，一键安装，这样很快。如果没有，则去官网进行下载git，安装。 关联自己的GitHub 配置GitHub用户名和邮箱： 12git config --global user.name [你的用户名]git config --global user.email [邮箱] 进行查看： 1git config --list 创建SSH Key 1ssh-keygen -t rsa -C &quot;邮箱地址&quot; 一路回车生成，最后根据它完成的提示信息，找到自己公钥所在路径，将公钥（id_rsa.pub）复制到GitHub里。 验证： 1ssh -T git@github.com 此时会有一个successfully信息 2、安装Node.js 官网下载安装Node.js，最后如果在命令行输入： 1node -v 如果出现版本信息，则说明安装成功。 3、Windows下安装Hexo git bash里输入： 1npm install -g hexo-cli]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
